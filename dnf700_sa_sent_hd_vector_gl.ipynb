{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from spacy.lang.en import English\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.layers import BatchNormalization, Lambda, Concatenate, Dropout, Conv1D, MaxPooling1D, Input, TimeDistributed, Dense, LSTM, RepeatVector, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from AttentionModules import SelfAttention_gl as SelfAttention,CrossAttention_gl as CrossAttention\n",
    "import sys,os\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['authors', 'evidence', 'headline', 'id', 'reason', 'claims', 'type',\n",
       "        'urls'],\n",
       "       dtype='object'),\n",
       " Index(['authors', 'headline', 'id', 'type', 'urls'], dtype='object'),\n",
       " 705,\n",
       " 705)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnf700 = pd.read_json('evaluation_set/deepnofakes/dnf_700/initial.json')\n",
    "dnf_eval = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V2.json')\n",
    "dnf_eval.columns = ['authors', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls'] \n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_array_id.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_array_id.p', 'rb') as fp:\n",
    "    articles300 = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors300 = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "dnf_eval.keys(), dnf700.keys(), len(articles.keys()), len(article_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits)\n",
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "test_batchsize = 50\n",
    "train_steps_per_epoch = 4\n",
    "val_steps_per_epoch = 1\n",
    "epochs = 2000\n",
    "max_sentences = 0\n",
    "for idx in articles.keys():\n",
    "    num = len(articles[idx])\n",
    "    if num>=max_sentences:\n",
    "        max_sentences = num\n",
    "        \n",
    "max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [authors, headline, id, type, urls]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdl = \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\"\n",
    "hdl = \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\"\n",
    "# hdl = \"If You Thought The Trump Child Rape Case In NY Couldn’t Get Much Worse — You Were Wrong\"\n",
    "# hdl = \"California Set to Let Public Schools Teach Primarily in Spanish\"\n",
    "dnf700[dnf700.headline==hdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sorted(dnf700.headline.unique())\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(VIDEO) Female College Students Protesting Because ‘Trump is a Rapist’',\n",
       " 'Assange Confirms: WikiLeaks Didn’t Get Emails From Russian Govt',\n",
       " 'BREAKING: Fraudulent Clinton Votes Discovered By The “Tens Of Thousands”',\n",
       " \"Clinton Camp Demands 'Compliant Citizenry' for Master Plan\",\n",
       " 'Clinton Received Debate Questions Week Before Debate',\n",
       " \"DOJ's Loretta Lynch Tried To Squash Comey's Letter To Congress\",\n",
       " 'Department of Homeland Security Chairman Officially Indicts Hillary Clinton of Treason',\n",
       " 'Developing: Obama WH admits that Hillary gave ISIS $400 million on accident',\n",
       " 'Erdoğan: US, the founder of ISIS',\n",
       " \"FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Healthcare Begins With A Bombshell! » 100percentfedUp.com\",\n",
       " 'FBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent Murder-Suicide',\n",
       " 'FBI Director Comey’s ‘Leaked’ Memo Explains Why He’s Reopening the Clinton Email Case',\n",
       " 'FBI director received millions from Clinton Foundation, his brother’s law firm does Clinton’s taxes',\n",
       " 'Former NATO Chief: We Need US as ‘World’s Policeman’',\n",
       " \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\",\n",
       " 'HE’S NEVER SOLD AN ORIGINAL PAINTING UNTIL NOW…And This One’s Going In The White House',\n",
       " 'HILLARY’S (Islamic) AMERICA IS ALREADY HERE where ‘Muslim NO-GO ZONES’ are popping up all over Michiganistan',\n",
       " \"Hillary Clinton Cut Her Tax Bill by 'Donating' $1 Million to Herself via the Clinton Foundation?\",\n",
       " 'Hillary Clinton Used Hand Signals to Rig Debate?',\n",
       " \"Hillary Clinton Wore 'Secret Earpiece' During Commander-in-Chief Forum\",\n",
       " 'Hillary Clinton Wore Secret Earpiece During First Presidential Debate?',\n",
       " \"Hillary Clinton in 2013: 'I Would Like to See People Like Donald Trump Run for Office\",\n",
       " \"Hillary Clinton's 'Sudden Move' of $1.8 Billion to Qatar Central Bank Stuns Financial World\",\n",
       " 'Hillary Clinton’s “Sudden Move” Of $1.8 Billion To Qatar Central Bank Stuns Financial World',\n",
       " 'Hillary Friend Bribed FBI Agent and His Wife',\n",
       " 'Hillary Personally Ordered ‘Donald Duck’ Troll Campaign',\n",
       " 'Hillary Sold Weapons To ISIS, Wikileaks Confirms',\n",
       " 'ISIS Leader Calls for American Muslim Voters to Support Hillary Clinton',\n",
       " 'Jill Stein Endorsed Donald Trump',\n",
       " 'Julian Assange Makes VERY Suspect Post Election Announcement, Seeks Pardon From Trump',\n",
       " 'KREMLIN: Putin Congratulates Trump, Hopes to Work Together Major Issues',\n",
       " 'LOL! BRITISH WIFE Of LIB ACTOR Who Said: “There Will Never Be A President Donald Trump”…Warns Americans About President-Elect Trump [VIDEO]',\n",
       " 'Leaked 2013 Trump Tax Return Shows He Paid Over 40 Million in Taxes',\n",
       " 'NSA Whistleblower Says DNC Email Hack Was Not by Russia, but by US Intelligence | Alternative',\n",
       " 'Obama Declares His Family Will Move to Canada If Trump Is Elected',\n",
       " 'Pentagon Officials Furious After Clinton Announces US Response Time for Nuclear Launch During Debate',\n",
       " 'Pentagon Seeks Another $6 Billion for Overseas Troop Deployments',\n",
       " \"Physician Confirms Hillary Clinton Has Parkinson's Disease\",\n",
       " 'President Obama Confirms He Will Refuse to Leave Office If Trump Is Elected',\n",
       " 'Reddit Users Declare War On Hillary’s Paid Internet Trolls',\n",
       " \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\",\n",
       " 'The Clinton Foundation has purchased over $137 million of illegal arms and ammunition',\n",
       " 'Top aide: Hillary ‘still not perfect in her head’, Wikileaks',\n",
       " 'Trump accuses Obama, Hillary Clinton of founding Daesh',\n",
       " 'US Officials See No Link Between Trump and Russia',\n",
       " 'US Officials Try to Scare Voters With Terror Threat',\n",
       " 'US Threatens Military Hacks on Russia’s Electric, Communications Grids Over Election',\n",
       " 'WIKILEAKS: Hillary Got $12 Million for Clinton Charity As Quid Pro Quo For Morocco Meeting',\n",
       " 'WikiLeaks CONFIRMS Hillary Sold Weapons to ISIS... Then Drops Another BOMBSHELL! Breaking News',\n",
       " 'WikiLeaks: Hillary Clinton knew Saudi, Qatar were funding ISIS – but still took their money for Foundation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_titles = sorted(dnf_eval.headline.unique())\n",
    "len(test_titles)\n",
    "test_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_test_titles = np.array(list(set(titles)-set(test_titles)))\n",
    "len(non_test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for train_index, val_index in kf.split(non_test_titles):\n",
    "    indices.append([train_index,val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
      " 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309\n",
      " 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
      " 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399\n",
      " 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417\n",
      " 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435\n",
      " 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453\n",
      " 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471\n",
      " 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489\n",
      " 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507\n",
      " 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525\n",
      " 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543\n",
      " 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561\n",
      " 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579\n",
      " 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597\n",
      " 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615\n",
      " 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633\n",
      " 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649] [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\n",
      " 256 257 258 259]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(520, 130, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_index, val_index = indices[np.random.randint(0,num_splits)]\n",
    "print(train_index,val_index)\n",
    "val_titles = non_test_titles[val_index]\n",
    "train_titles = non_test_titles[train_index]\n",
    "len(train_titles),len(val_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    sentencizer = English()\n",
    "    sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return sentencizer, nlp\n",
    "sentencizer, nlp = load_spacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        idx = idx.strip()\n",
    "        \n",
    "            \n",
    "#         cl = dataframe[dataframe.Article==idx]['Claim'].values\n",
    "#         sentences=articles[ar_id]\n",
    "#         print(len(sentences))\n",
    "        if mode=='test':\n",
    "            hd = dnf_eval[dnf_eval.headline==idx]['headline'].values[0].lower()\n",
    "            ar_id = dnf_eval[dnf_eval.headline==idx]['id'].values[0]\n",
    "            cl = dnf_eval[dnf_eval.headline==idx]['claims'].values[0]\n",
    "            ar_claims.append(cl)\n",
    "            sentences = articles300[ar_id]\n",
    "            vectors = article_vectors300[ar_id]\n",
    "        else:\n",
    "            try:\n",
    "                hd = dataframe[dataframe.headline==idx]['headline'].values[0].lower()\n",
    "                ar_id = dataframe[dataframe.headline==idx]['id'].values[0]\n",
    "                ar_claims.append('None')\n",
    "                sentences=articles[ar_id]\n",
    "                vectors = article_vectors[ar_id]\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(idx)\n",
    "            \n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "        \n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = datagen_dnf(train_batchsize,dnf700,mode='train')\n",
    "vdg = datagen_dnf(val_batchsize,dnf700,mode='val')\n",
    "test_dg = datagen_dnf(test_batchsize,dnf700,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(test_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['sentence_vectors'].shape, x['headline_vector'].shape, y['headline_token_classes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 125, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 125, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 125, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 125, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 125, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 125, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 125, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 125, 1024)    0           ca1[0][0]                        \n",
      "                                                                 ca2[0][0]                        \n",
      "                                                                 ca3[0][0]                        \n",
      "                                                                 ca4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 125, 1024)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 125, 1024)    4096        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 63, 256)      786688      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 32, 256)      196864      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16, 256)      196864      conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 8, 256)       196864      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 8, 256)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 256)       1024        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          131584      global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_headline_vector (Dense)  (None, 300)          153900      batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,465,356\n",
      "Trainable params: 2,460,684\n",
      "Non-trainable params: 4,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2130pt\" viewBox=\"0.00 0.00 1891.50 2130.00\" width=\"1892pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2126)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-2126 1887.5,-2126 1887.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140493965793544 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140493965793544</title>\n",
       "<polygon fill=\"none\" points=\"903.5,-2075.5 903.5,-2121.5 1244.5,-2121.5 1244.5,-2075.5 903.5,-2075.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"991.5\" y=\"-2094.8\">sentence_vectors: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1079.5,-2075.5 1079.5,-2121.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1107\" y=\"-2106.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1079.5,-2098.5 1134.5,-2098.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1107\" y=\"-2083.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1134.5,-2075.5 1134.5,-2121.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1189.5\" y=\"-2106.3\">(None, 125, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1134.5,-2098.5 1244.5,-2098.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1189.5\" y=\"-2083.3\">(None, 125, 300)</text>\n",
       "</g>\n",
       "<!-- 140493965793936 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140493965793936</title>\n",
       "<polygon fill=\"none\" points=\"930,-1992.5 930,-2038.5 1218,-2038.5 1218,-1992.5 930,-1992.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"991.5\" y=\"-2011.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1053,-1992.5 1053,-2038.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1080.5\" y=\"-2023.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1053,-2015.5 1108,-2015.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1080.5\" y=\"-2000.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1108,-1992.5 1108,-2038.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1163\" y=\"-2023.3\">(None, 125, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1108,-2015.5 1218,-2015.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1163\" y=\"-2000.3\">(None, 125, 16)</text>\n",
       "</g>\n",
       "<!-- 140493965793544&#45;&gt;140493965793936 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140493965793544-&gt;140493965793936</title>\n",
       "<path d=\"M1074,-2075.37C1074,-2067.15 1074,-2057.66 1074,-2048.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1077.5,-2048.61 1074,-2038.61 1070.5,-2048.61 1077.5,-2048.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493965794048 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140493965794048</title>\n",
       "<polygon fill=\"none\" points=\"932,-1909.5 932,-1955.5 1216,-1955.5 1216,-1909.5 932,-1909.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"994.5\" y=\"-1928.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1057,-1909.5 1057,-1955.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1084.5\" y=\"-1940.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1057,-1932.5 1112,-1932.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1084.5\" y=\"-1917.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1112,-1909.5 1112,-1955.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-1940.3\">(None, 125, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1112,-1932.5 1216,-1932.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-1917.3\">(None, 125, 16)</text>\n",
       "</g>\n",
       "<!-- 140493965793936&#45;&gt;140493965794048 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140493965793936-&gt;140493965794048</title>\n",
       "<path d=\"M1074,-1992.37C1074,-1984.15 1074,-1974.66 1074,-1965.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1077.5,-1965.61 1074,-1955.61 1070.5,-1965.61 1077.5,-1965.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140497575065808 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140497575065808</title>\n",
       "<polygon fill=\"none\" points=\"933,-1826.5 933,-1872.5 1215,-1872.5 1215,-1826.5 933,-1826.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"994.5\" y=\"-1845.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1056,-1826.5 1056,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1083.5\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1056,-1849.5 1111,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1083.5\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1111,-1826.5 1111,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1163\" y=\"-1857.3\">(None, 125, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1111,-1849.5 1215,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1163\" y=\"-1834.3\">(None, 125, 32)</text>\n",
       "</g>\n",
       "<!-- 140493965794048&#45;&gt;140497575065808 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140493965794048-&gt;140497575065808</title>\n",
       "<path d=\"M1074,-1909.37C1074,-1901.15 1074,-1891.66 1074,-1882.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1077.5,-1882.61 1074,-1872.61 1070.5,-1882.61 1077.5,-1882.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493965882928 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140493965882928</title>\n",
       "<polygon fill=\"none\" points=\"932,-1743.5 932,-1789.5 1216,-1789.5 1216,-1743.5 932,-1743.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"994.5\" y=\"-1762.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1057,-1743.5 1057,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1084.5\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1057,-1766.5 1112,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1084.5\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1112,-1743.5 1112,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-1774.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1112,-1766.5 1216,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-1751.3\">(None, 125, 32)</text>\n",
       "</g>\n",
       "<!-- 140497575065808&#45;&gt;140493965882928 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140497575065808-&gt;140493965882928</title>\n",
       "<path d=\"M1074,-1826.37C1074,-1818.15 1074,-1808.66 1074,-1799.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1077.5,-1799.61 1074,-1789.61 1070.5,-1799.61 1077.5,-1799.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493965884048 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140493965884048</title>\n",
       "<polygon fill=\"none\" points=\"864.5,-1660.5 864.5,-1706.5 1283.5,-1706.5 1283.5,-1660.5 864.5,-1660.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"994.5\" y=\"-1679.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1124.5,-1660.5 1124.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1152\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1124.5,-1683.5 1179.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1152\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1179.5,-1660.5 1179.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1231.5\" y=\"-1691.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1179.5,-1683.5 1283.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1231.5\" y=\"-1668.3\">(None, 125, 32)</text>\n",
       "</g>\n",
       "<!-- 140493965882928&#45;&gt;140493965884048 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140493965882928-&gt;140493965884048</title>\n",
       "<path d=\"M1074,-1743.37C1074,-1735.15 1074,-1725.66 1074,-1716.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1077.5,-1716.61 1074,-1706.61 1070.5,-1716.61 1077.5,-1716.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493965245408 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140493965245408</title>\n",
       "<polygon fill=\"none\" points=\"265.5,-1577.5 265.5,-1623.5 656.5,-1623.5 656.5,-1577.5 265.5,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331.5\" y=\"-1596.8\">sa1: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"397.5,-1577.5 397.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"397.5,-1600.5 452.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"452.5,-1577.5 452.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-1608.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"452.5,-1600.5 656.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-1585.3\">[(None, 125, 32), (125, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493965884048&#45;&gt;140493965245408 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140493965884048-&gt;140493965245408</title>\n",
       "<path d=\"M907.305,-1660.47C823.752,-1649.43 722.559,-1636.06 637.667,-1624.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"638.034,-1621.36 627.662,-1623.52 637.117,-1628.3 638.034,-1621.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493964677072 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140493964677072</title>\n",
       "<polygon fill=\"none\" points=\"674.5,-1577.5 674.5,-1623.5 1065.5,-1623.5 1065.5,-1577.5 674.5,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"740.5\" y=\"-1596.8\">sa2: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"806.5,-1577.5 806.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"834\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"806.5,-1600.5 861.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"834\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"861.5,-1577.5 861.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-1608.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"861.5,-1600.5 1065.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-1585.3\">[(None, 125, 32), (125, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493965884048&#45;&gt;140493964677072 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140493965884048-&gt;140493964677072</title>\n",
       "<path d=\"M1018.53,-1660.47C992.668,-1650.21 961.733,-1637.92 934.806,-1627.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"936.049,-1623.96 925.463,-1623.52 933.466,-1630.47 936.049,-1623.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493963394800 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140493963394800</title>\n",
       "<polygon fill=\"none\" points=\"1083.5,-1577.5 1083.5,-1623.5 1474.5,-1623.5 1474.5,-1577.5 1083.5,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1149.5\" y=\"-1596.8\">sa3: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1215.5,-1577.5 1215.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1243\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1215.5,-1600.5 1270.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1243\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1270.5,-1577.5 1270.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1372.5\" y=\"-1608.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1270.5,-1600.5 1474.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1372.5\" y=\"-1585.3\">[(None, 125, 32), (125, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493965884048&#45;&gt;140493963394800 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140493965884048-&gt;140493963394800</title>\n",
       "<path d=\"M1129.75,-1660.47C1155.73,-1650.21 1186.82,-1637.92 1213.88,-1627.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1215.25,-1630.45 1223.26,-1623.52 1212.68,-1623.94 1215.25,-1630.45\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493962661112 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140493962661112</title>\n",
       "<polygon fill=\"none\" points=\"1492.5,-1577.5 1492.5,-1623.5 1883.5,-1623.5 1883.5,-1577.5 1492.5,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1558.5\" y=\"-1596.8\">sa4: SelfAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1624.5,-1577.5 1624.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1652\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1624.5,-1600.5 1679.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1652\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1679.5,-1577.5 1679.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1781.5\" y=\"-1608.3\">(None, 125, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1679.5,-1600.5 1883.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1781.5\" y=\"-1585.3\">[(None, 125, 32), (125, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493965884048&#45;&gt;140493962661112 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140493965884048-&gt;140493962661112</title>\n",
       "<path d=\"M1240.97,-1660.47C1324.66,-1649.43 1426.01,-1636.06 1511.04,-1624.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1511.61,-1628.3 1521.07,-1623.52 1510.69,-1621.36 1511.61,-1628.3\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493964930072 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140493964930072</title>\n",
       "<polygon fill=\"none\" points=\"764.5,-1494.5 764.5,-1540.5 1383.5,-1540.5 1383.5,-1494.5 764.5,-1494.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"848.5\" y=\"-1513.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"932.5,-1494.5 932.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"960\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"932.5,-1517.5 987.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"960\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"987.5,-1494.5 987.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185.5\" y=\"-1525.3\">[(None, 125, 32), (None, 125, 32), (None, 125, 32), (None, 125, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"987.5,-1517.5 1383.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185.5\" y=\"-1502.3\">(None, 125, 128)</text>\n",
       "</g>\n",
       "<!-- 140493965245408&#45;&gt;140493964930072 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140493965245408-&gt;140493964930072</title>\n",
       "<path d=\"M627.695,-1577.47C711.248,-1566.43 812.441,-1553.06 897.333,-1541.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"897.883,-1545.3 907.338,-1540.52 896.966,-1538.36 897.883,-1545.3\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493964677072&#45;&gt;140493964930072 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140493964677072-&gt;140493964930072</title>\n",
       "<path d=\"M925.474,-1577.47C951.332,-1567.21 982.267,-1554.92 1009.19,-1544.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1010.53,-1547.47 1018.54,-1540.52 1007.95,-1540.96 1010.53,-1547.47\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493963394800&#45;&gt;140493964930072 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140493963394800-&gt;140493964930072</title>\n",
       "<path d=\"M1223.25,-1577.47C1197.27,-1567.21 1166.18,-1554.92 1139.12,-1544.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1140.32,-1540.94 1129.74,-1540.52 1137.75,-1547.45 1140.32,-1540.94\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493962661112&#45;&gt;140493964930072 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140493962661112-&gt;140493964930072</title>\n",
       "<path d=\"M1521.03,-1577.47C1437.34,-1566.43 1335.99,-1553.06 1250.96,-1541.84\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1251.31,-1538.36 1240.93,-1540.52 1250.39,-1545.3 1251.31,-1538.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493965794384 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140493965794384</title>\n",
       "<polygon fill=\"none\" points=\"403.5,-1494.5 403.5,-1540.5 746.5,-1540.5 746.5,-1494.5 403.5,-1494.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506\" y=\"-1513.8\">input_headline_vector: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"608.5,-1494.5 608.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"608.5,-1517.5 663.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"663.5,-1494.5 663.5,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"705\" y=\"-1525.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"663.5,-1517.5 746.5,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"705\" y=\"-1502.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140493960763264 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140493960763264</title>\n",
       "<polygon fill=\"none\" points=\"486,-1411.5 486,-1457.5 726,-1457.5 726,-1411.5 486,-1411.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"537\" y=\"-1430.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"588,-1411.5 588,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"615.5\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"588,-1434.5 643,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"615.5\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"643,-1411.5 643,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-1442.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"643,-1434.5 726,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-1419.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140493965794384&#45;&gt;140493960763264 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140493965794384-&gt;140493960763264</title>\n",
       "<path d=\"M583.471,-1494.37C586.683,-1485.97 590.406,-1476.24 593.89,-1467.14\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"597.234,-1468.2 597.539,-1457.61 590.696,-1465.7 597.234,-1468.2\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493964930128 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140493964930128</title>\n",
       "<polygon fill=\"none\" points=\"928,-1411.5 928,-1457.5 1216,-1457.5 1216,-1411.5 928,-1411.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"989.5\" y=\"-1430.8\">conv1d_3: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1051,-1411.5 1051,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1078.5\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1051,-1434.5 1106,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1078.5\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1106,-1411.5 1106,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161\" y=\"-1442.3\">(None, 125, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1106,-1434.5 1216,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161\" y=\"-1419.3\">(None, 125, 256)</text>\n",
       "</g>\n",
       "<!-- 140493964930072&#45;&gt;140493964930128 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140493964930072-&gt;140493964930128</title>\n",
       "<path d=\"M1073.45,-1494.37C1073.25,-1486.15 1073.02,-1476.66 1072.8,-1467.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1076.29,-1467.52 1072.55,-1457.61 1069.29,-1467.69 1076.29,-1467.52\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493960601216 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140493960601216</title>\n",
       "<polygon fill=\"none\" points=\"476.5,-1328.5 476.5,-1374.5 751.5,-1374.5 751.5,-1328.5 476.5,-1328.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538\" y=\"-1347.8\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-1328.5 599.5,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-1351.5 654.5,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"627\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"654.5,-1328.5 654.5,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-1359.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"654.5,-1351.5 751.5,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-1336.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140493960763264&#45;&gt;140493960601216 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140493960763264-&gt;140493960601216</title>\n",
       "<path d=\"M608.186,-1411.37C608.997,-1403.15 609.935,-1393.66 610.817,-1384.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"614.317,-1384.9 611.817,-1374.61 607.351,-1384.21 614.317,-1384.9\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961195024 -->\n",
       "<g class=\"node\" id=\"node16\"><title>140493961195024</title>\n",
       "<polygon fill=\"none\" points=\"912,-1328.5 912,-1374.5 1202,-1374.5 1202,-1328.5 912,-1328.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"974.5\" y=\"-1347.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1037,-1328.5 1037,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1064.5\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1037,-1351.5 1092,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1064.5\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1092,-1328.5 1092,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1147\" y=\"-1359.3\">(None, 125, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1092,-1351.5 1202,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1147\" y=\"-1336.3\">(None, 125, 256)</text>\n",
       "</g>\n",
       "<!-- 140493964930128&#45;&gt;140493961195024 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140493964930128-&gt;140493961195024</title>\n",
       "<path d=\"M1067.9,-1411.37C1066.38,-1403.15 1064.62,-1393.66 1062.97,-1384.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1066.36,-1383.8 1061.09,-1374.61 1059.47,-1385.08 1066.36,-1383.8\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493960597800 -->\n",
       "<g class=\"node\" id=\"node17\"><title>140493960597800</title>\n",
       "<polygon fill=\"none\" points=\"410,-1245.5 410,-1291.5 822,-1291.5 822,-1245.5 410,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"540\" y=\"-1264.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"670,-1245.5 670,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"697.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"670,-1268.5 725,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"697.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"725,-1245.5 725,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"773.5\" y=\"-1276.3\">(None, 1, 256)</text>\n",
       "<polyline fill=\"none\" points=\"725,-1268.5 822,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"773.5\" y=\"-1253.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140493960601216&#45;&gt;140493960597800 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140493960601216-&gt;140493960597800</title>\n",
       "<path d=\"M614.547,-1328.37C614.749,-1320.15 614.984,-1310.66 615.204,-1301.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"618.706,-1301.69 615.454,-1291.61 611.708,-1301.52 618.706,-1301.69\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961194744 -->\n",
       "<g class=\"node\" id=\"node18\"><title>140493961194744</title>\n",
       "<polygon fill=\"none\" points=\"840.5,-1245.5 840.5,-1291.5 1265.5,-1291.5 1265.5,-1245.5 840.5,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"970.5\" y=\"-1264.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1100.5,-1245.5 1100.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1128\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1100.5,-1268.5 1155.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1128\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1155.5,-1245.5 1155.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1210.5\" y=\"-1276.3\">(None, 125, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1155.5,-1268.5 1265.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1210.5\" y=\"-1253.3\">(None, 125, 256)</text>\n",
       "</g>\n",
       "<!-- 140493961195024&#45;&gt;140493961194744 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>140493961195024-&gt;140493961194744</title>\n",
       "<path d=\"M1055.91,-1328.37C1055.5,-1320.15 1055.03,-1310.66 1054.59,-1301.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1058.08,-1301.42 1054.09,-1291.61 1051.09,-1301.77 1058.08,-1301.42\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959871680 -->\n",
       "<g class=\"node\" id=\"node19\"><title>140493959871680</title>\n",
       "<polygon fill=\"none\" points=\"844,-1162.5 844,-1208.5 1248,-1208.5 1248,-1162.5 844,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"915\" y=\"-1181.8\">ca1: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"986,-1162.5 986,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1013.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"986,-1185.5 1041,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1013.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1041,-1162.5 1041,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-1193.3\">[(None, 1, 256), (None, 125, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1041,-1185.5 1248,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1144.5\" y=\"-1170.3\">[(None, 125, 256), (1, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493960597800&#45;&gt;140493959871680 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>140493960597800-&gt;140493959871680</title>\n",
       "<path d=\"M732.931,-1245.47C790.529,-1234.62 860.078,-1221.52 918.98,-1210.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"919.913,-1213.81 929.092,-1208.52 918.617,-1206.93 919.913,-1213.81\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959212616 -->\n",
       "<g class=\"node\" id=\"node20\"><title>140493959212616</title>\n",
       "<polygon fill=\"none\" points=\"1266,-1162.5 1266,-1208.5 1670,-1208.5 1670,-1162.5 1266,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1337\" y=\"-1181.8\">ca2: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"1408,-1162.5 1408,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1435.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1408,-1185.5 1463,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1435.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1463,-1162.5 1463,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1566.5\" y=\"-1193.3\">[(None, 1, 256), (None, 125, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1463,-1185.5 1670,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1566.5\" y=\"-1170.3\">[(None, 125, 256), (1, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493960597800&#45;&gt;140493959212616 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>140493960597800-&gt;140493959212616</title>\n",
       "<path d=\"M822.252,-1245.83C825.187,-1245.55 828.104,-1245.27 831,-1245 1016.93,-1227.42 1066.18,-1227.08 1255.84,-1209.1\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1256.24,-1212.58 1265.87,-1208.15 1255.58,-1205.61 1256.24,-1212.58\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493957926416 -->\n",
       "<g class=\"node\" id=\"node21\"><title>140493957926416</title>\n",
       "<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 404,-1208.5 404,-1162.5 0,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71\" y=\"-1181.8\">ca3: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"142,-1162.5 142,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"142,-1185.5 197,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"197,-1162.5 197,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-1193.3\">[(None, 1, 256), (None, 125, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"197,-1185.5 404,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300.5\" y=\"-1170.3\">[(None, 125, 256), (1, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493960597800&#45;&gt;140493957926416 -->\n",
       "<g class=\"edge\" id=\"edge24\"><title>140493960597800-&gt;140493957926416</title>\n",
       "<path d=\"M503.42,-1245.47C448.079,-1234.65 381.28,-1221.58 324.645,-1210.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"325.044,-1207.01 314.558,-1208.52 323.7,-1213.88 325.044,-1207.01\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493957192896 -->\n",
       "<g class=\"node\" id=\"node22\"><title>140493957192896</title>\n",
       "<polygon fill=\"none\" points=\"422,-1162.5 422,-1208.5 826,-1208.5 826,-1162.5 422,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493\" y=\"-1181.8\">ca4: CrossAttention_gl</text>\n",
       "<polyline fill=\"none\" points=\"564,-1162.5 564,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"591.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"564,-1185.5 619,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"591.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"619,-1162.5 619,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"722.5\" y=\"-1193.3\">[(None, 1, 256), (None, 125, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"619,-1185.5 826,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"722.5\" y=\"-1170.3\">[(None, 125, 256), (1, 125), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140493960597800&#45;&gt;140493957192896 -->\n",
       "<g class=\"edge\" id=\"edge26\"><title>140493960597800-&gt;140493957192896</title>\n",
       "<path d=\"M618.186,-1245.37C618.997,-1237.15 619.935,-1227.66 620.817,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"624.317,-1218.9 621.817,-1208.61 617.351,-1218.21 624.317,-1218.9\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961194744&#45;&gt;140493959871680 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>140493961194744-&gt;140493959871680</title>\n",
       "<path d=\"M1051.09,-1245.37C1050.38,-1237.15 1049.56,-1227.66 1048.78,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1052.26,-1218.27 1047.91,-1208.61 1045.28,-1218.87 1052.26,-1218.27\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961194744&#45;&gt;140493959212616 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>140493961194744-&gt;140493959212616</title>\n",
       "<path d=\"M1165.85,-1245.47C1221.33,-1234.65 1288.29,-1221.58 1345.06,-1210.5\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1346.03,-1213.87 1355.17,-1208.52 1344.68,-1207 1346.03,-1213.87\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961194744&#45;&gt;140493957926416 -->\n",
       "<g class=\"edge\" id=\"edge25\"><title>140493961194744-&gt;140493957926416</title>\n",
       "<path d=\"M840.324,-1245.88C837.195,-1245.58 834.086,-1245.29 831,-1245 648.612,-1227.9 600.292,-1226.92 414.255,-1209.11\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"414.424,-1205.61 404.135,-1208.14 413.755,-1212.58 414.424,-1205.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493961194744&#45;&gt;140493957192896 -->\n",
       "<g class=\"edge\" id=\"edge27\"><title>140493961194744-&gt;140493957192896</title>\n",
       "<path d=\"M936.341,-1245.47C878.876,-1234.62 809.489,-1221.52 750.724,-1210.43\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"751.112,-1206.94 740.636,-1208.52 749.813,-1213.82 751.112,-1206.94\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493956386208 -->\n",
       "<g class=\"node\" id=\"node23\"><title>140493956386208</title>\n",
       "<polygon fill=\"none\" points=\"512,-1079.5 512,-1125.5 1158,-1125.5 1158,-1079.5 512,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"596\" y=\"-1098.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"680,-1079.5 680,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"707.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"680,-1102.5 735,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"707.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"735,-1079.5 735,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"946.5\" y=\"-1110.3\">[(None, 125, 256), (None, 125, 256), (None, 125, 256), (None, 125, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"735,-1102.5 1158,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"946.5\" y=\"-1087.3\">(None, 125, 1024)</text>\n",
       "</g>\n",
       "<!-- 140493959871680&#45;&gt;140493956386208 -->\n",
       "<g class=\"edge\" id=\"edge28\"><title>140493959871680-&gt;140493956386208</title>\n",
       "<path d=\"M988.622,-1162.47C961.877,-1152.21 929.881,-1139.92 902.03,-1129.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"902.957,-1125.84 892.367,-1125.52 900.448,-1132.37 902.957,-1125.84\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959212616&#45;&gt;140493956386208 -->\n",
       "<g class=\"edge\" id=\"edge29\"><title>140493959212616-&gt;140493956386208</title>\n",
       "<path d=\"M1295.87,-1162.47C1209.5,-1151.42 1104.88,-1138.03 1017.16,-1126.81\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1017.46,-1123.32 1007.1,-1125.52 1016.57,-1130.26 1017.46,-1123.32\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493957926416&#45;&gt;140493956386208 -->\n",
       "<g class=\"edge\" id=\"edge30\"><title>140493957926416-&gt;140493956386208</title>\n",
       "<path d=\"M374.134,-1162.47C460.5,-1151.42 565.12,-1138.03 652.836,-1126.81\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"653.426,-1130.26 662.9,-1125.52 652.537,-1123.32 653.426,-1130.26\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493957192896&#45;&gt;140493956386208 -->\n",
       "<g class=\"edge\" id=\"edge31\"><title>140493957192896-&gt;140493956386208</title>\n",
       "<path d=\"M681.378,-1162.47C708.123,-1152.21 740.119,-1139.92 767.97,-1129.23\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"769.552,-1132.37 777.633,-1125.52 767.043,-1125.84 769.552,-1132.37\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959494680 -->\n",
       "<g class=\"node\" id=\"node24\"><title>140493959494680</title>\n",
       "<polygon fill=\"none\" points=\"686.5,-996.5 686.5,-1042.5 983.5,-1042.5 983.5,-996.5 686.5,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-1015.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"811.5,-996.5 811.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"811.5,-1019.5 866.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"866.5,-996.5 866.5,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-1027.3\">(None, 125, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"866.5,-1019.5 983.5,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-1004.3\">(None, 125, 1024)</text>\n",
       "</g>\n",
       "<!-- 140493956386208&#45;&gt;140493959494680 -->\n",
       "<g class=\"edge\" id=\"edge32\"><title>140493956386208-&gt;140493959494680</title>\n",
       "<path d=\"M835,-1079.37C835,-1071.15 835,-1061.66 835,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-1052.61 835,-1042.61 831.5,-1052.61 838.5,-1052.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959494736 -->\n",
       "<g class=\"node\" id=\"node25\"><title>140493959494736</title>\n",
       "<polygon fill=\"none\" points=\"619,-913.5 619,-959.5 1051,-959.5 1051,-913.5 619,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-932.8\">batch_normalization_4: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"879,-913.5 879,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"906.5\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"879,-936.5 934,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"906.5\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"934,-913.5 934,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-944.3\">(None, 125, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"934,-936.5 1051,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-921.3\">(None, 125, 1024)</text>\n",
       "</g>\n",
       "<!-- 140493959494680&#45;&gt;140493959494736 -->\n",
       "<g class=\"edge\" id=\"edge33\"><title>140493959494680-&gt;140493959494736</title>\n",
       "<path d=\"M835,-996.366C835,-988.152 835,-978.658 835,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-969.607 835,-959.607 831.5,-969.607 838.5,-969.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493959493784 -->\n",
       "<g class=\"node\" id=\"node26\"><title>140493959493784</title>\n",
       "<polygon fill=\"none\" points=\"687.5,-830.5 687.5,-876.5 982.5,-876.5 982.5,-830.5 687.5,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-849.8\">conv1d_4: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"810.5,-830.5 810.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"810.5,-853.5 865.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-830.5 865.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-861.3\">(None, 125, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-853.5 982.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-838.3\">(None, 63, 256)</text>\n",
       "</g>\n",
       "<!-- 140493959494736&#45;&gt;140493959493784 -->\n",
       "<g class=\"edge\" id=\"edge34\"><title>140493959494736-&gt;140493959493784</title>\n",
       "<path d=\"M835,-913.366C835,-905.152 835,-895.658 835,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-886.607 835,-876.607 831.5,-886.607 838.5,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493955263624 -->\n",
       "<g class=\"node\" id=\"node27\"><title>140493955263624</title>\n",
       "<polygon fill=\"none\" points=\"694,-747.5 694,-793.5 976,-793.5 976,-747.5 694,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"755.5\" y=\"-766.8\">conv1d_5: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"817,-747.5 817,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"817,-770.5 872,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"872,-747.5 872,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-778.3\">(None, 63, 256)</text>\n",
       "<polyline fill=\"none\" points=\"872,-770.5 976,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-755.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 140493959493784&#45;&gt;140493955263624 -->\n",
       "<g class=\"edge\" id=\"edge35\"><title>140493959493784-&gt;140493955263624</title>\n",
       "<path d=\"M835,-830.366C835,-822.152 835,-812.658 835,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-803.607 835,-793.607 831.5,-803.607 838.5,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493954645408 -->\n",
       "<g class=\"node\" id=\"node28\"><title>140493954645408</title>\n",
       "<polygon fill=\"none\" points=\"694,-664.5 694,-710.5 976,-710.5 976,-664.5 694,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"755.5\" y=\"-683.8\">conv1d_6: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"817,-664.5 817,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"817,-687.5 872,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"872,-664.5 872,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-695.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"872,-687.5 976,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-672.3\">(None, 16, 256)</text>\n",
       "</g>\n",
       "<!-- 140493955263624&#45;&gt;140493954645408 -->\n",
       "<g class=\"edge\" id=\"edge36\"><title>140493955263624-&gt;140493954645408</title>\n",
       "<path d=\"M835,-747.366C835,-739.152 835,-729.658 835,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-720.607 835,-710.607 831.5,-720.607 838.5,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493954356448 -->\n",
       "<g class=\"node\" id=\"node29\"><title>140493954356448</title>\n",
       "<polygon fill=\"none\" points=\"694,-581.5 694,-627.5 976,-627.5 976,-581.5 694,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"755.5\" y=\"-600.8\">conv1d_7: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"817,-581.5 817,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"817,-604.5 872,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"872,-581.5 872,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-612.3\">(None, 16, 256)</text>\n",
       "<polyline fill=\"none\" points=\"872,-604.5 976,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924\" y=\"-589.3\">(None, 8, 256)</text>\n",
       "</g>\n",
       "<!-- 140493954645408&#45;&gt;140493954356448 -->\n",
       "<g class=\"edge\" id=\"edge37\"><title>140493954645408-&gt;140493954356448</title>\n",
       "<path d=\"M835,-664.366C835,-656.152 835,-646.658 835,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-637.607 835,-627.607 831.5,-637.607 838.5,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493953963120 -->\n",
       "<g class=\"node\" id=\"node30\"><title>140493953963120</title>\n",
       "<polygon fill=\"none\" points=\"696.5,-498.5 696.5,-544.5 973.5,-544.5 973.5,-498.5 696.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"759\" y=\"-517.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"821.5,-498.5 821.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"849\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"821.5,-521.5 876.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"849\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"876.5,-498.5 876.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-529.3\">(None, 8, 256)</text>\n",
       "<polyline fill=\"none\" points=\"876.5,-521.5 973.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-506.3\">(None, 8, 256)</text>\n",
       "</g>\n",
       "<!-- 140493954356448&#45;&gt;140493953963120 -->\n",
       "<g class=\"edge\" id=\"edge38\"><title>140493954356448-&gt;140493953963120</title>\n",
       "<path d=\"M835,-581.366C835,-573.152 835,-563.658 835,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-554.607 835,-544.607 831.5,-554.607 838.5,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493954046216 -->\n",
       "<g class=\"node\" id=\"node31\"><title>140493954046216</title>\n",
       "<polygon fill=\"none\" points=\"629,-415.5 629,-461.5 1041,-461.5 1041,-415.5 629,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"759\" y=\"-434.8\">batch_normalization_5: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"889,-415.5 889,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"889,-438.5 944,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"944,-415.5 944,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-446.3\">(None, 8, 256)</text>\n",
       "<polyline fill=\"none\" points=\"944,-438.5 1041,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-423.3\">(None, 8, 256)</text>\n",
       "</g>\n",
       "<!-- 140493953963120&#45;&gt;140493954046216 -->\n",
       "<g class=\"edge\" id=\"edge39\"><title>140493953963120-&gt;140493954046216</title>\n",
       "<path d=\"M835,-498.366C835,-490.152 835,-480.658 835,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-471.607 835,-461.607 831.5,-471.607 838.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493953965472 -->\n",
       "<g class=\"node\" id=\"node32\"><title>140493953965472</title>\n",
       "<polygon fill=\"none\" points=\"597.5,-332.5 597.5,-378.5 1072.5,-378.5 1072.5,-332.5 597.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"759\" y=\"-351.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"920.5,-332.5 920.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"920.5,-355.5 975.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"975.5,-332.5 975.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1024\" y=\"-363.3\">(None, 8, 256)</text>\n",
       "<polyline fill=\"none\" points=\"975.5,-355.5 1072.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1024\" y=\"-340.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140493954046216&#45;&gt;140493953965472 -->\n",
       "<g class=\"edge\" id=\"edge40\"><title>140493954046216-&gt;140493953965472</title>\n",
       "<path d=\"M835,-415.366C835,-407.152 835,-397.658 835,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-388.607 835,-378.607 831.5,-388.607 838.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493953651320 -->\n",
       "<g class=\"node\" id=\"node33\"><title>140493953651320</title>\n",
       "<polygon fill=\"none\" points=\"715,-249.5 715,-295.5 955,-295.5 955,-249.5 715,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-268.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"817,-249.5 817,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"817,-272.5 872,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"844.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"872,-249.5 872,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-280.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"872,-272.5 955,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140493953965472&#45;&gt;140493953651320 -->\n",
       "<g class=\"edge\" id=\"edge41\"><title>140493953965472-&gt;140493953651320</title>\n",
       "<path d=\"M835,-332.366C835,-324.152 835,-314.658 835,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-305.607 835,-295.607 831.5,-305.607 838.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493953626064 -->\n",
       "<g class=\"node\" id=\"node34\"><title>140493953626064</title>\n",
       "<polygon fill=\"none\" points=\"703.5,-166.5 703.5,-212.5 966.5,-212.5 966.5,-166.5 703.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-185.8\">dropout_6: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"828.5,-166.5 828.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"856\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"828.5,-189.5 883.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"856\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"883.5,-166.5 883.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"883.5,-189.5 966.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"925\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140493953651320&#45;&gt;140493953626064 -->\n",
       "<g class=\"edge\" id=\"edge42\"><title>140493953651320-&gt;140493953626064</title>\n",
       "<path d=\"M835,-249.366C835,-241.152 835,-231.658 835,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-222.607 835,-212.607 831.5,-222.607 838.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493953623376 -->\n",
       "<g class=\"node\" id=\"node35\"><title>140493953623376</title>\n",
       "<polygon fill=\"none\" points=\"636,-83.5 636,-129.5 1034,-129.5 1034,-83.5 636,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-102.8\">batch_normalization_6: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"896,-83.5 896,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"896,-106.5 951,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"923.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"951,-83.5 951,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"951,-106.5 1034,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"992.5\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140493953626064&#45;&gt;140493953623376 -->\n",
       "<g class=\"edge\" id=\"edge43\"><title>140493953626064-&gt;140493953623376</title>\n",
       "<path d=\"M835,-166.366C835,-158.152 835,-148.658 835,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-139.607 835,-129.607 831.5,-139.607 838.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140493952958760 -->\n",
       "<g class=\"node\" id=\"node36\"><title>140493952958760</title>\n",
       "<polygon fill=\"none\" points=\"673.5,-0.5 673.5,-46.5 996.5,-46.5 996.5,-0.5 673.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"766\" y=\"-19.8\">output_headline_vector: Dense</text>\n",
       "<polyline fill=\"none\" points=\"858.5,-0.5 858.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"886\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"858.5,-23.5 913.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"886\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"913.5,-0.5 913.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"913.5,-23.5 996.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"955\" y=\"-8.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140493953623376&#45;&gt;140493952958760 -->\n",
       "<g class=\"edge\" id=\"edge44\"><title>140493953623376-&gt;140493952958760</title>\n",
       "<path d=\"M835,-83.3664C835,-75.1516 835,-65.6579 835,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"838.5,-56.6068 835,-46.6068 831.5,-56.6069 838.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp_sentence_vectors = Input(shape=(max_sentences, 300), name='sentence_vectors')\n",
    "    inp_headline_vector = Input(shape=(300,), name='input_headline_vector')\n",
    "    conv1 = Conv1D(filters=16,kernel_size=3,strides=1,activation='relu', padding='same')(inp_sentence_vectors)\n",
    "    conv1 = Dropout(0.5)(conv1)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=3,strides=1,activation='relu', padding='same')(conv1)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    sent_sa_feat_1, sent_beta_1, sent_gamma_1 = SelfAttention(int(conv2.shape[-1]), name = 'sa1')(conv2)\n",
    "    sent_sa_feat_2, sent_beta_2, sent_gamma_2 = SelfAttention(int(conv2.shape[-1]), name = 'sa2')(conv2)\n",
    "    sent_sa_feat_3, sent_beta_3, sent_gamma_3 = SelfAttention(int(conv2.shape[-1]), name = 'sa3')(conv2)\n",
    "    sent_sa_feat_4, sent_beta_4, sent_gamma_4 = SelfAttention(int(conv2.shape[-1]), name = 'sa4')(conv2)\n",
    "    concat1 = Concatenate()([sent_sa_feat_1,sent_sa_feat_2,sent_sa_feat_3,sent_sa_feat_4])\n",
    "    conv3 = Conv1D(filters=256,kernel_size=3, strides=1, activation='relu', padding='same')(concat1)\n",
    "    conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    headline = Dense(256, activation='relu')(inp_headline_vector)\n",
    "    headline = Lambda(lambda x:K.expand_dims(x, axis=1))(headline)\n",
    "    headline = BatchNormalization()(headline)\n",
    "    sent_hd_sa_feat_1, sent_hd_beta_1, sent_hd_gamma_1 = CrossAttention(int(conv3.shape[-1]), name = 'ca1')([headline,conv3])\n",
    "    sent_hd_sa_feat_2, sent_hd_beta_2, sent_hd_gamma_2 = CrossAttention(int(conv3.shape[-1]), name = 'ca2')([headline,conv3])\n",
    "    sent_hd_sa_feat_3, sent_hd_beta_3, sent_hd_gamma_3 = CrossAttention(int(conv3.shape[-1]), name = 'ca3')([headline,conv3])\n",
    "    sent_hd_sa_feat_4, sent_hd_beta_4, sent_hd_gamma_4 = CrossAttention(int(conv3.shape[-1]), name = 'ca4')([headline,conv3])  \n",
    "    concat3 = Concatenate()([sent_hd_sa_feat_1,sent_hd_sa_feat_2,sent_hd_sa_feat_3,sent_hd_sa_feat_4])\n",
    "    concat3 = Dropout(0.5)(concat3)\n",
    "    concat3 = BatchNormalization()(concat3)\n",
    "    conv5 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(concat3)\n",
    "    conv6 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "    conv7 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv6)\n",
    "    conv8 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv7)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    gap = GlobalAveragePooling1D()(conv8)\n",
    "#     repeat = RepeatVector(50)(gap)\n",
    "#     lstm = LSTM(256,return_sequences=True)(repeat)\n",
    "    dense1 = Dense(512,activation='relu')(gap)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    gen_hd_vector = Dense(300,activation='linear', name='output_headline_vector')(dense1)\n",
    "    model = Model([inp_sentence_vectors,inp_headline_vector],gen_hd_vector)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001,beta_1=0.0,beta_2=0.99),loss='mse')\n",
    "model.summary()\n",
    "# print('model params:',model.count_params())\n",
    "SVG(model_to_dot(model,show_layer_names=True,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now()\n",
    "mc = ModelCheckpoint('weights/dnf700_sa_sent_hd_vector_gl.hdf5',save_best_only=True,save_weights_only=True)\n",
    "tb = TensorBoard(batch_size=32,log_dir='logs/dnf700_sa_sent_hd_vector_gl/{0}'.format(dt.timestamp()),write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0340 - val_loss: 0.0356\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0337 - val_loss: 0.0307\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0339 - val_loss: 0.0303\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 1s 345ms/step - loss: 0.0342 - val_loss: 0.0305\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 0.0337 - val_loss: 0.0348\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0321 - val_loss: 0.0316\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0332 - val_loss: 0.0315\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0315 - val_loss: 0.0323\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0313 - val_loss: 0.0311\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0315 - val_loss: 0.0303\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0304 - val_loss: 0.0320\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 3s 849ms/step - loss: 0.0300 - val_loss: 0.0303\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0294 - val_loss: 0.0308\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0287 - val_loss: 0.0284\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0290 - val_loss: 0.0285\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0276 - val_loss: 0.0262\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0274 - val_loss: 0.0261\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 3s 802ms/step - loss: 0.0269 - val_loss: 0.0265\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 3s 861ms/step - loss: 0.0271 - val_loss: 0.0283\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 3s 812ms/step - loss: 0.0258 - val_loss: 0.0250\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0274 - val_loss: 0.0279\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 3s 859ms/step - loss: 0.0249 - val_loss: 0.0273\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 4s 929ms/step - loss: 0.0267 - val_loss: 0.0275\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 3s 838ms/step - loss: 0.0255 - val_loss: 0.0261\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 3s 850ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 3s 857ms/step - loss: 0.0241 - val_loss: 0.0243\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0250 - val_loss: 0.0222\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.0234 - val_loss: 0.0250\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0226 - val_loss: 0.0221\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0244 - val_loss: 0.0260\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0232 - val_loss: 0.0222\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 2s 585ms/step - loss: 0.0234 - val_loss: 0.0233\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0224 - val_loss: 0.0230\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 0.0222 - val_loss: 0.0226\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0216 - val_loss: 0.0227\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0220 - val_loss: 0.0215\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0213 - val_loss: 0.0214\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0205 - val_loss: 0.0202\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0204 - val_loss: 0.0222\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0206 - val_loss: 0.0217\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.0216 - val_loss: 0.0199\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0209 - val_loss: 0.0200\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0217 - val_loss: 0.0196\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0200 - val_loss: 0.0187\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0205 - val_loss: 0.0199\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0185 - val_loss: 0.0198\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0202 - val_loss: 0.0205\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0188 - val_loss: 0.0177\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 3s 797ms/step - loss: 0.0177 - val_loss: 0.0180\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0181 - val_loss: 0.0175\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 3s 786ms/step - loss: 0.0171 - val_loss: 0.0200\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 2s 622ms/step - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.0157 - val_loss: 0.0166\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0167 - val_loss: 0.0160\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0161 - val_loss: 0.0163\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0168 - val_loss: 0.0146\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0173 - val_loss: 0.0158\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0162 - val_loss: 0.0152\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0155 - val_loss: 0.0144\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0146 - val_loss: 0.0142\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0147 - val_loss: 0.0154\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 2s 560ms/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0138 - val_loss: 0.0132\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0141 - val_loss: 0.0129\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 2s 594ms/step - loss: 0.0139 - val_loss: 0.0127\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 3s 794ms/step - loss: 0.0137 - val_loss: 0.0128\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0143 - val_loss: 0.0129\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 3s 778ms/step - loss: 0.0141 - val_loss: 0.0127\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 3s 788ms/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 82/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 829ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 3s 792ms/step - loss: 0.0133 - val_loss: 0.0136\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 0.0127 - val_loss: 0.0125\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0129 - val_loss: 0.0120\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 4s 910ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0124 - val_loss: 0.0145\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.0118 - val_loss: 0.0135\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0128 - val_loss: 0.0114\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0123 - val_loss: 0.0107\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 3s 789ms/step - loss: 0.0114 - val_loss: 0.0132\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0128 - val_loss: 0.0118\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 3s 853ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.0124 - val_loss: 0.0118\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 3s 868ms/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 4s 968ms/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 3s 863ms/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 3s 828ms/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 3s 781ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 2s 605ms/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 3s 786ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0117 - val_loss: 0.0097\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0100 - val_loss: 0.0117\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0122 - val_loss: 0.0109\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0128 - val_loss: 0.0115\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0095 - val_loss: 0.0112\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0121 - val_loss: 0.0109\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0117 - val_loss: 0.0120\n",
      "Epoch 162/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 3s 797ms/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 3s 874ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0127 - val_loss: 0.0105\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 2s 622ms/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0097 - val_loss: 0.0111\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 3s 797ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 3s 821ms/step - loss: 0.0126 - val_loss: 0.0092\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.0112 - val_loss: 0.0133\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 2s 553ms/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0103 - val_loss: 0.0129\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 3s 775ms/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 3s 840ms/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 3s 824ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0119 - val_loss: 0.0095\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 3s 793ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.0120 - val_loss: 0.0098\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 4s 914ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 3s 794ms/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0123 - val_loss: 0.0111\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 3s 844ms/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 4s 994ms/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0122 - val_loss: 0.0107\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0089\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0094\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 242/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 1s/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0122 - val_loss: 0.0114\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0122 - val_loss: 0.0106\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 4s 960ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0120 - val_loss: 0.0110\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0096\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0126 - val_loss: 0.0102\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0103\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0146 - val_loss: 0.0114\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 322/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0129\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0093\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0103\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0099\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0121\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0117\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0100\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0118\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0098\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0125 - val_loss: 0.0109\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0136\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0125 - val_loss: 0.0102\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0122\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 407/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0092\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0129 - val_loss: 0.0118\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0130 - val_loss: 0.0097\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0092 - val_loss: 0.0109\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0136\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0126\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0127 - val_loss: 0.0113\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0119\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0089\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0103\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0134 - val_loss: 0.0114\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0101\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0124\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0128 - val_loss: 0.0103\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0091\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0126\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0122 - val_loss: 0.0105\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0125 - val_loss: 0.0100\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0115\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0121\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0116\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0104\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 572/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0129\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0094\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0098\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0116\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0126 - val_loss: 0.0093\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0127\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0130\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0097 - val_loss: 0.0109\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0121\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0113\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0131 - val_loss: 0.0107\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 655/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0103\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0104\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0094\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0097 - val_loss: 0.0104\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0129 - val_loss: 0.0114\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0124\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0132\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0098\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0141\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0102\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0122\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 737/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0116\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0090\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0105\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0109\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0093\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0092\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0118\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0116\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0140\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0125\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0098\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0093\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0116\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0127\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0117\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 820/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0119\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0102\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0096 - val_loss: 0.0100\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0131\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0129 - val_loss: 0.0108\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0112\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0105\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 902/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0126 - val_loss: 0.0103\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0113\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0090\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0096\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0127 - val_loss: 0.0113\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0131\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0096\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0095\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0115\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0099\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0128 - val_loss: 0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 985/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0124 - val_loss: 0.0122\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0094 - val_loss: 0.0110\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0116\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0098\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0134\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0121\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0129 - val_loss: 0.0114\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0123 - val_loss: 0.0105\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0115\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0123 - val_loss: 0.0112\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0120\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0096 - val_loss: 0.0113\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0095\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0115\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0126\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0129 - val_loss: 0.0107\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0103\n",
      "Epoch 1067/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0112\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0113\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0127\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0110\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0099\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0108\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0098 - val_loss: 0.0098\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0117\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0114\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 1149/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0125 - val_loss: 0.0118\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0111\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0120 - val_loss: 0.0106\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0115\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0098\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0104\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0127 - val_loss: 0.0113\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0126 - val_loss: 0.0105\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0089\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0107\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0124\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0102\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0095\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0108\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1231/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0127 - val_loss: 0.0113\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0124\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0109\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0126 - val_loss: 0.0108\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0115\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0129\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0106\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0109\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0126 - val_loss: 0.0105\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0092\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0130\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0112\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0097 - val_loss: 0.0100\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1310/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 1313/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0119\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0114\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0104\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0109\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0119\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0133 - val_loss: 0.0108\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0130\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0124 - val_loss: 0.0105\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0111\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0088\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0097\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0127\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0117\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0129 - val_loss: 0.0122\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1395/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0101\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0095\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0092\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0095\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0133 - val_loss: 0.0124\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0127 - val_loss: 0.0102\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0093\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0126 - val_loss: 0.0115\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0102\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0095\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0126 - val_loss: 0.0106\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0090\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0096\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 1477/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0127\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0124 - val_loss: 0.0098\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0094\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0099\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0089\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0098\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0103\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0090\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0090\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0125\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0125 - val_loss: 0.0103\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 1559/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0134\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0119 - val_loss: 0.0110\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0130 - val_loss: 0.0103\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0140\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0091\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0127 - val_loss: 0.0115\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0127\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0125 - val_loss: 0.0109\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0103\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0131\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0121 - val_loss: 0.0098\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0117\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0099\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0099 - val_loss: 0.0117\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1641/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0124 - val_loss: 0.0101\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0125 - val_loss: 0.0099\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0093\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0108\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0118 - val_loss: 0.0111\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0101 - val_loss: 0.0129\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0127 - val_loss: 0.0102\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0121 - val_loss: 0.0117\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1723/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0125\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0097\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0134 - val_loss: 0.0121\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0117 - val_loss: 0.0102\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0122 - val_loss: 0.0113\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0125 - val_loss: 0.0116\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0137\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0131\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0100 - val_loss: 0.0116\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0096 - val_loss: 0.0108\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0122 - val_loss: 0.0143\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0111 - val_loss: 0.0087\n",
      "Epoch 1805/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0093\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0132\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0121 - val_loss: 0.0102\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0115 - val_loss: 0.0120\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0117 - val_loss: 0.0093\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0124 - val_loss: 0.0117\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 3s 795ms/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0135\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0113 - val_loss: 0.0133\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 4s 946ms/step - loss: 0.0124 - val_loss: 0.0103\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 4s 942ms/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0101\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0129 - val_loss: 0.0102\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 4s 990ms/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0115 - val_loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0130 - val_loss: 0.0103\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 4s 919ms/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0087\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0096 - val_loss: 0.0113\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0125 - val_loss: 0.0097\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 4s 985ms/step - loss: 0.0112 - val_loss: 0.0126\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0132 - val_loss: 0.0108\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0103 - val_loss: 0.0125\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0096\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 4s 947ms/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0135 - val_loss: 0.0105\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0090\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0101\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 4s 946ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0131\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 4s 983ms/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0129 - val_loss: 0.0119\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0131\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0128\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 4s 984ms/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 1968/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0128\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 4s 938ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 4s 939ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 4s 890ms/step - loss: 0.0122 - val_loss: 0.0107\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 4s 958ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 4s 983ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0104 - val_loss: 0.0122\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.0108 - val_loss: 0.0111\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(tdg, callbacks=[mc,tb], initial_epoch=0\n",
    "                           ,steps_per_epoch=train_steps_per_epoch\n",
    "                           ,validation_data=vdg\n",
    "                           ,validation_steps=val_steps_per_epoch\n",
    "                           ,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# acc = hist.history['acc']\n",
    "# loss = hist.history['loss']\n",
    "\n",
    "# # Create count of the number of epochs\n",
    "# epoch_count = range(1, len(acc) + 1)\n",
    "\n",
    "# # Visualize loss history\n",
    "# # plt.plot(epoch_count, acc, 'b-')\n",
    "# fig, ax = plt.subplots(ncols=2,sharex=True)\n",
    "# ax[0].plot(epoch_count, loss, 'r--')\n",
    "# ax[0].legend(['Loss'])\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].set_ylabel('Loss')\n",
    "# ax[1].plot(epoch_count, acc, 'b-')\n",
    "# ax[1].legend(['Accuracy'])\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].set_xlabel('Accuracy')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_res[['acc','val_acc']].plot()\n",
    "# hd_nlp = nlp(\"What is you name my name is Anthony Gonsalves What is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony Gonsalves!\".lower())\n",
    "# len(hd_nlp[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<generator..., use_multiprocessing=True, steps=5)`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010988671518862247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/dnf700_sa_sent_hd_vector_gl.hdf5')\n",
    "model.evaluate_generator(test_dg,steps=5,pickle_safe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(test_dg)\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trump accuses obama, hillary clinton of founding daesh',\n",
       "       'pentagon seeks another $6 billion for overseas troop deployments',\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'us officials try to scare voters with terror threat',\n",
       "       'nsa whistleblower says dnc email hack was not by russia, but by us intelligence | alternative',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'assange confirms: wikileaks didn’t get emails from russian govt',\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'us threatens military hacks on russia’s electric, communications grids over election',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'hillary personally ordered ‘donald duck’ troll campaign',\n",
       "       'fbi agent suspected in hillary email leaks found dead in apparent murder-suicide',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'developing: obama wh admits that hillary gave isis $400 million on accident',\n",
       "       'hillary sold weapons to isis, wikileaks confirms',\n",
       "       'former nato chief: we need us as ‘world’s policeman’',\n",
       "       'hillary personally ordered ‘donald duck’ troll campaign',\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'hillary sold weapons to isis, wikileaks confirms',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'us officials see no link between trump and russia',\n",
       "       'obama declares his family will move to canada if trump is elected',\n",
       "       'wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news',\n",
       "       'hillary friend bribed fbi agent and his wife',\n",
       "       'department of homeland security chairman officially indicts hillary clinton of treason',\n",
       "       'assange confirms: wikileaks didn’t get emails from russian govt',\n",
       "       'us officials see no link between trump and russia',\n",
       "       'trump accuses obama, hillary clinton of founding daesh',\n",
       "       'jill stein endorsed donald trump',\n",
       "       \"doj's loretta lynch tried to squash comey's letter to congress\",\n",
       "       'trump accuses obama, hillary clinton of founding daesh',\n",
       "       \"hillary clinton's 'sudden move' of $1.8 billion to qatar central bank stuns financial world\",\n",
       "       'hillary sold weapons to isis, wikileaks confirms',\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'wikileaks: hillary clinton knew saudi, qatar were funding isis – but still took their money for foundation',\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'kremlin: putin congratulates trump, hopes to work together major issues',\n",
       "       'pentagon seeks another $6 billion for overseas troop deployments',\n",
       "       'developing: obama wh admits that hillary gave isis $400 million on accident',\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       'hillary personally ordered ‘donald duck’ troll campaign',\n",
       "       'wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting',\n",
       "       'developing: obama wh admits that hillary gave isis $400 million on accident',\n",
       "       'hillary sold weapons to isis, wikileaks confirms',\n",
       "       'wikileaks: hillary clinton knew saudi, qatar were funding isis – but still took their money for foundation',\n",
       "       'wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting'],\n",
       "      dtype='<U106')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'former nato chief: we need us as ‘world’s policeman’'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = np.random.randint(0,50)\n",
    "display(x['headline'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.',\n",
       " 'Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.',\n",
       " 'Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.',\n",
       " 'This of course closely mirrors recent Pentagon talk of wars in the decades to come.',\n",
       " 'The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.',\n",
       " 'Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['sentences'][test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 125, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 125, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 125, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 125, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 125, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 125, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 125, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 125, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 125, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 125, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 125, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 125, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 125, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 125, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 125, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 125, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 125, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 125, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 125, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 125, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 125, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 125, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 125, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 125, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 125, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 125, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention_gl)          [(None, 125, 32), (1 2378        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 125, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 125, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 125, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 125, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention_gl)         [(None, 125, 256), ( 148034      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,322\n",
      "Trainable params: 350,234\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(model.inputs,model.get_layer(name='ca1').output)\n",
    "model_2 = Model(model.inputs,model.get_layer(name='ca2').output)\n",
    "model_3 = Model(model.inputs,model.get_layer(name='ca3').output)\n",
    "model_4 = Model(model.inputs,model.get_layer(name='ca4').output)\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b1+b2+b3+b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_N = 5\n",
    "t = b[test_idx][0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "b[test_idx][0][:len(x['sentences'][test_idx])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'former nato chief: we need us as ‘world’s policeman’'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.',\n",
       " 'Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.',\n",
       " 'The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.',\n",
       " 'Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x['headline'][test_idx])\n",
    "display(x['claims'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 : Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”\n",
      "4 : The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.\n",
      "3 : This of course closely mirrors recent Pentagon talk of wars in the decades to come.\n",
      "2 : Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.\n",
      "1 : Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.\n"
     ]
    }
   ],
   "source": [
    "for s in t:\n",
    "    if s>=len(x['sentences'][test_idx]):continue\n",
    "    print(s,':',x['sentences'][test_idx][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "h_s_attended_vector = b[test_idx][0][:len(x['sentences'][test_idx])]\n",
    "h_s_attended_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s_attended_vector = pd.DataFrame(h_s_attended_vector)\n",
    "\n",
    "\n",
    "xw = df_h_s_attended_vector.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xw_scaled = min_max_scaler.fit_transform(xw)\n",
    "df_h_s_attended_vector = pd.DataFrame(xw_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc6cf725ac8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(2.0,7.0)})\n",
    "sns.heatmap(df_h_s_attended_vector, annot=True, cmap='YlGnBu', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 125, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 125, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa1 (SelfAttention_gl)       [(None, 125, 32), (125, 1 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 125, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 125, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa2 (SelfAttention_gl)       [(None, 125, 32), (125, 1 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 125, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 125, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa3 (SelfAttention_gl)       [(None, 125, 32), (125, 1 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 125, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 125, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa4 (SelfAttention_gl)       [(None, 125, 32), (125, 1 2378      \n",
      "=================================================================\n",
      "Total params: 18,490\n",
      "Trainable params: 18,426\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s1 = Model(model.inputs,model.get_layer(name='sa1').output)\n",
    "model_s2 = Model(model.inputs,model.get_layer(name='sa2').output)\n",
    "model_s3 = Model(model.inputs,model.get_layer(name='sa3').output)\n",
    "model_s4 = Model(model.inputs,model.get_layer(name='sa4').output)\n",
    "model_s1.summary()\n",
    "model_s2.summary()\n",
    "model_s3.summary()\n",
    "model_s4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sb1, sg1 = model_s1.predict([x['sentence_vectors'],x['input_headline_vector']])\n",
    "_, sb2, sg2 = model_s2.predict(x)\n",
    "_, sb3, sg3 = model_s3.predict(x)\n",
    "_, sb4, sg4 = model_s4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = sb1[test_idx]#+sb2[test_idx]+sb3[test_idx]+sb4[test_idx]\n",
    "sb = sb[:len(x['sentences'][test_idx]),:len(x['sentences'][test_idx])]\n",
    "\n",
    "sb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = pd.DataFrame(sb)\n",
    "\n",
    "\n",
    "# zx = df_sb.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# zx_scaled = min_max_scaler.fit_transform(zx)\n",
    "# df_sb = pd.DataFrame(zx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc6cf771438>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(14.0,12.0)})\n",
    "sns.heatmap(pd.DataFrame(df_sb),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This of course closely mirrors recent Pentagon talk of wars in the decades to come.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                            0\n",
       "0  In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.                     \n",
       "1  Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.\n",
       "2  Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.                                       \n",
       "3  This of course closely mirrors recent Pentagon talk of wars in the decades to come.                                                                                                                                                                                       \n",
       "4  The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.                                 \n",
       "5  Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”                                                                                                                        "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x['sentences'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
