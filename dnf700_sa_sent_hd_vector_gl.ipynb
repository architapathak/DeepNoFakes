{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from spacy.lang.en import English\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.layers import BatchNormalization, Lambda, Concatenate, Dropout, Conv1D, MaxPooling1D, Input, TimeDistributed, Dense, LSTM, RepeatVector, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from AttentionModules import SelfAttention, CrossAttention\n",
    "import sys,os\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['authors', 'claim_ids', 'evidence', 'headline', 'id', 'reason',\n",
       "        'claims', 'type', 'urls'],\n",
       "       dtype='object'),\n",
       " Index(['authors', 'headline', 'id', 'type', 'urls'], dtype='object'),\n",
       " 705,\n",
       " 705)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnf700 = pd.read_json('evaluation_set/deepnofakes/dnf_700/initial.json')\n",
    "dnf_eval = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V4.json')\n",
    "dnf_eval.columns = ['authors','claim_ids', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls'] \n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_array_id.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_array_id.p', 'rb') as fp:\n",
    "    articles300 = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors300 = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "dnf_eval.keys(), dnf700.keys(), len(articles.keys()), len(article_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits)\n",
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "test_batchsize = 50\n",
    "train_steps_per_epoch = 4\n",
    "val_steps_per_epoch = 1\n",
    "epochs = 2000\n",
    "max_sentences = 500\n",
    "# for idx in articles.keys():\n",
    "#     num = len(articles[idx])\n",
    "#     if num>=max_sentences:\n",
    "#         max_sentences = num\n",
    "        \n",
    "max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [authors, headline, id, type, urls]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdl = \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\"\n",
    "hdl = \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\"\n",
    "# hdl = \"If You Thought The Trump Child Rape Case In NY Couldn’t Get Much Worse — You Were Wrong\"\n",
    "# hdl = \"California Set to Let Public Schools Teach Primarily in Spanish\"\n",
    "dnf700[dnf700.headline==hdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sorted(dnf700.headline.unique())\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(VIDEO) Female College Students Protesting Because ‘Trump is a Rapist’',\n",
       " 'Assange Confirms: WikiLeaks Didn’t Get Emails From Russian Govt',\n",
       " 'BREAKING: Fraudulent Clinton Votes Discovered By The Tens Of Thousands',\n",
       " \"Clinton Camp Demands 'Compliant Citizenry' for Master Plan\",\n",
       " 'Clinton Received Debate Questions Week Before Debate',\n",
       " \"DOJ's Loretta Lynch Tried To Squash Comey's Letter To Congress\",\n",
       " 'Department of Homeland Security Chairman Officially Indicts Hillary Clinton of Treason',\n",
       " 'Developing: Obama WH admits that Hillary gave ISIS $400 million on accident',\n",
       " 'Erdoğan: US, the founder of ISIS',\n",
       " \"FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Healthcare Begins With A Bombshell! » 100percentfedUp.com\",\n",
       " 'FBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent Murder-Suicide',\n",
       " 'FBI Director Comey’s ‘Leaked’ Memo Explains Why He’s Reopening the Clinton Email Case',\n",
       " 'FBI director received millions from Clinton Foundation, his brother’s law firm does Clinton’s taxes',\n",
       " 'Former NATO Chief: We Need US as ‘World’s Policeman’',\n",
       " \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\",\n",
       " 'HE’S NEVER SOLD AN ORIGINAL PAINTING UNTIL NOW…And This One’s Going In The White House',\n",
       " 'HILLARY’S (Islamic) AMERICA IS ALREADY HERE where ‘Muslim NO-GO ZONES’ are popping up all over Michiganistan',\n",
       " \"Hillary Clinton Cut Her Tax Bill by 'Donating' $1 Million to Herself via the Clinton Foundation?\",\n",
       " 'Hillary Clinton Used Hand Signals to Rig Debate?',\n",
       " \"Hillary Clinton Wore 'Secret Earpiece' During Commander-in-Chief Forum\",\n",
       " 'Hillary Clinton Wore Secret Earpiece During First Presidential Debate?',\n",
       " \"Hillary Clinton in 2013: 'I Would Like to See People Like Donald Trump Run for Office\",\n",
       " \"Hillary Clinton's 'Sudden Move' of $1.8 Billion to Qatar Central Bank Stuns Financial World\",\n",
       " 'Hillary Clinton’s Sudden Move Of $1.8 Billion To Qatar Central Bank Stuns Financial World',\n",
       " 'Hillary Friend Bribed FBI Agent and His Wife',\n",
       " 'Hillary Personally Ordered ‘Donald Duck’ Troll Campaign',\n",
       " 'Hillary Sold Weapons To ISIS, Wikileaks Confirms',\n",
       " 'ISIS Leader Calls for American Muslim Voters to Support Hillary Clinton',\n",
       " 'Jill Stein Endorsed Donald Trump',\n",
       " 'Julian Assange Makes VERY Suspect Post Election Announcement, Seeks Pardon From Trump',\n",
       " 'KREMLIN: Putin Congratulates Trump, Hopes to Work Together Major Issues',\n",
       " 'LOL! BRITISH WIFE Of LIB ACTOR Who Said: There Will Never Be A President Donald Trump…Warns Americans About President-Elect Trump [VIDEO]',\n",
       " 'Leaked 2013 Trump Tax Return Shows He Paid Over 40 Million in Taxes',\n",
       " 'NSA Whistleblower Says DNC Email Hack Was Not by Russia, but by US Intelligence | Alternative',\n",
       " 'Obama Declares His Family Will Move to Canada If Trump Is Elected',\n",
       " 'Pentagon Officials Furious After Clinton Announces US Response Time for Nuclear Launch During Debate',\n",
       " 'Pentagon Seeks Another $6 Billion for Overseas Troop Deployments',\n",
       " \"Physician Confirms Hillary Clinton Has Parkinson's Disease\",\n",
       " 'President Obama Confirms He Will Refuse to Leave Office If Trump Is Elected',\n",
       " 'Reddit Users Declare War On Hillary’s Paid Internet Trolls',\n",
       " \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\",\n",
       " 'The Clinton Foundation has purchased over $137 million of illegal arms and ammunition',\n",
       " 'Top aide: Hillary ‘still not perfect in her head’, Wikileaks',\n",
       " 'Trump accuses Obama, Hillary Clinton of founding Daesh',\n",
       " 'US Officials See No Link Between Trump and Russia',\n",
       " 'US Officials Try to Scare Voters With Terror Threat',\n",
       " 'US Threatens Military Hacks on Russia’s Electric, Communications Grids Over Election',\n",
       " 'WIKILEAKS: Hillary Got $12 Million for Clinton Charity As Quid Pro Quo For Morocco Meeting',\n",
       " 'WikiLeaks CONFIRMS Hillary Sold Weapons to ISIS... Then Drops Another BOMBSHELL! Breaking News',\n",
       " 'WikiLeaks: Hillary Clinton knew Saudi, Qatar were funding ISIS – but still took their money for Foundation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_titles = sorted(dnf_eval.headline.unique())\n",
    "len(test_titles)\n",
    "test_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_test_titles = np.array(list(set(titles)-set(test_titles)))\n",
    "len(non_test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for train_index, val_index in kf.split(non_test_titles):\n",
    "    indices.append([train_index,val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522] [523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
      " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
      " 649 650 651 652]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(523, 130, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_index, val_index = indices[np.random.randint(0,num_splits)]\n",
    "print(train_index,val_index)\n",
    "val_titles = non_test_titles[val_index]\n",
    "train_titles = non_test_titles[train_index]\n",
    "len(train_titles),len(val_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    sentencizer = English()\n",
    "    sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return sentencizer, nlp\n",
    "sentencizer, nlp = load_spacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        idx = idx.strip()\n",
    "        \n",
    "            \n",
    "#         cl = dataframe[dataframe.Article==idx]['Claim'].values\n",
    "#         sentences=articles[ar_id]\n",
    "#         print(len(sentences))\n",
    "        if mode=='test':\n",
    "            hd = dnf_eval[dnf_eval.headline==idx]['headline'].values[0].lower()\n",
    "            ar_id = dnf_eval[dnf_eval.headline==idx]['id'].values[0]\n",
    "            cl = dnf_eval[dnf_eval.headline==idx]['claims'].values[0]\n",
    "            ar_claims.append(cl)\n",
    "            sentences = articles300[ar_id]\n",
    "            vectors = article_vectors300[ar_id]\n",
    "        else:\n",
    "            try:\n",
    "                hd = dataframe[dataframe.headline==idx]['headline'].values[0].lower()\n",
    "                ar_id = dataframe[dataframe.headline==idx]['id'].values[0]\n",
    "                ar_claims.append('None')\n",
    "                sentences=articles[ar_id]\n",
    "                vectors = article_vectors[ar_id]\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(idx)\n",
    "            \n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "        \n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = datagen_dnf(train_batchsize,dnf700,mode='train')\n",
    "vdg = datagen_dnf(val_batchsize,dnf700,mode='val')\n",
    "test_dg = datagen_dnf(test_batchsize,dnf700,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(test_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['sentence_vectors'].shape, x['headline_vector'].shape, y['headline_token_classes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 500, 1024)    0           ca1[0][0]                        \n",
      "                                                                 ca2[0][0]                        \n",
      "                                                                 ca3[0][0]                        \n",
      "                                                                 ca4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 500, 1024)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 500, 1024)    4096        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 250, 256)     786688      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 125, 256)     196864      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 63, 256)      196864      conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 32, 256)      196864      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 256)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 256)      1024        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          131584      global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_headline_vector (Dense)  (None, 300)          153900      batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,465,348\n",
      "Trainable params: 2,460,676\n",
      "Non-trainable params: 4,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2130pt\" viewBox=\"0.00 0.00 1810.50 2130.00\" width=\"1811pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2126)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-2126 1806.5,-2126 1806.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140648275970592 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140648275970592</title>\n",
       "<polygon fill=\"none\" points=\"857,-2075.5 857,-2121.5 1198,-2121.5 1198,-2075.5 857,-2075.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"945\" y=\"-2094.8\">sentence_vectors: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1033,-2075.5 1033,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1060.5\" y=\"-2106.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1033,-2098.5 1088,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1060.5\" y=\"-2083.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1088,-2075.5 1088,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1143\" y=\"-2106.3\">(None, 500, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1088,-2098.5 1198,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1143\" y=\"-2083.3\">(None, 500, 300)</text>\n",
       "</g>\n",
       "<!-- 140648275970984 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140648275970984</title>\n",
       "<polygon fill=\"none\" points=\"883.5,-1992.5 883.5,-2038.5 1171.5,-2038.5 1171.5,-1992.5 883.5,-1992.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"945\" y=\"-2011.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1006.5,-1992.5 1006.5,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1034\" y=\"-2023.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1006.5,-2015.5 1061.5,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1034\" y=\"-2000.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1061.5,-1992.5 1061.5,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-2023.3\">(None, 500, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1061.5,-2015.5 1171.5,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-2000.3\">(None, 500, 16)</text>\n",
       "</g>\n",
       "<!-- 140648275970592&#45;&gt;140648275970984 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140648275970592-&gt;140648275970984</title>\n",
       "<path d=\"M1027.5,-2075.3799C1027.5,-2067.1745 1027.5,-2057.7679 1027.5,-2048.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-2048.784 1027.5,-2038.784 1024.0001,-2048.784 1031.0001,-2048.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648275971096 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140648275971096</title>\n",
       "<polygon fill=\"none\" points=\"885.5,-1909.5 885.5,-1955.5 1169.5,-1955.5 1169.5,-1909.5 885.5,-1909.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1928.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1909.5 1010.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1940.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1932.5 1065.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1917.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1909.5 1065.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1940.3\">(None, 500, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1932.5 1169.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1917.3\">(None, 500, 16)</text>\n",
       "</g>\n",
       "<!-- 140648275970984&#45;&gt;140648275971096 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140648275970984-&gt;140648275971096</title>\n",
       "<path d=\"M1027.5,-1992.3799C1027.5,-1984.1745 1027.5,-1974.7679 1027.5,-1965.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1965.784 1027.5,-1955.784 1024.0001,-1965.784 1031.0001,-1965.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648275971824 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140648275971824</title>\n",
       "<polygon fill=\"none\" points=\"886.5,-1826.5 886.5,-1872.5 1168.5,-1872.5 1168.5,-1826.5 886.5,-1826.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1845.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1009.5,-1826.5 1009.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1037\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1009.5,-1849.5 1064.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1037\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1064.5,-1826.5 1064.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-1857.3\">(None, 500, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1064.5,-1849.5 1168.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-1834.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 140648275971096&#45;&gt;140648275971824 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140648275971096-&gt;140648275971824</title>\n",
       "<path d=\"M1027.5,-1909.3799C1027.5,-1901.1745 1027.5,-1891.7679 1027.5,-1882.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1882.784 1027.5,-1872.784 1024.0001,-1882.784 1031.0001,-1882.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648276095496 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140648276095496</title>\n",
       "<polygon fill=\"none\" points=\"885.5,-1743.5 885.5,-1789.5 1169.5,-1789.5 1169.5,-1743.5 885.5,-1743.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1762.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1743.5 1010.5,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1766.5 1065.5,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1743.5 1065.5,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1774.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1766.5 1169.5,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1751.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 140648275971824&#45;&gt;140648276095496 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140648275971824-&gt;140648276095496</title>\n",
       "<path d=\"M1027.5,-1826.3799C1027.5,-1818.1745 1027.5,-1808.7679 1027.5,-1799.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1799.784 1027.5,-1789.784 1024.0001,-1799.784 1031.0001,-1799.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648276098016 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140648276098016</title>\n",
       "<polygon fill=\"none\" points=\"818,-1660.5 818,-1706.5 1237,-1706.5 1237,-1660.5 818,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1679.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1078,-1660.5 1078,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1105.5\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1078,-1683.5 1133,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1105.5\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1133,-1660.5 1133,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185\" y=\"-1691.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1133,-1683.5 1237,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185\" y=\"-1668.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 140648276095496&#45;&gt;140648276098016 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140648276095496-&gt;140648276098016</title>\n",
       "<path d=\"M1027.5,-1743.3799C1027.5,-1735.1745 1027.5,-1725.7679 1027.5,-1716.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1716.784 1027.5,-1706.784 1024.0001,-1716.784 1031.0001,-1716.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648237288360 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140648237288360</title>\n",
       "<polygon fill=\"none\" points=\"252.5,-1577.5 252.5,-1623.5 626.5,-1623.5 626.5,-1577.5 252.5,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-1596.8\">sa1: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"367.5,-1577.5 367.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"395\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"367.5,-1600.5 422.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"395\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"422.5,-1577.5 422.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.5\" y=\"-1608.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"422.5,-1600.5 626.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.5\" y=\"-1585.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648276098016&#45;&gt;140648237288360 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140648276098016-&gt;140648237288360</title>\n",
       "<path d=\"M864.4901,-1660.4901C786.1844,-1649.4367 692.2952,-1636.1837 612.7036,-1624.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"612.9631,-1621.4508 602.572,-1623.5187 611.9847,-1628.3821 612.9631,-1621.4508\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648236699208 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140648236699208</title>\n",
       "<polygon fill=\"none\" points=\"644.5,-1577.5 644.5,-1623.5 1018.5,-1623.5 1018.5,-1577.5 644.5,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"702\" y=\"-1596.8\">sa2: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"759.5,-1577.5 759.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"759.5,-1600.5 814.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"814.5,-1577.5 814.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-1608.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"814.5,-1600.5 1018.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-1585.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648276098016&#45;&gt;140648236699208 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140648276098016-&gt;140648236699208</title>\n",
       "<path d=\"M973.1634,-1660.4901C949.0535,-1650.2803 920.5118,-1638.1938 895.3877,-1627.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"896.5156,-1624.2313 885.9423,-1623.5547 893.7859,-1630.6771 896.5156,-1624.2313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648235414976 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140648235414976</title>\n",
       "<polygon fill=\"none\" points=\"1036.5,-1577.5 1036.5,-1623.5 1410.5,-1623.5 1410.5,-1577.5 1036.5,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-1596.8\">sa3: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1151.5,-1577.5 1151.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1179\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1151.5,-1600.5 1206.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1179\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1206.5,-1577.5 1206.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1308.5\" y=\"-1608.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1206.5,-1600.5 1410.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1308.5\" y=\"-1585.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648276098016&#45;&gt;140648235414976 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140648276098016-&gt;140648235414976</title>\n",
       "<path d=\"M1081.8366,-1660.4901C1105.9465,-1650.2803 1134.4882,-1638.1938 1159.6123,-1627.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1161.2141,-1630.6771 1169.0577,-1623.5547 1158.4844,-1624.2313 1161.2141,-1630.6771\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648234721464 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140648234721464</title>\n",
       "<polygon fill=\"none\" points=\"1428.5,-1577.5 1428.5,-1623.5 1802.5,-1623.5 1802.5,-1577.5 1428.5,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1486\" y=\"-1596.8\">sa4: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1543.5,-1577.5 1543.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1571\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1543.5,-1600.5 1598.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1571\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1598.5,-1577.5 1598.5,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1700.5\" y=\"-1608.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1598.5,-1600.5 1802.5,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1700.5\" y=\"-1585.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648276098016&#45;&gt;140648234721464 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140648276098016-&gt;140648234721464</title>\n",
       "<path d=\"M1190.5099,-1660.4901C1268.8156,-1649.4367 1362.7048,-1636.1837 1442.2964,-1624.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1443.0153,-1628.3821 1452.428,-1623.5187 1442.0369,-1621.4508 1443.0153,-1628.3821\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233861072 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140648233861072</title>\n",
       "<polygon fill=\"none\" points=\"718,-1494.5 718,-1540.5 1337,-1540.5 1337,-1494.5 718,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"802\" y=\"-1513.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"886,-1494.5 886,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"886,-1517.5 941,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"941,-1494.5 941,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1139\" y=\"-1525.3\">[(None, 500, 32), (None, 500, 32), (None, 500, 32), (None, 500, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"941,-1517.5 1337,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1139\" y=\"-1502.3\">(None, 500, 128)</text>\n",
       "</g>\n",
       "<!-- 140648237288360&#45;&gt;140648233861072 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140648237288360-&gt;140648233861072</title>\n",
       "<path d=\"M602.5099,-1577.4901C680.8156,-1566.4367 774.7048,-1553.1837 854.2964,-1541.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"855.0153,-1545.3821 864.428,-1540.5187 854.0369,-1538.4508 855.0153,-1545.3821\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648236699208&#45;&gt;140648233861072 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140648236699208-&gt;140648233861072</title>\n",
       "<path d=\"M885.8366,-1577.4901C909.9465,-1567.2803 938.4882,-1555.1938 963.6123,-1544.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"965.2141,-1547.6771 973.0577,-1540.5547 962.4844,-1541.2313 965.2141,-1547.6771\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648235414976&#45;&gt;140648233861072 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140648235414976-&gt;140648233861072</title>\n",
       "<path d=\"M1169.1634,-1577.4901C1145.0535,-1567.2803 1116.5118,-1555.1938 1091.3877,-1544.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1092.5156,-1541.2313 1081.9423,-1540.5547 1089.7859,-1547.6771 1092.5156,-1541.2313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648234721464&#45;&gt;140648233861072 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140648234721464-&gt;140648233861072</title>\n",
       "<path d=\"M1452.4901,-1577.4901C1374.1844,-1566.4367 1280.2952,-1553.1837 1200.7036,-1541.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1200.9631,-1538.4508 1190.572,-1540.5187 1199.9847,-1545.3821 1200.9631,-1538.4508\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648275971432 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140648275971432</title>\n",
       "<polygon fill=\"none\" points=\"357,-1494.5 357,-1540.5 700,-1540.5 700,-1494.5 357,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-1513.8\">input_headline_vector: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"562,-1494.5 562,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.5\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"562,-1517.5 617,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.5\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"617,-1494.5 617,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658.5\" y=\"-1525.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"617,-1517.5 700,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658.5\" y=\"-1502.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140648233219912 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140648233219912</title>\n",
       "<polygon fill=\"none\" points=\"439.5,-1411.5 439.5,-1457.5 679.5,-1457.5 679.5,-1411.5 439.5,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"490.5\" y=\"-1430.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1411.5 541.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1434.5 596.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"596.5,-1411.5 596.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638\" y=\"-1442.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"596.5,-1434.5 679.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638\" y=\"-1419.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140648275971432&#45;&gt;140648233219912 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140648275971432-&gt;140648233219912</title>\n",
       "<path d=\"M537.1352,-1494.3799C540.2665,-1485.9962 543.8662,-1476.3584 547.2495,-1467.2996\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"550.5834,-1468.3766 550.8036,-1457.784 544.0259,-1465.9273 550.5834,-1468.3766\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648236961016 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140648236961016</title>\n",
       "<polygon fill=\"none\" points=\"881.5,-1411.5 881.5,-1457.5 1169.5,-1457.5 1169.5,-1411.5 881.5,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"943\" y=\"-1430.8\">conv1d_3: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1004.5,-1411.5 1004.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1032\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1004.5,-1434.5 1059.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1032\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1059.5,-1411.5 1059.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1114.5\" y=\"-1442.3\">(None, 500, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1059.5,-1434.5 1169.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1114.5\" y=\"-1419.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 140648233861072&#45;&gt;140648236961016 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140648233861072-&gt;140648236961016</title>\n",
       "<path d=\"M1026.9429,-1494.3799C1026.7452,-1486.1745 1026.5185,-1476.7679 1026.3043,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1029.801,-1467.6968 1026.0611,-1457.784 1022.8031,-1467.8655 1029.801,-1467.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648232561800 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140648232561800</title>\n",
       "<polygon fill=\"none\" points=\"437,-1328.5 437,-1374.5 712,-1374.5 712,-1328.5 437,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"498.5\" y=\"-1347.8\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"560,-1328.5 560,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"560,-1351.5 615,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"615,-1328.5 615,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"663.5\" y=\"-1359.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"615,-1351.5 712,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"663.5\" y=\"-1336.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140648233219912&#45;&gt;140648232561800 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140648233219912-&gt;140648232561800</title>\n",
       "<path d=\"M563.6783,-1411.3799C565.1612,-1403.1745 566.8612,-1393.7679 568.4677,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"571.9578,-1385.2471 570.292,-1374.784 565.0693,-1384.0021 571.9578,-1385.2471\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135576 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140648233135576</title>\n",
       "<polygon fill=\"none\" points=\"876.5,-1328.5 876.5,-1374.5 1166.5,-1374.5 1166.5,-1328.5 876.5,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"939\" y=\"-1347.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1001.5,-1328.5 1001.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1029\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1001.5,-1351.5 1056.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1029\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1056.5,-1328.5 1056.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-1359.3\">(None, 500, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1056.5,-1351.5 1166.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-1336.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 140648236961016&#45;&gt;140648233135576 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140648236961016-&gt;140648233135576</title>\n",
       "<path d=\"M1024.3858,-1411.3799C1023.9903,-1403.1745 1023.537,-1393.7679 1023.1086,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1026.5995,-1384.6039 1022.6221,-1374.784 1019.6076,-1384.9409 1026.5995,-1384.6039\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648232034656 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140648232034656</title>\n",
       "<polygon fill=\"none\" points=\"376.5,-1245.5 376.5,-1291.5 788.5,-1291.5 788.5,-1245.5 376.5,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-1264.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"636.5,-1245.5 636.5,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"664\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"636.5,-1268.5 691.5,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"664\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"691.5,-1245.5 691.5,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"740\" y=\"-1276.3\">(None, 1, 256)</text>\n",
       "<polyline fill=\"none\" points=\"691.5,-1268.5 788.5,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"740\" y=\"-1253.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140648232561800&#45;&gt;140648232034656 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140648232561800-&gt;140648232034656</title>\n",
       "<path d=\"M576.7284,-1328.3799C577.5193,-1320.1745 578.426,-1310.7679 579.2828,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"582.7801,-1302.0737 580.2558,-1291.784 575.8124,-1301.4021 582.7801,-1302.0737\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135016 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140648233135016</title>\n",
       "<polygon fill=\"none\" points=\"807,-1245.5 807,-1291.5 1232,-1291.5 1232,-1245.5 807,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"937\" y=\"-1264.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1067,-1245.5 1067,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1067,-1268.5 1122,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1122,-1245.5 1122,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1177\" y=\"-1276.3\">(None, 500, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1122,-1268.5 1232,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1177\" y=\"-1253.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 140648233135576&#45;&gt;140648233135016 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140648233135576-&gt;140648233135016</title>\n",
       "<path d=\"M1020.9429,-1328.3799C1020.7452,-1320.1745 1020.5185,-1310.7679 1020.3043,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1023.801,-1301.6968 1020.0611,-1291.784 1016.8031,-1301.8655 1023.801,-1301.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231812064 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140648231812064</title>\n",
       "<polygon fill=\"none\" points=\"810,-1162.5 810,-1208.5 1197,-1208.5 1197,-1162.5 810,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872.5\" y=\"-1181.8\">ca1: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"935,-1162.5 935,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"962.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"935,-1185.5 990,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"962.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"990,-1162.5 990,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093.5\" y=\"-1193.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"990,-1185.5 1197,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093.5\" y=\"-1170.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648232034656&#45;&gt;140648231812064 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140648232034656-&gt;140648231812064</title>\n",
       "<path d=\"M699.2129,-1245.4901C754.3078,-1234.6282 820.1773,-1221.642 876.519,-1210.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"877.4259,-1213.9229 886.5601,-1208.5547 876.0719,-1207.0551 877.4259,-1213.9229\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231673080 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>140648231673080</title>\n",
       "<polygon fill=\"none\" points=\"1215,-1162.5 1215,-1208.5 1602,-1208.5 1602,-1162.5 1215,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1277.5\" y=\"-1181.8\">ca2: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"1340,-1162.5 1340,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1367.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1340,-1185.5 1395,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1367.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1395,-1162.5 1395,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1498.5\" y=\"-1193.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1395,-1185.5 1602,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1498.5\" y=\"-1170.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648232034656&#45;&gt;140648231673080 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>140648232034656-&gt;140648231673080</title>\n",
       "<path d=\"M788.5682,-1245.986C791.9028,-1245.6525 795.2153,-1245.3236 798.5,-1245 976.3881,-1227.477 1023.7095,-1227.5124 1204.8542,-1209.1557\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1205.3964,-1212.6187 1214.9903,-1208.1236 1204.6872,-1205.6547 1205.3964,-1212.6187\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648229904624 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>140648229904624</title>\n",
       "<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 387,-1208.5 387,-1162.5 0,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-1181.8\">ca3: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"125,-1162.5 125,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"125,-1185.5 180,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"180,-1162.5 180,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-1193.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"180,-1185.5 387,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-1170.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648232034656&#45;&gt;140648229904624 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>140648232034656-&gt;140648229904624</title>\n",
       "<path d=\"M474.6584,-1245.4901C423.962,-1234.6731 363.3923,-1221.7495 311.4764,-1210.6723\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"312.0616,-1207.2185 301.5514,-1208.5547 310.6009,-1214.0644 312.0616,-1207.2185\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648229159936 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>140648229159936</title>\n",
       "<polygon fill=\"none\" points=\"405,-1162.5 405,-1208.5 792,-1208.5 792,-1162.5 405,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"467.5\" y=\"-1181.8\">ca4: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"530,-1162.5 530,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"530,-1185.5 585,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"585,-1162.5 585,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"688.5\" y=\"-1193.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"585,-1185.5 792,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"688.5\" y=\"-1170.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140648232034656&#45;&gt;140648229159936 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>140648232034656-&gt;140648229159936</title>\n",
       "<path d=\"M586.9569,-1245.3799C588.5386,-1237.1745 590.352,-1227.7679 592.0656,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"595.5553,-1219.2658 594.0115,-1208.784 588.6818,-1217.9407 595.5553,-1219.2658\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135016&#45;&gt;140648231812064 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>140648233135016-&gt;140648231812064</title>\n",
       "<path d=\"M1015.0431,-1245.3799C1013.4614,-1237.1745 1011.648,-1227.7679 1009.9344,-1218.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1013.3182,-1217.9407 1007.9885,-1208.784 1006.4447,-1219.2658 1013.3182,-1217.9407\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135016&#45;&gt;140648231673080 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>140648233135016-&gt;140648231673080</title>\n",
       "<path d=\"M1127.3416,-1245.4901C1178.038,-1234.6731 1238.6077,-1221.7495 1290.5236,-1210.6723\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1291.3991,-1214.0644 1300.4486,-1208.5547 1289.9384,-1207.2185 1291.3991,-1214.0644\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135016&#45;&gt;140648229904624 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>140648233135016-&gt;140648229904624</title>\n",
       "<path d=\"M806.8249,-1245.9162C803.6957,-1245.6071 800.5859,-1245.3015 797.5,-1245 622.1938,-1227.873 575.6096,-1227.3786 397.0043,-1209.1432\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"397.3151,-1205.6568 387.0102,-1208.1184 396.601,-1212.6203 397.3151,-1205.6568\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648233135016&#45;&gt;140648229159936 -->\n",
       "<g class=\"edge\" id=\"edge27\">\n",
       "<title>140648233135016-&gt;140648229159936</title>\n",
       "<path d=\"M902.7871,-1245.4901C847.6922,-1234.6282 781.8227,-1221.642 725.481,-1210.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"725.9281,-1207.0551 715.4399,-1208.5547 724.5741,-1213.9229 725.9281,-1207.0551\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140650590560608 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>140650590560608</title>\n",
       "<polygon fill=\"none\" points=\"477.5,-1079.5 477.5,-1125.5 1123.5,-1125.5 1123.5,-1079.5 477.5,-1079.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-1098.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1079.5 645.5,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1102.5 700.5,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1079.5 700.5,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"912\" y=\"-1110.3\">[(None, 500, 256), (None, 500, 256), (None, 500, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1102.5 1123.5,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"912\" y=\"-1087.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 140648231812064&#45;&gt;140650590560608 -->\n",
       "<g class=\"edge\" id=\"edge28\">\n",
       "<title>140648231812064-&gt;140650590560608</title>\n",
       "<path d=\"M947.2228,-1162.4901C922.1419,-1152.2353 892.4302,-1140.0872 866.3257,-1129.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"867.4675,-1126.0996 856.8867,-1125.5547 864.8183,-1132.579 867.4675,-1126.0996\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231673080&#45;&gt;140650590560608 -->\n",
       "<g class=\"edge\" id=\"edge29\">\n",
       "<title>140648231673080-&gt;140650590560608</title>\n",
       "<path d=\"M1239.9455,-1162.4901C1158.8115,-1151.4142 1061.4981,-1138.1297 979.0922,-1126.8802\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"979.5002,-1123.4035 969.1187,-1125.5187 978.5534,-1130.3392 979.5002,-1123.4035\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648229904624&#45;&gt;140650590560608 -->\n",
       "<g class=\"edge\" id=\"edge30\">\n",
       "<title>140648229904624-&gt;140650590560608</title>\n",
       "<path d=\"M361.7773,-1162.4901C442.7778,-1151.4142 539.9312,-1138.1297 622.2015,-1126.8802\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.725,-1130.3413 632.1586,-1125.5187 621.7766,-1123.4058 622.725,-1130.3413\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648229159936&#45;&gt;140650590560608 -->\n",
       "<g class=\"edge\" id=\"edge31\">\n",
       "<title>140648229159936-&gt;140650590560608</title>\n",
       "<path d=\"M654.5,-1162.4901C679.4573,-1152.2353 709.0226,-1140.0872 734.9986,-1129.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"736.4717,-1132.5927 744.3911,-1125.5547 733.8112,-1126.1179 736.4717,-1132.5927\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231435008 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>140648231435008</title>\n",
       "<polygon fill=\"none\" points=\"652,-996.5 652,-1042.5 949,-1042.5 949,-996.5 652,-996.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-1015.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"777,-996.5 777,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"777,-1019.5 832,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"832,-996.5 832,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-1027.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"832,-1019.5 949,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-1004.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 140650590560608&#45;&gt;140648231435008 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>140650590560608-&gt;140648231435008</title>\n",
       "<path d=\"M800.5,-1079.3799C800.5,-1071.1745 800.5,-1061.7679 800.5,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-1052.784 800.5,-1042.784 797.0001,-1052.784 804.0001,-1052.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231434504 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>140648231434504</title>\n",
       "<polygon fill=\"none\" points=\"584.5,-913.5 584.5,-959.5 1016.5,-959.5 1016.5,-913.5 584.5,-913.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-932.8\">batch_normalization_4: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"844.5,-913.5 844.5,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"844.5,-936.5 899.5,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"899.5,-913.5 899.5,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-944.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"899.5,-936.5 1016.5,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-921.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 140648231435008&#45;&gt;140648231434504 -->\n",
       "<g class=\"edge\" id=\"edge33\">\n",
       "<title>140648231435008-&gt;140648231434504</title>\n",
       "<path d=\"M800.5,-996.3799C800.5,-988.1745 800.5,-978.7679 800.5,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-969.784 800.5,-959.784 797.0001,-969.784 804.0001,-969.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648231434168 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>140648231434168</title>\n",
       "<polygon fill=\"none\" points=\"653,-830.5 653,-876.5 948,-876.5 948,-830.5 653,-830.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-849.8\">conv1d_4: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"776,-830.5 776,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776,-853.5 831,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"831,-830.5 831,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-861.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"831,-853.5 948,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-838.3\">(None, 250, 256)</text>\n",
       "</g>\n",
       "<!-- 140648231434504&#45;&gt;140648231434168 -->\n",
       "<g class=\"edge\" id=\"edge34\">\n",
       "<title>140648231434504-&gt;140648231434168</title>\n",
       "<path d=\"M800.5,-913.3799C800.5,-905.1745 800.5,-895.7679 800.5,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-886.784 800.5,-876.784 797.0001,-886.784 804.0001,-886.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648227645704 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>140648227645704</title>\n",
       "<polygon fill=\"none\" points=\"656.5,-747.5 656.5,-793.5 944.5,-793.5 944.5,-747.5 656.5,-747.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-766.8\">conv1d_5: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-747.5 779.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-770.5 834.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-747.5 834.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-778.3\">(None, 250, 256)</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-770.5 944.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-755.3\">(None, 125, 256)</text>\n",
       "</g>\n",
       "<!-- 140648231434168&#45;&gt;140648227645704 -->\n",
       "<g class=\"edge\" id=\"edge35\">\n",
       "<title>140648231434168-&gt;140648227645704</title>\n",
       "<path d=\"M800.5,-830.3799C800.5,-822.1745 800.5,-812.7679 800.5,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-803.784 800.5,-793.784 797.0001,-803.784 804.0001,-803.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225759976 -->\n",
       "<g class=\"node\" id=\"node28\">\n",
       "<title>140648225759976</title>\n",
       "<polygon fill=\"none\" points=\"656.5,-664.5 656.5,-710.5 944.5,-710.5 944.5,-664.5 656.5,-664.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-683.8\">conv1d_6: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-664.5 779.5,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-687.5 834.5,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-664.5 834.5,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-695.3\">(None, 125, 256)</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-687.5 944.5,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-672.3\">(None, 63, 256)</text>\n",
       "</g>\n",
       "<!-- 140648227645704&#45;&gt;140648225759976 -->\n",
       "<g class=\"edge\" id=\"edge36\">\n",
       "<title>140648227645704-&gt;140648225759976</title>\n",
       "<path d=\"M800.5,-747.3799C800.5,-739.1745 800.5,-729.7679 800.5,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-720.784 800.5,-710.784 797.0001,-720.784 804.0001,-720.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225882576 -->\n",
       "<g class=\"node\" id=\"node29\">\n",
       "<title>140648225882576</title>\n",
       "<polygon fill=\"none\" points=\"659.5,-581.5 659.5,-627.5 941.5,-627.5 941.5,-581.5 659.5,-581.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-600.8\">conv1d_7: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-581.5 782.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-604.5 837.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-581.5 837.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-612.3\">(None, 63, 256)</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-604.5 941.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-589.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 140648225759976&#45;&gt;140648225882576 -->\n",
       "<g class=\"edge\" id=\"edge37\">\n",
       "<title>140648225759976-&gt;140648225882576</title>\n",
       "<path d=\"M800.5,-664.3799C800.5,-656.1745 800.5,-646.7679 800.5,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-637.784 800.5,-627.784 797.0001,-637.784 804.0001,-637.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225558032 -->\n",
       "<g class=\"node\" id=\"node30\">\n",
       "<title>140648225558032</title>\n",
       "<polygon fill=\"none\" points=\"658.5,-498.5 658.5,-544.5 942.5,-544.5 942.5,-498.5 658.5,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-517.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-498.5 783.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-521.5 838.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-498.5 838.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-529.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-521.5 942.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-506.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 140648225882576&#45;&gt;140648225558032 -->\n",
       "<g class=\"edge\" id=\"edge38\">\n",
       "<title>140648225882576-&gt;140648225558032</title>\n",
       "<path d=\"M800.5,-581.3799C800.5,-573.1745 800.5,-563.7679 800.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-554.784 800.5,-544.784 797.0001,-554.784 804.0001,-554.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225557304 -->\n",
       "<g class=\"node\" id=\"node31\">\n",
       "<title>140648225557304</title>\n",
       "<polygon fill=\"none\" points=\"591,-415.5 591,-461.5 1010,-461.5 1010,-415.5 591,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-434.8\">batch_normalization_5: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"851,-415.5 851,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"851,-438.5 906,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"906,-415.5 906,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-446.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"906,-438.5 1010,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-423.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 140648225558032&#45;&gt;140648225557304 -->\n",
       "<g class=\"edge\" id=\"edge39\">\n",
       "<title>140648225558032-&gt;140648225557304</title>\n",
       "<path d=\"M800.5,-498.3799C800.5,-490.1745 800.5,-480.7679 800.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-471.784 800.5,-461.784 797.0001,-471.784 804.0001,-471.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225555512 -->\n",
       "<g class=\"node\" id=\"node32\">\n",
       "<title>140648225555512</title>\n",
       "<polygon fill=\"none\" points=\"559.5,-332.5 559.5,-378.5 1041.5,-378.5 1041.5,-332.5 559.5,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-351.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"882.5,-332.5 882.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"882.5,-355.5 937.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"937.5,-332.5 937.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"989.5\" y=\"-363.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"937.5,-355.5 1041.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"989.5\" y=\"-340.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140648225557304&#45;&gt;140648225555512 -->\n",
       "<g class=\"edge\" id=\"edge40\">\n",
       "<title>140648225557304-&gt;140648225555512</title>\n",
       "<path d=\"M800.5,-415.3799C800.5,-407.1745 800.5,-397.7679 800.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-388.784 800.5,-378.784 797.0001,-388.784 804.0001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648225884368 -->\n",
       "<g class=\"node\" id=\"node33\">\n",
       "<title>140648225884368</title>\n",
       "<polygon fill=\"none\" points=\"680.5,-249.5 680.5,-295.5 920.5,-295.5 920.5,-249.5 680.5,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731.5\" y=\"-268.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-249.5 782.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-272.5 837.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-249.5 837.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879\" y=\"-280.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-272.5 920.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140648225555512&#45;&gt;140648225884368 -->\n",
       "<g class=\"edge\" id=\"edge41\">\n",
       "<title>140648225555512-&gt;140648225884368</title>\n",
       "<path d=\"M800.5,-332.3799C800.5,-324.1745 800.5,-314.7679 800.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-305.784 800.5,-295.784 797.0001,-305.784 804.0001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648224613488 -->\n",
       "<g class=\"node\" id=\"node34\">\n",
       "<title>140648224613488</title>\n",
       "<polygon fill=\"none\" points=\"669,-166.5 669,-212.5 932,-212.5 932,-166.5 669,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731.5\" y=\"-185.8\">dropout_6: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"794,-166.5 794,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"821.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"794,-189.5 849,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"821.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"849,-166.5 849,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"849,-189.5 932,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140648225884368&#45;&gt;140648224613488 -->\n",
       "<g class=\"edge\" id=\"edge42\">\n",
       "<title>140648225884368-&gt;140648224613488</title>\n",
       "<path d=\"M800.5,-249.3799C800.5,-241.1745 800.5,-231.7679 800.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-222.784 800.5,-212.784 797.0001,-222.784 804.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648224705448 -->\n",
       "<g class=\"node\" id=\"node35\">\n",
       "<title>140648224705448</title>\n",
       "<polygon fill=\"none\" points=\"601.5,-83.5 601.5,-129.5 999.5,-129.5 999.5,-83.5 601.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731.5\" y=\"-102.8\">batch_normalization_6: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"861.5,-83.5 861.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"861.5,-106.5 916.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"916.5,-83.5 916.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"916.5,-106.5 999.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140648224613488&#45;&gt;140648224705448 -->\n",
       "<g class=\"edge\" id=\"edge43\">\n",
       "<title>140648224613488-&gt;140648224705448</title>\n",
       "<path d=\"M800.5,-166.3799C800.5,-158.1745 800.5,-148.7679 800.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-139.784 800.5,-129.784 797.0001,-139.784 804.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648224452336 -->\n",
       "<g class=\"node\" id=\"node36\">\n",
       "<title>140648224452336</title>\n",
       "<polygon fill=\"none\" points=\"639,-.5 639,-46.5 962,-46.5 962,-.5 639,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731.5\" y=\"-19.8\">output_headline_vector: Dense</text>\n",
       "<polyline fill=\"none\" points=\"824,-.5 824,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"851.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"824,-23.5 879,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"851.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"879,-.5 879,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920.5\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"879,-23.5 962,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920.5\" y=\"-8.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140648224705448&#45;&gt;140648224452336 -->\n",
       "<g class=\"edge\" id=\"edge44\">\n",
       "<title>140648224705448-&gt;140648224452336</title>\n",
       "<path d=\"M800.5,-83.3799C800.5,-75.1745 800.5,-65.7679 800.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-56.784 800.5,-46.784 797.0001,-56.784 804.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp_sentence_vectors = Input(shape=(max_sentences, 300), name='sentence_vectors')\n",
    "    inp_headline_vector = Input(shape=(300,), name='input_headline_vector')\n",
    "    conv1 = Conv1D(filters=16,kernel_size=3,strides=1,activation='relu', padding='same')(inp_sentence_vectors)\n",
    "    conv1 = Dropout(0.5)(conv1)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=3,strides=1,activation='relu', padding='same')(conv1)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    sent_sa_feat_1, sent_beta_1, sent_gamma_1 = SelfAttention(int(conv2.shape[-1]), name = 'sa1')(conv2)\n",
    "    sent_sa_feat_2, sent_beta_2, sent_gamma_2 = SelfAttention(int(conv2.shape[-1]), name = 'sa2')(conv2)\n",
    "    sent_sa_feat_3, sent_beta_3, sent_gamma_3 = SelfAttention(int(conv2.shape[-1]), name = 'sa3')(conv2)\n",
    "    sent_sa_feat_4, sent_beta_4, sent_gamma_4 = SelfAttention(int(conv2.shape[-1]), name = 'sa4')(conv2)\n",
    "    concat1 = Concatenate()([sent_sa_feat_1,sent_sa_feat_2,sent_sa_feat_3,sent_sa_feat_4])\n",
    "    conv3 = Conv1D(filters=256,kernel_size=3, strides=1, activation='relu', padding='same')(concat1)\n",
    "    conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    headline = Dense(256, activation='relu')(inp_headline_vector)\n",
    "    headline = Lambda(lambda x:K.expand_dims(x, axis=1))(headline)\n",
    "    headline = BatchNormalization()(headline)\n",
    "    sent_hd_sa_feat_1, sent_hd_beta_1, sent_hd_gamma_1 = CrossAttention(int(conv3.shape[-1]), name = 'ca1')([headline,conv3])\n",
    "    sent_hd_sa_feat_2, sent_hd_beta_2, sent_hd_gamma_2 = CrossAttention(int(conv3.shape[-1]), name = 'ca2')([headline,conv3])\n",
    "    sent_hd_sa_feat_3, sent_hd_beta_3, sent_hd_gamma_3 = CrossAttention(int(conv3.shape[-1]), name = 'ca3')([headline,conv3])\n",
    "    sent_hd_sa_feat_4, sent_hd_beta_4, sent_hd_gamma_4 = CrossAttention(int(conv3.shape[-1]), name = 'ca4')([headline,conv3])  \n",
    "    concat3 = Concatenate()([sent_hd_sa_feat_1,sent_hd_sa_feat_2,sent_hd_sa_feat_3,sent_hd_sa_feat_4])\n",
    "    concat3 = Dropout(0.5)(concat3)\n",
    "    concat3 = BatchNormalization()(concat3)\n",
    "    conv5 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(concat3)\n",
    "    conv6 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "    conv7 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv6)\n",
    "    conv8 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv7)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    gap = GlobalAveragePooling1D()(conv8)\n",
    "#     repeat = RepeatVector(50)(gap)\n",
    "#     lstm = LSTM(256,return_sequences=True)(repeat)\n",
    "    dense1 = Dense(512,activation='relu')(gap)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    gen_hd_vector = Dense(300,activation='linear', name='output_headline_vector')(dense1)\n",
    "    model = Model([inp_sentence_vectors,inp_headline_vector],gen_hd_vector)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001,beta_1=0.0,beta_2=0.99),loss='mse')\n",
    "model.summary()\n",
    "# print('model params:',model.count_params())\n",
    "SVG(model_to_dot(model,show_layer_names=True,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now()\n",
    "mc = ModelCheckpoint('weights/dnf700_sa_sent_hd_vector_gl.hdf5',save_best_only=True,save_weights_only=True)\n",
    "tb = TensorBoard(batch_size=32,log_dir='logs/dnf700_sa_sent_hd_vector_gl/{0}'.format(dt.timestamp()),write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.1550 - val_loss: 0.3712\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 109ms/step - loss: 1.1650 - val_loss: 0.3526\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 1.1615 - val_loss: 0.3359\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 1.1248 - val_loss: 0.3075\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 3s 810ms/step - loss: 1.1313 - val_loss: 0.2789\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 1.1184 - val_loss: 0.2711\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 3s 794ms/step - loss: 1.1070 - val_loss: 0.2639\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 1.0799 - val_loss: 0.2310\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 1.0988 - val_loss: 0.2132\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 1.0608 - val_loss: 0.2084\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 1.0457 - val_loss: 0.2052\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 1.0498 - val_loss: 0.2088\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 3s 811ms/step - loss: 1.0324 - val_loss: 0.1869\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 1.0269 - val_loss: 0.1759\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 1.0151 - val_loss: 0.1773\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 1.0111 - val_loss: 0.1686\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 1.0047 - val_loss: 0.1590\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 1.0018 - val_loss: 0.1541\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9851 - val_loss: 0.1426\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9804 - val_loss: 0.1435\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9422 - val_loss: 0.1444\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9500 - val_loss: 0.1355\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9533 - val_loss: 0.1337\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9227 - val_loss: 0.1252\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9183 - val_loss: 0.1137\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.9350 - val_loss: 0.1201\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9139 - val_loss: 0.1216\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9177 - val_loss: 0.1165\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8849 - val_loss: 0.1039\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8927 - val_loss: 0.1076\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8589 - val_loss: 0.0995\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8793 - val_loss: 0.0997\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8767 - val_loss: 0.0985\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8755 - val_loss: 0.1035\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8396 - val_loss: 0.0903\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8474 - val_loss: 0.1035\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8357 - val_loss: 0.0882\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8350 - val_loss: 0.0917\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8438 - val_loss: 0.0825\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8176 - val_loss: 0.0772\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.8106 - val_loss: 0.0805\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.8261 - val_loss: 0.0844\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 4s 951ms/step - loss: 0.8206 - val_loss: 0.0794\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7854 - val_loss: 0.0738\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.7865 - val_loss: 0.0779\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.7905 - val_loss: 0.0752\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7891 - val_loss: 0.0763\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.7677 - val_loss: 0.0673\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 4s 922ms/step - loss: 0.7659 - val_loss: 0.0703\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 4s 995ms/step - loss: 0.7518 - val_loss: 0.0680\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7605 - val_loss: 0.0619\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7481 - val_loss: 0.0657\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7398 - val_loss: 0.0726\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7387 - val_loss: 0.0647\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7178 - val_loss: 0.0622\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.7183 - val_loss: 0.0596\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6848 - val_loss: 0.0629\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7019 - val_loss: 0.0568\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6983 - val_loss: 0.0588\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.7012 - val_loss: 0.0577\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.7173 - val_loss: 0.0556\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.7001 - val_loss: 0.0576\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.6702 - val_loss: 0.0529\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6701 - val_loss: 0.0518\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6946 - val_loss: 0.0535\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.6744 - val_loss: 0.0551\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.6547 - val_loss: 0.0568\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6573 - val_loss: 0.0594\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.6671 - val_loss: 0.0522\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 4s 998ms/step - loss: 0.6529 - val_loss: 0.0539\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.6491 - val_loss: 0.0509\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.6217 - val_loss: 0.0522\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.6172 - val_loss: 0.0570\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.6115 - val_loss: 0.0509\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6155 - val_loss: 0.0481\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.5953 - val_loss: 0.0467\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5954 - val_loss: 0.0472\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5955 - val_loss: 0.0485\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.5724 - val_loss: 0.0493\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5800 - val_loss: 0.0467\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.5613 - val_loss: 0.0490\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5657 - val_loss: 0.0448\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.5377 - val_loss: 0.0441\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.5469 - val_loss: 0.0435\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.5311 - val_loss: 0.0456\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5496 - val_loss: 0.0486\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5140 - val_loss: 0.0462\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.5094 - val_loss: 0.0428\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4670 - val_loss: 0.0434\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4732 - val_loss: 0.0454\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4811 - val_loss: 0.0383\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 6s 1s/step - loss: 0.4837 - val_loss: 0.0411\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.4470 - val_loss: 0.0395\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.4690 - val_loss: 0.0418\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.4516 - val_loss: 0.0416\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.4484 - val_loss: 0.0375\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.4251 - val_loss: 0.0425\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.4379 - val_loss: 0.0375\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.4301 - val_loss: 0.0384\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3823 - val_loss: 0.0409\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3968 - val_loss: 0.0400\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.4088 - val_loss: 0.0420\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.3843 - val_loss: 0.0403\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3741 - val_loss: 0.0397\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3786 - val_loss: 0.0367\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3788 - val_loss: 0.0403\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.3700 - val_loss: 0.0407\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.3530 - val_loss: 0.0362\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3379 - val_loss: 0.0618\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3148 - val_loss: 0.0370\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.3233 - val_loss: 0.0368\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3066 - val_loss: 0.0446\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.3195 - val_loss: 0.0657\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.3108 - val_loss: 0.0356\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2959 - val_loss: 0.0437\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.3082 - val_loss: 0.0739\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2958 - val_loss: 0.3147\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.3152 - val_loss: 0.0336\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2668 - val_loss: 0.0328\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2657 - val_loss: 0.0298\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.2644 - val_loss: 0.0276\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2446 - val_loss: 0.0286\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2500 - val_loss: 0.0312\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.2337 - val_loss: 0.0285\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2533 - val_loss: 0.0269\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.2224 - val_loss: 0.0281\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.2331 - val_loss: 0.0271\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2459 - val_loss: 0.0253\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2161 - val_loss: 0.0247\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.2249 - val_loss: 0.0254\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.2366 - val_loss: 0.0246\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.2061 - val_loss: 0.0239\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1996 - val_loss: 0.0238\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1980 - val_loss: 0.0223\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1964 - val_loss: 0.0246\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1973 - val_loss: 0.0223\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.2052 - val_loss: 0.0228\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.2059 - val_loss: 0.0200\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1977 - val_loss: 0.0205\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1920 - val_loss: 0.0210\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1954 - val_loss: 0.0225\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1839 - val_loss: 0.0185\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1744 - val_loss: 0.0197\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1756 - val_loss: 0.0603\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1965 - val_loss: 0.0180\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1617 - val_loss: 0.0187\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1731 - val_loss: 0.0188\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1806 - val_loss: 0.0153\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1679 - val_loss: 0.0177\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1718 - val_loss: 0.0194\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1650 - val_loss: 0.2036\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1813 - val_loss: 0.0192\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1539 - val_loss: 0.0477\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1621 - val_loss: 0.0695\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1537 - val_loss: 0.1118\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1643 - val_loss: 0.0903\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1564 - val_loss: 0.2056\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1536 - val_loss: 0.0154\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1606 - val_loss: 0.0166\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1656 - val_loss: 0.0219\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1337 - val_loss: 0.0966\n",
      "Epoch 162/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1512 - val_loss: 0.1817\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1597 - val_loss: 0.0358\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1538 - val_loss: 0.0138\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1486 - val_loss: 0.0520\n",
      "Epoch 166/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 10s 3s/step - loss: 0.1520 - val_loss: 0.0534\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1366 - val_loss: 0.0572\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1315 - val_loss: 0.1035\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1520 - val_loss: 0.2119\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1338 - val_loss: 0.1910\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1472 - val_loss: 0.0137\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1399 - val_loss: 0.0155\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1366 - val_loss: 0.0166\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1373 - val_loss: 0.0124\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 0.1395 - val_loss: 0.0131\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1287 - val_loss: 0.0123\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1154 - val_loss: 0.0311\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1325 - val_loss: 0.0140\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 0.1255 - val_loss: 0.0142\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 0.1329 - val_loss: 0.0142\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1205 - val_loss: 0.0137\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.1098 - val_loss: 0.0422\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1242 - val_loss: 0.3686\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1189 - val_loss: 0.0170\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1196 - val_loss: 0.1819\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1220 - val_loss: 0.2059\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.1265 - val_loss: 0.0982\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1249 - val_loss: 0.0119\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1142 - val_loss: 0.0787\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1118 - val_loss: 0.2220\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1176 - val_loss: 0.3891\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1300 - val_loss: 0.0364\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1041 - val_loss: 0.1737\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1013 - val_loss: 0.1412\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1229 - val_loss: 0.0132\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1138 - val_loss: 0.0111\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.1041 - val_loss: 0.0177\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1082 - val_loss: 0.3683\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0980 - val_loss: 0.5447\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1139 - val_loss: 0.2833\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1120 - val_loss: 0.5226\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1069 - val_loss: 0.1456\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1064 - val_loss: 0.0126\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1070 - val_loss: 0.0133\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1012 - val_loss: 0.0467\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1061 - val_loss: 0.2829\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.1066 - val_loss: 0.0171\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0926 - val_loss: 0.0432\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0973 - val_loss: 0.0116\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0907 - val_loss: 0.2513\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0995 - val_loss: 0.0411\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0888 - val_loss: 0.1081\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0971 - val_loss: 0.0479\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0925 - val_loss: 0.1758\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0903 - val_loss: 0.1297\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0862 - val_loss: 0.4144\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0912 - val_loss: 0.3009\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.1000 - val_loss: 0.0261\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0807 - val_loss: 0.0343\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0762 - val_loss: 0.1156\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0818 - val_loss: 0.1071\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 0.0807 - val_loss: 0.1052\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0830 - val_loss: 0.0503\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 0.0788 - val_loss: 0.2464\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0638 - val_loss: 0.2213\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0848 - val_loss: 0.0115\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0864 - val_loss: 0.0138\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0864 - val_loss: 0.0129\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0824 - val_loss: 0.0156\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0786 - val_loss: 0.0184\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0668 - val_loss: 0.0210\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0676 - val_loss: 0.0523\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0733 - val_loss: 0.0341\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0762 - val_loss: 0.0645\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0730 - val_loss: 0.0130\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0756 - val_loss: 0.0136\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0695 - val_loss: 0.0110\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0723 - val_loss: 0.0154\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0698 - val_loss: 0.0477\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0750 - val_loss: 0.0111\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0685 - val_loss: 0.0101\n",
      "Epoch 242/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0614 - val_loss: 0.0114\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0608 - val_loss: 0.0168\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0590 - val_loss: 0.0358\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0613 - val_loss: 0.0664\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0617 - val_loss: 0.0118\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0578 - val_loss: 0.0122\n",
      "Epoch 248/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 14s 3s/step - loss: 0.0712 - val_loss: 0.0113\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0647 - val_loss: 0.0106\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0487 - val_loss: 0.0125\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0535 - val_loss: 0.0490\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0525 - val_loss: 0.2778\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0553 - val_loss: 0.0105\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0544 - val_loss: 0.0107\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0488 - val_loss: 0.0199\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0536 - val_loss: 0.0358\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0531 - val_loss: 0.0266\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0478 - val_loss: 0.0280\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0488 - val_loss: 0.1139\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0494 - val_loss: 0.0161\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0492 - val_loss: 0.0115\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0458 - val_loss: 0.0104\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0525 - val_loss: 0.0113\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0430 - val_loss: 0.0143\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0443 - val_loss: 0.1483\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0436 - val_loss: 0.0159\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0426 - val_loss: 0.0758\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0418 - val_loss: 0.0112\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0362 - val_loss: 0.0390\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0361 - val_loss: 0.0906\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0376 - val_loss: 0.0591\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0386 - val_loss: 0.0352\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0363 - val_loss: 0.2053\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0380 - val_loss: 0.1724\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0362 - val_loss: 0.5284\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0338 - val_loss: 0.3013\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0301 - val_loss: 0.0991\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0320 - val_loss: 0.0863\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0294 - val_loss: 0.0545\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0311 - val_loss: 0.0507\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0312 - val_loss: 0.2000\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0294 - val_loss: 0.0580\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0356 - val_loss: 0.0132\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0366 - val_loss: 0.0100\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0306 - val_loss: 0.0123\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0260 - val_loss: 0.0112\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0281 - val_loss: 0.0136\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0258 - val_loss: 0.0151\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0219 - val_loss: 0.0332\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0247 - val_loss: 0.0230\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0275 - val_loss: 0.0715\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0252 - val_loss: 0.1029\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0257 - val_loss: 0.0134\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0232 - val_loss: 0.0160\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0222 - val_loss: 0.0401\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0273 - val_loss: 0.0113\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0226 - val_loss: 0.0106\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0222 - val_loss: 0.0114\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0218 - val_loss: 0.0131\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0240 - val_loss: 0.0161\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0208 - val_loss: 0.0218\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0205 - val_loss: 0.0143\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0221 - val_loss: 0.0134\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0216 - val_loss: 0.0121\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0192 - val_loss: 0.0101\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0206 - val_loss: 0.0126\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0192 - val_loss: 0.0109\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0206 - val_loss: 0.0106\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0172 - val_loss: 0.0125\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0198 - val_loss: 0.0114\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0188 - val_loss: 0.0109\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0185 - val_loss: 0.0105\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0174 - val_loss: 0.0104\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0168 - val_loss: 0.0130\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0162 - val_loss: 0.0098\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0169 - val_loss: 0.0133\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0162 - val_loss: 0.0099\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0155 - val_loss: 0.0121\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0185 - val_loss: 0.0106\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0163 - val_loss: 0.0084\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0152 - val_loss: 0.0097\n",
      "Epoch 322/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0146 - val_loss: 0.0118\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0185 - val_loss: 0.0099\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0155 - val_loss: 0.0163\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0154 - val_loss: 0.0107\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0160 - val_loss: 0.0129\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0130 - val_loss: 0.0116\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0125 - val_loss: 0.0109\n",
      "Epoch 330/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0138 - val_loss: 0.0125\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0134 - val_loss: 0.0101\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0108\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0133 - val_loss: 0.0124\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0118\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0100\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0125 - val_loss: 0.0110\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0135 - val_loss: 0.0123\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0151 - val_loss: 0.0123\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0132 - val_loss: 0.0100\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0127 - val_loss: 0.0097\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0126 - val_loss: 0.0109\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0102\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0169 - val_loss: 0.0128\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0102\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0139 - val_loss: 0.0125\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0108\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0103\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0124 - val_loss: 0.0126\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0132 - val_loss: 0.0102\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0112\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0129 - val_loss: 0.0114\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0132 - val_loss: 0.0108\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0140\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0127 - val_loss: 0.0102\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0125 - val_loss: 0.0105\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0133\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0103\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0135\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0170 - val_loss: 0.0116\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0122 - val_loss: 0.0133\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0089\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0120\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0100\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0091\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0120\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 412/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0108\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0115\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0119\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0115\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0142 - val_loss: 0.0110\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0124 - val_loss: 0.0100\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0127\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0104\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0135\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0122\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0104\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0126\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0086\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0119\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0091\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0139\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0096\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0130\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0128 - val_loss: 0.0097\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0151\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.1246\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0118 - val_loss: 0.0976\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0431\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0117 - val_loss: 0.0398\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0301\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0171\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0104\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0394\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0095\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 494/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0126\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0103\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0130 - val_loss: 0.0119\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0124 - val_loss: 0.0105\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0135\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0136 - val_loss: 0.0116\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0131\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0119\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.2487\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0764\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0126\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0114\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0121 - val_loss: 0.0124\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0092\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0135\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0114\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0132\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0131 - val_loss: 0.0124\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0140\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0132 - val_loss: 0.0121\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 576/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0150\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0102\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0109\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0091\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0191\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0114\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0109\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0088\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0396\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0443\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0165\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0129 - val_loss: 0.0122\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0140\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0129\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0101 - val_loss: 0.0133\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0086\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0116\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0131\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0117\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0113\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0362\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0184\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0154\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0134\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 658/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0090\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0124 - val_loss: 0.5483\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0128\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.2710\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0123 - val_loss: 0.0092\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.3221\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.1266\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0590\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0240\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0161\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0183\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0303\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0110\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0089\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0091\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0101 - val_loss: 0.0093\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0087\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0092\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0104\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0093\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.2620\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 3.5359\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0100 - val_loss: 0.4280\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0373\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0151\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0116\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0109\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0098 - val_loss: 0.0104\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 740/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0124 - val_loss: 0.0113\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0092\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0090\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0107\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0095\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0147\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0098 - val_loss: 0.0118\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0133\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0113\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0111\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0096\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0107\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0128 - val_loss: 0.0109\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.1313\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0207\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0134 - val_loss: 0.0122\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0091\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 822/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0132\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0092\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0086\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0082\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0090\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0137\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0126 - val_loss: 0.0097\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0118 - val_loss: 0.0097\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0096\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0097\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0130 - val_loss: 0.0106\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0141\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0128\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0124\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0128 - val_loss: 0.0102\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0097 - val_loss: 0.0101\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0096\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0108\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0118 - val_loss: 0.0101\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0081\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0141\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0125 - val_loss: 0.0125\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0135\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0091\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0091\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 904/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0084\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0098\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0096\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0113\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0126\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0137\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0099\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0091\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0135\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0109\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0093\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0086\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0094\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0135 - val_loss: 0.0107\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0132\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0125 - val_loss: 0.0113\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0101\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0125\n",
      "Epoch 986/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0086\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0112\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0091\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0122 - val_loss: 0.0101\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0105\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0138\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0114\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0097\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0095\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0132 - val_loss: 0.0102\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0094\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0093\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0130\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0133\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0117 - val_loss: 0.0104\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0115 - val_loss: 0.0136\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0116 - val_loss: 0.0130\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0083\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0118\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0093\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0096\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0137 - val_loss: 0.0092\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0096\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0129\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0092\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0142\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0129\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0134\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0088\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0087\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0093\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0097\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0134\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0096\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0136\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0125 - val_loss: 0.0101\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0096 - val_loss: 5.6632\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 1.0331\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.2806\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0808\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0195\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0126\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0090\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0108\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0097 - val_loss: 0.0130\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0086\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0129\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0127\n",
      "Epoch 1148/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0129\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0099\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.9608\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.3306\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0110 - val_loss: 0.0665\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0292\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.5677\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.1073\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0160\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0126 - val_loss: 0.0113\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0114\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0089\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0813\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0464\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0207\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0141\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0133\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0127 - val_loss: 0.0117\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0087\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0127\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0112\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0132\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0099\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0089\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0118 - val_loss: 0.0117\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0129 - val_loss: 0.0120\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0135\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0129\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0087\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0091\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0134 - val_loss: 0.0105\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0132\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0130\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0102\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0123 - val_loss: 0.0118\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0099\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0104\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0093\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0120 - val_loss: 0.0122\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0090\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0134\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0124\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0089\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0098\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0094\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0107\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0101\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0110\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0132\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0131 - val_loss: 0.0140\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0125 - val_loss: 0.0088\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0122 - val_loss: 0.0123\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0127 - val_loss: 0.0115\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0122\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0122 - val_loss: 0.0116\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1310/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0146\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0127\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0095\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0126\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0123 - val_loss: 0.0109\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0122 - val_loss: 0.0111\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0124\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0093\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0115 - val_loss: 0.0109\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0095 - val_loss: 0.0107\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0087\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0125 - val_loss: 0.0100\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0124 - val_loss: 0.0104\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0106\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0119\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0092\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0151\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0135\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0097\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0089\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0128\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0091\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0136\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0117\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0091\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0117\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0128 - val_loss: 0.0155\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0122\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0106\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0106\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0092\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0097\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0132\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0087\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0087\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0117\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0090\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0092\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0105\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1472/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0123\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0117\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0123 - val_loss: 0.0111\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0124\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0113\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0130\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0088\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0136\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0105\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0097 - val_loss: 0.0103\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0132\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0091\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0118 - val_loss: 0.0129\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0124 - val_loss: 0.0095\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0133 - val_loss: 0.0105\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0104 - val_loss: 0.0133\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0087\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0135 - val_loss: 0.0127\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0107\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0153\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0090\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0136\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0098\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0133\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0122\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0094\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0096\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0137\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0125 - val_loss: 0.0108\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0134\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0105\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0138\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0108\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0133\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0140\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0119 - val_loss: 0.0104\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0101\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0113\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0130 - val_loss: 0.0114\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0108\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 1634/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0087\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0108\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0130 - val_loss: 0.0112\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0086\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0127 - val_loss: 0.0093\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 7.5015\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.7266\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0743\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0195\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0163\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0121 - val_loss: 0.0097\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0120\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0119\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0114 - val_loss: 0.0102\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0094 - val_loss: 0.0112\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0100\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0122\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0113\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0089\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0094\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0139\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0128\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0109\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0092\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0109\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0098 - val_loss: 0.0112\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0121 - val_loss: 0.0112\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0118\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0143\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0127\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0126 - val_loss: 0.0111\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0124 - val_loss: 0.0100\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0110\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0097\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0136\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0130\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0134\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0119 - val_loss: 0.0130\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0113\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0144\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0122 - val_loss: 0.0101\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0140\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0119\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0121 - val_loss: 0.0110\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 22.2106\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 1.5803\n",
      "Epoch 1796/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.3172\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 131.0215\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0113 - val_loss: 5.4145\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 2.0618\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.9648\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0100 - val_loss: 0.4657\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.2099\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.1004\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0160\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0101 - val_loss: 0.0130\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0094\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0129\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0139\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0088\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0138\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0093\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0117 - val_loss: 0.0105\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0136\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0124\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0125\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0119\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0094\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0108\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0083\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0116\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0092\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0095\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0130\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0116 - val_loss: 0.0110\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0109 - val_loss: 0.0130\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0130 - val_loss: 0.0096\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0119 - val_loss: 0.0095\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 4.8881\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0102 - val_loss: 0.0720\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0091\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0105 - val_loss: 0.0133\n",
      "Epoch 1958/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0120\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0117 - val_loss: 0.0122\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0118 - val_loss: 0.0089\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0093\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0124 - val_loss: 0.0113\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0115 - val_loss: 0.0097\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0112 - val_loss: 0.0141\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0122 - val_loss: 0.0093\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 0.0113 - val_loss: 0.0145\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.0100 - val_loss: 0.0128\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(tdg, callbacks=[mc,tb], initial_epoch=0\n",
    "                           ,steps_per_epoch=train_steps_per_epoch\n",
    "                           ,validation_data=vdg\n",
    "                           ,validation_steps=val_steps_per_epoch\n",
    "                           ,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# acc = hist.history['acc']\n",
    "# loss = hist.history['loss']\n",
    "\n",
    "# # Create count of the number of epochs\n",
    "# epoch_count = range(1, len(acc) + 1)\n",
    "\n",
    "# # Visualize loss history\n",
    "# # plt.plot(epoch_count, acc, 'b-')\n",
    "# fig, ax = plt.subplots(ncols=2,sharex=True)\n",
    "# ax[0].plot(epoch_count, loss, 'r--')\n",
    "# ax[0].legend(['Loss'])\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].set_ylabel('Loss')\n",
    "# ax[1].plot(epoch_count, acc, 'b-')\n",
    "# ax[1].legend(['Accuracy'])\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].set_xlabel('Accuracy')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_res[['acc','val_acc']].plot()\n",
    "# hd_nlp = nlp(\"What is you name my name is Anthony Gonsalves What is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony Gonsalves!\".lower())\n",
    "# len(hd_nlp[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<generator..., steps=5, use_multiprocessing=True)`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01053589079529047"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/dnf700_sa_sent_hd_vector_gl.hdf5')\n",
    "model.evaluate_generator(test_dg,steps=5,pickle_safe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(test_dg)\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['breaking: fraudulent clinton votes discovered by the tens of thousands',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'top aide: hillary ‘still not perfect in her head’, wikileaks',\n",
       "       'julian assange makes very suspect post election announcement, seeks pardon from trump',\n",
       "       'breaking: fraudulent clinton votes discovered by the tens of thousands',\n",
       "       \"hillary clinton cut her tax bill by 'donating' $1 million to herself via the clinton foundation?\",\n",
       "       'pentagon officials furious after clinton announces us response time for nuclear launch during debate',\n",
       "       'hillary’s (islamic) america is already here where ‘muslim no-go zones’ are popping up all over michiganistan',\n",
       "       'wikileaks: hillary clinton knew saudi, qatar were funding isis – but still took their money for foundation',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       \"hillary clinton in 2013: 'i would like to see people like donald trump run for office\",\n",
       "       'hillary clinton wore secret earpiece during first presidential debate?',\n",
       "       'wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'pentagon officials furious after clinton announces us response time for nuclear launch during debate',\n",
       "       'top aide: hillary ‘still not perfect in her head’, wikileaks',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       'erdoğan: us, the founder of isis',\n",
       "       'isis leader calls for american muslim voters to support hillary clinton',\n",
       "       \"ted cruz said 'if something happens to hillary' he'll 'run as a democrat against trump'\",\n",
       "       'hillary friend bribed fbi agent and his wife',\n",
       "       'hillary’s (islamic) america is already here where ‘muslim no-go zones’ are popping up all over michiganistan',\n",
       "       \"hillary clinton in 2013: 'i would like to see people like donald trump run for office\",\n",
       "       'kremlin: putin congratulates trump, hopes to work together major issues',\n",
       "       \"hillary clinton's 'sudden move' of $1.8 billion to qatar central bank stuns financial world\",\n",
       "       'he’s never sold an original painting until now…and this one’s going in the white house',\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news',\n",
       "       'us officials see no link between trump and russia',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'breaking: fraudulent clinton votes discovered by the tens of thousands',\n",
       "       'reddit users declare war on hillary’s paid internet trolls',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'hillary’s (islamic) america is already here where ‘muslim no-go zones’ are popping up all over michiganistan',\n",
       "       'us threatens military hacks on russia’s electric, communications grids over election',\n",
       "       'fbi agent suspected in hillary email leaks found dead in apparent murder-suicide',\n",
       "       'developing: obama wh admits that hillary gave isis $400 million on accident',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'nsa whistleblower says dnc email hack was not by russia, but by us intelligence | alternative',\n",
       "       \"hillary clinton wore 'secret earpiece' during commander-in-chief forum\"],\n",
       "      dtype='<U108')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hillary clinton in 2013: 'i would like to see people like donald trump run for office\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = np.random.randint(0,50)\n",
    "display(x['headline'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Before running against billionaire real estate mogul Donald Trump for the presidency, Secretary of State Hillary Clinton told an audience at a private, paid speech she wanted to see more successful businessmen and women run for office because they can’t be bought.',\n",
       " 'In a speech made public by Wikileaks – which released an email from Hillary for America Research Director Tony Caark containing three attached speeches given at a private Goldman Sachs events – Clinton spoke and took audience questions at the “Builders and Innovators Summit” hosted by Goldman Sachs on October 29, 2013.',\n",
       " 'Answering a question about businessmen in politics, Clinton said that they are “most often the people that look over the horizon,” and therefore share a vision that many politicians of today lack. “',\n",
       " 'And that’s a very good question and thank you for asking it.',\n",
       " 'Yes, I would love to see more businessmen go into politics because I believe they would bring in an entirely different mindset and strategies than what we’re used to seeing traditionally,” she opined.',\n",
       " 'And then she just had to go on. “',\n",
       " 'In fact, when you say businessmen and women, I can’t help but think of a particular one that I would just love to see running for the presidency at some point in the future,” Clinton added. “',\n",
       " 'I don’t know what it is exactly about him, I can’t quite put my finger on it, but my instinct is almost never wrong.',\n",
       " 'And it’s telling me that Donald Trump would be very successful if he were to venture into politics in the future.”',\n",
       " 'Asked to elaborate on her statement, the former Secretary of State argued that she thinks that businessmen “can’t be bought” and that they’re “very honest.” “',\n",
       " 'And I think that goes especially for Donald Trump, whose successful projects and business ventures have made him synonymous with big business and, more importantly, creating thousands of jobs.',\n",
       " 'I also think he understands the philanthropic and charitable side of things quite well, which is a crucial skill for any politician,” she praised her current counter-candidate.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['sentences'][test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(model.inputs,model.get_layer(name='ca1').output)\n",
    "model_2 = Model(model.inputs,model.get_layer(name='ca2').output)\n",
    "model_3 = Model(model.inputs,model.get_layer(name='ca3').output)\n",
    "model_4 = Model(model.inputs,model.get_layer(name='ca4').output)\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b1+b2+b3+b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 10,  9,  8,  7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_N = 5\n",
    "t = b[test_idx][0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.685927"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "b[test_idx][0][:len(x['sentences'][test_idx])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hillary clinton in 2013: 'i would like to see people like donald trump run for office\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Before running against billionaire real estate mogul Donald Trump for the presidency, Secretary of State Hillary Clinton told an audience at a private, paid speech she wanted to see more successful businessmen and women run for office because they can’t be bought.',\n",
       " 'And it’s telling me that Donald Trump would be very successful if he were to venture into politics in the future.',\n",
       " 'Asked to elaborate on her statement, the former Secretary of State argued that she thinks that businessmen can’t be bought and that they’re very honest.',\n",
       " 'And I think that goes especially for Donald Trump, whose successful projects and business ventures have made him synonymous with big business and, more importantly, creating thousands of jobs.',\n",
       " 'I also think he understands the philanthropic and charitable side of things quite well, which is a crucial skill for any politician, she praised her current counter-candidate.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x['headline'][test_idx])\n",
    "display(x['claims'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 : I also think he understands the philanthropic and charitable side of things quite well, which is a crucial skill for any politician,” she praised her current counter-candidate.\n",
      "10 : And I think that goes especially for Donald Trump, whose successful projects and business ventures have made him synonymous with big business and, more importantly, creating thousands of jobs.\n",
      "9 : Asked to elaborate on her statement, the former Secretary of State argued that she thinks that businessmen “can’t be bought” and that they’re “very honest.” “\n",
      "8 : And it’s telling me that Donald Trump would be very successful if he were to venture into politics in the future.”\n",
      "7 : I don’t know what it is exactly about him, I can’t quite put my finger on it, but my instinct is almost never wrong.\n"
     ]
    }
   ],
   "source": [
    "for s in t:\n",
    "    if s>=len(x['sentences'][test_idx]):continue\n",
    "    print(s,':',x['sentences'][test_idx][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.685927"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "h_s_attended_vector = b[test_idx][0][:len(x['sentences'][test_idx])]\n",
    "h_s_attended_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s_attended_vector = pd.DataFrame(h_s_attended_vector)\n",
    "\n",
    "\n",
    "xw = df_h_s_attended_vector.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xw_scaled = min_max_scaler.fit_transform(xw)\n",
    "df_h_s_attended_vector = pd.DataFrame(xw_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feaf573a710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(2.0,7.0)})\n",
    "sns.heatmap(df_h_s_attended_vector, annot=True, cmap='YlGnBu', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa1 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa2 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa3 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa4 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s1 = Model(model.inputs,model.get_layer(name='sa1').output)\n",
    "model_s2 = Model(model.inputs,model.get_layer(name='sa2').output)\n",
    "model_s3 = Model(model.inputs,model.get_layer(name='sa3').output)\n",
    "model_s4 = Model(model.inputs,model.get_layer(name='sa4').output)\n",
    "model_s1.summary()\n",
    "model_s2.summary()\n",
    "model_s3.summary()\n",
    "model_s4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sb1, sg1 = model_s1.predict([x['sentence_vectors'],x['input_headline_vector']])\n",
    "_, sb2, sg2 = model_s2.predict(x)\n",
    "_, sb3, sg3 = model_s3.predict(x)\n",
    "_, sb4, sg4 = model_s4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = sb1[test_idx]#+sb2[test_idx]+sb3[test_idx]+sb4[test_idx]\n",
    "sb = sb[:len(x['sentences'][test_idx]),:len(x['sentences'][test_idx])]\n",
    "\n",
    "sb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = pd.DataFrame(sb)\n",
    "\n",
    "\n",
    "# zx = df_sb.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# zx_scaled = min_max_scaler.fit_transform(zx)\n",
    "# df_sb = pd.DataFrame(zx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feaf4dbf978>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(14.0,12.0)})\n",
    "sns.heatmap(pd.DataFrame(df_sb),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Before running against billionaire real estate mogul Donald Trump for the presidency, Secretary of State Hillary Clinton told an audience at a private, paid speech she wanted to see more successful businessmen and women run for office because they can’t be bought.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a speech made public by Wikileaks – which released an email from Hillary for America Research Director Tony Caark containing three attached speeches given at a private Goldman Sachs events – Clinton spoke and took audience questions at the “Builders and Innovators Summit” hosted by Goldman Sachs on October 29, 2013.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answering a question about businessmen in politics, Clinton said that they are “most often the people that look over the horizon,” and therefore share a vision that many politicians of today lack. “</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And that’s a very good question and thank you for asking it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes, I would love to see more businessmen go into politics because I believe they would bring in an entirely different mindset and strategies than what we’re used to seeing traditionally,” she opined.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And then she just had to go on. “</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In fact, when you say businessmen and women, I can’t help but think of a particular one that I would just love to see running for the presidency at some point in the future,” Clinton added. “</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don’t know what it is exactly about him, I can’t quite put my finger on it, but my instinct is almost never wrong.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>And it’s telling me that Donald Trump would be very successful if he were to venture into politics in the future.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Asked to elaborate on her statement, the former Secretary of State argued that she thinks that businessmen “can’t be bought” and that they’re “very honest.” “</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>And I think that goes especially for Donald Trump, whose successful projects and business ventures have made him synonymous with big business and, more importantly, creating thousands of jobs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I also think he understands the philanthropic and charitable side of things quite well, which is a crucial skill for any politician,” she praised her current counter-candidate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                   0\n",
       "0   Before running against billionaire real estate mogul Donald Trump for the presidency, Secretary of State Hillary Clinton told an audience at a private, paid speech she wanted to see more successful businessmen and women run for office because they can’t be bought.                                                        \n",
       "1   In a speech made public by Wikileaks – which released an email from Hillary for America Research Director Tony Caark containing three attached speeches given at a private Goldman Sachs events – Clinton spoke and took audience questions at the “Builders and Innovators Summit” hosted by Goldman Sachs on October 29, 2013.\n",
       "2   Answering a question about businessmen in politics, Clinton said that they are “most often the people that look over the horizon,” and therefore share a vision that many politicians of today lack. “                                                                                                                          \n",
       "3   And that’s a very good question and thank you for asking it.                                                                                                                                                                                                                                                                    \n",
       "4   Yes, I would love to see more businessmen go into politics because I believe they would bring in an entirely different mindset and strategies than what we’re used to seeing traditionally,” she opined.                                                                                                                        \n",
       "5   And then she just had to go on. “                                                                                                                                                                                                                                                                                               \n",
       "6   In fact, when you say businessmen and women, I can’t help but think of a particular one that I would just love to see running for the presidency at some point in the future,” Clinton added. “                                                                                                                                 \n",
       "7   I don’t know what it is exactly about him, I can’t quite put my finger on it, but my instinct is almost never wrong.                                                                                                                                                                                                            \n",
       "8   And it’s telling me that Donald Trump would be very successful if he were to venture into politics in the future.”                                                                                                                                                                                                              \n",
       "9   Asked to elaborate on her statement, the former Secretary of State argued that she thinks that businessmen “can’t be bought” and that they’re “very honest.” “                                                                                                                                                                  \n",
       "10  And I think that goes especially for Donald Trump, whose successful projects and business ventures have made him synonymous with big business and, more importantly, creating thousands of jobs.                                                                                                                                \n",
       "11  I also think he understands the philanthropic and charitable side of things quite well, which is a crucial skill for any politician,” she praised her current counter-candidate.                                                                                                                                                "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x['sentences'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf_eval():\n",
    "\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "\n",
    "    for idx in dnf_eval.id: \n",
    "        hd = dnf_eval[dnf_eval.id==idx]['headline'].values[0].lower()\n",
    "        ar_id = dnf_eval[dnf_eval.id==idx]['id'].values[0]\n",
    "        cl = dnf_eval[dnf_eval.id==idx]['claim_ids'].values[0]\n",
    "        ar_claims.append(cl)\n",
    "        sentences = articles300[ar_id]\n",
    "        vectors = article_vectors300[ar_id]\n",
    "\n",
    "\n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "    #         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "\n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "\n",
    "        inputs = {\n",
    "            'article_id': np.array(ar_ids)\n",
    "            ,'headline': np.array(hds)\n",
    "            ,'sentence_vectors' : np.array(ar_sents)\n",
    "            ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "            ,'claims':np.array(ar_claims)\n",
    "            ,'sentences':np.array(ar_sentences)\n",
    "        }\n",
    "        outputs = {\n",
    "            'headline_token_classes': np.array(ar_head_classes)\n",
    "            ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "        }\n",
    "    return inputs,outputs\n",
    "testX,testY = datagen_dnf_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display(testX['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.405, 0.5403333333333333, 0.4629795486600846)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(testX)\n",
    "_, b2, g2 = model_2.predict(testX)\n",
    "_, b3, g3 = model_3.predict(testX)\n",
    "_, b4, g4 = model_4.predict(testX)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in range(len(testX['headline'])):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(testX['claims'][test_idx])\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    pred = b[0][:len(testX['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "    \n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for p in pred:\n",
    "        if p in claims:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    for c in claims:\n",
    "        if c not in pred:\n",
    "            fn+=1\n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "#     print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "#     counter+=1\n",
    "#     if counter==5:\n",
    "#         break\n",
    "#     print(\"----------------------------\")\n",
    "#     for s in t:\n",
    "#         if s>=len(x['sentences'][test_idx]):continue\n",
    "#         x['sentences'][test_idx][s]\n",
    "np.average(ps), np.average(rs), 2*np.average(ps)*np.average(rs)/(np.average(ps)+ np.average(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20662ab176c4e739c8179df1ed90a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c324e7e8a04d0a9cba51640afea1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20425531914893613, 0.08813007323645622, 0.12313225423932511)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hd_tp_cdc = pd.read_csv('evaluation_set/cdc_ibm/headline_topic_mapping.csv')\n",
    "df_ar_cl_cdc = pd.read_csv('evaluation_set/cdc_ibm/article_claim_mapping.csv')\n",
    "df_hd_tp_dnf = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V4.json')\n",
    "df_hd_tp_dnf.columns = ['authors','claim_ids', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls']\n",
    "with open('evaluation_set/cdc_ibm/articles.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/cdc_ibm/article_vectors.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "df_hd_tp_cdc.keys(),df_ar_cl_cdc.keys(), len(articles.keys()), len(article_vectors.keys()), df_hd_tp_dnf.keys()\n",
    "test_titles = []\n",
    "for ar in df_ar_cl_cdc.Article.unique():\n",
    "    if len(df_ar_cl_cdc[df_ar_cl_cdc.Article==ar]['Claim'].values)>8:\n",
    "        test_titles.append(ar)\n",
    "ar_ids,ar_sents,ar_sentences,ar_head_vectors,ar_head_classes,hds,claims=[],[],[],[],[],[],[]\n",
    "for idx in tqdm_notebook(test_titles):\n",
    "#     print(idx)\n",
    "    hd = df_hd_tp_cdc[df_hd_tp_cdc.Title==idx]['Headline'].values[0].lower()\n",
    "    hds.append(hd)\n",
    "    ar_id = df_hd_tp_cdc[df_hd_tp_cdc.Title==idx]['article Id'].values[0]\n",
    "    cl = df_ar_cl_cdc[df_ar_cl_cdc.Article==idx]['Claim'].values\n",
    "    claims.append(cl)\n",
    "#     sentences=articles[ar_id]\n",
    "#     ar_sentences.append(ar_sentences)\n",
    "    #         print(len(sentences))\n",
    "    sents = np.zeros((max_sentences,300))\n",
    "    vectors = article_vectors[ar_id]\n",
    "    sents[:len(vectors)] = vectors[:max_sentences]\n",
    "    ar_ids.append(ar_id)\n",
    "    ar_sents.append(sents)\n",
    "    hd_nlp = nlp(hd.lower())\n",
    "    head_classes = np.zeros(50, dtype='int')\n",
    "    for i in range(len(hd_nlp)):\n",
    "        head_classes[i] = hd_nlp[i].rank\n",
    "    ar_head_vectors.append(hd_nlp.vector)\n",
    "    ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "inputs = {\n",
    "    'article_id': np.array(ar_ids)\n",
    "    ,'headline': np.array(hds)\n",
    "    ,'sentence_vectors' : np.array(ar_sents)\n",
    "#     ,'sentences' : np.array(ar_sentences)\n",
    "    ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "    ,'claims':np.array(claims)\n",
    "}\n",
    "outputs = {\n",
    "    'headline_token_classes': np.array(ar_head_classes)\n",
    "    ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "}\n",
    "threshold = 0.95\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(inputs)\n",
    "_, b2, g2 = model_2.predict(inputs)\n",
    "_, b3, g3 = model_3.predict(inputs)\n",
    "_, b4, g4 = model_4.predict(inputs)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in tqdm_notebook(range(len(inputs['headline']))):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(inputs['claims'][test_idx])\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    ids = b[0][:len(articles[inputs['article_id'][test_idx]])].argsort()[-best_N:][::-1]\n",
    "#     print(ids)\n",
    "    pred = np.array(articles[inputs['article_id'][test_idx]])[ids]\n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for i in range(len(pred)):\n",
    "    #     print('===========***********',i,'***********============')\n",
    "        t5 = nlp(str(pred[i]))\n",
    "        flag = False\n",
    "        #pred_claim_sent.append(pred[i])\n",
    "    #     print(t5.vector)\n",
    "        for j in range(len(cl)):\n",
    "            _c = nlp(cl[j])\n",
    "    #         print(_c.vector)\n",
    "    #         print('top_5:',t5.text)\n",
    "    #         print('-------------------')\n",
    "    #         print('ground_truth:',_c.text)\n",
    "    #         print('t5:{0}, cl:{1}, sim: {2}'.format(i,j,np.around(t5.similarity(_c),4)))\n",
    "    #         print('===================================================================')\n",
    "            if np.around(t5.similarity(_c),4) > threshold:\n",
    "                tp+=1\n",
    "                flag = True\n",
    "                break\n",
    "        if flag is False:\n",
    "            fp+=1\n",
    "        \n",
    "            \n",
    "   \n",
    "    #     print(t5.vector)\n",
    "    for j in range(len(cl)):\n",
    "        _c = nlp(cl[j])\n",
    "        flag = False\n",
    "        for i in range(len(pred)):\n",
    "    #     print('===========***********',i,'***********============')\n",
    "            t5 = nlp(str(pred[i]))\n",
    "        \n",
    "    #         print(_c.vector)\n",
    "    #         print('top_5:',t5.text)\n",
    "    #         print('-------------------')\n",
    "    #         print('ground_truth:',_c.text)\n",
    "    #         print('t5:{0}, cl:{1}, sim: {2}'.format(i,j,np.around(t5.similarity(_c),4)))\n",
    "    #         print('===================================================================')\n",
    "            if np.around(t5.similarity(_c),4) > threshold:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag is False:\n",
    "            fn+=1\n",
    "         \n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "#     print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "\n",
    "np.average(ps), np.average(rs), 2*np.average(ps)*np.average(rs)/(np.average(ps)+ np.average(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
