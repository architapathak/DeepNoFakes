{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from spacy.lang.en import English\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.layers import BatchNormalization, Lambda, Concatenate, Dropout, Conv1D, MaxPooling1D, Input, TimeDistributed, Dense, LSTM, RepeatVector, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from AttentionModules import SelfAttention,CrossAttention\n",
    "import sys,os\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['authors', 'claim_ids', 'evidence', 'headline', 'id', 'reason',\n",
       "        'claims', 'type', 'urls'],\n",
       "       dtype='object'),\n",
       " Index(['authors', 'evidence', 'headline', 'id', 'reason', 'type', 'urls'], dtype='object'),\n",
       " 300,\n",
       " 300)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnf300 = pd.read_json('evaluation_set/deepnofakes/dnf_300/combined_300.json').T\n",
    "dnf_eval = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V4.json')\n",
    "# display(dnf_eval.head(2))\n",
    "dnf_eval.columns = ['authors','claim_ids', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls'] \n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_array_id.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "dnf_eval.keys(), dnf300.keys(), len(articles.keys()), len(article_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits)\n",
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "test_batchsize = 50\n",
    "train_steps_per_epoch = 4\n",
    "val_steps_per_epoch = 1\n",
    "epochs = 2000\n",
    "max_sentences = 0\n",
    "for idx in articles.keys():\n",
    "    num = len(articles[idx])\n",
    "    if num>=max_sentences:\n",
    "        max_sentences = num\n",
    "        \n",
    "max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sorted(dnf300.headline.unique())\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_titles = sorted(dnf_eval.headline.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = sorted(dnf300.headline.unique())\n",
    "non_test_titles = np.array(list(set(titles)-set(test_titles)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for train_index, val_index in kf.split(non_test_titles):\n",
    "    indices.append([train_index,val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252] [153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170\n",
      " 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188\n",
      " 189 190 191 192 193 194 195 196 197 198 199 200 201 202]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(203, 50, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_index, val_index = indices[np.random.randint(0,num_splits)]\n",
    "print(train_index,val_index)\n",
    "val_titles = non_test_titles[val_index]\n",
    "train_titles = non_test_titles[train_index]\n",
    "len(train_titles),len(val_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    sentencizer = English()\n",
    "    sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return sentencizer, nlp\n",
    "sentencizer, nlp = load_spacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        idx = idx.strip()\n",
    "        \n",
    "            \n",
    "#         cl = dataframe[dataframe.Article==idx]['Claim'].values\n",
    "#         sentences=articles[ar_id]\n",
    "#         print(len(sentences))\n",
    "        if mode=='test':\n",
    "            hd = dnf_eval[dnf_eval.headline==idx]['headline'].values[0].lower()\n",
    "            ar_id = dnf_eval[dnf_eval.headline==idx]['id'].values[0]\n",
    "            cl = dnf_eval[dnf_eval.headline==idx]['claim_ids'].values[0]\n",
    "            ar_claims.append(cl)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                hd = dataframe[dataframe.headline==idx]['headline'].values[0].lower()\n",
    "                ar_id = dataframe[dataframe.headline==idx]['id'].values[0]\n",
    "                ar_claims.append('None')\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(idx)\n",
    "        sentences = articles[ar_id]\n",
    "        vectors = article_vectors[ar_id]\n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "        \n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = datagen_dnf(train_batchsize,dnf300,mode='train')\n",
    "vdg = datagen_dnf(val_batchsize,dnf300,mode='val')\n",
    "test_dg = datagen_dnf(test_batchsize,dnf300,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(test_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 4, 6]), list([1, 2, 4]), list([0, 2, 3]), list([3, 4, 5]),\n",
       "       list([0, 1, 2, 3]), list([0, 2, 4, 6, 7]), list([0, 5, 6]),\n",
       "       list([0, 5, 18]), list([0, 1, 2]), list([0, 8, 9, 10, 11]),\n",
       "       list([2, 3]), list([1, 2, 3]), list([0, 2, 3]), list([0, 1, 6]),\n",
       "       list([1, 2, 5, 6]), list([2, 4, 6]), list([0, 2, 3]),\n",
       "       list([0, 5, 7]), list([1, 5, 13]), list([0, 2, 3]),\n",
       "       list([0, 4, 6]), list([0, 1, 3, 9, 10]), list([0, 2, 3]),\n",
       "       list([4, 7, 8, 9]), list([0, 1]), list([0, 5, 18]),\n",
       "       list([0, 1, 2, 4]), list([0, 5, 7]), list([0, 1, 4, 5]),\n",
       "       list([0, 1, 2, 6]), list([0, 2, 3]), list([1, 2, 5, 6]),\n",
       "       list([0, 1, 2, 6]), list([0, 4, 5]), list([0, 1, 5, 10]),\n",
       "       list([0, 5, 7]), list([2, 9, 10, 12, 13]), list([0, 2, 3]),\n",
       "       list([0, 5, 18]), list([0, 2, 4, 10]), list([0, 2, 3, 4]),\n",
       "       list([3, 6, 7, 8, 10]), list([0, 3, 4]), list([2, 4, 6]),\n",
       "       list([3, 4, 5]), list([0, 1, 2, 4]), list([0, 2, 3]),\n",
       "       list([0, 1, 6]), list([3, 4, 5]), list([0, 1, 2])], dtype=object)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['sentence_vectors'].shape, x['headline_vector'].shape, y['headline_token_classes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 32)       1568        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 35, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 32)       128         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 256)      98560       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 35, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 256)       1024        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 256)      1024        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 35, 1024)     0           ca1[0][0]                        \n",
      "                                                                 ca2[0][0]                        \n",
      "                                                                 ca3[0][0]                        \n",
      "                                                                 ca4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 35, 1024)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 1024)     4096        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 18, 256)      786688      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 9, 256)       196864      conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5, 256)       196864      conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 3, 256)       196864      conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 3, 256)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 3, 256)       1024        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 256)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          131584      global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512)          2048        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_headline_vector (Dense)  (None, 300)          153900      batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,465,348\n",
      "Trainable params: 2,460,676\n",
      "Non-trainable params: 4,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2130pt\" viewBox=\"0.00 0.00 1754.00 2130.00\" width=\"1754pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2126)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-2126 1750,-2126 1750,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140033052460312 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140033052460312</title>\n",
       "<polygon fill=\"none\" points=\"843.5,-2075.5 843.5,-2121.5 1178.5,-2121.5 1178.5,-2075.5 843.5,-2075.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"931.5\" y=\"-2094.8\">sentence_vectors: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1019.5,-2075.5 1019.5,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1047\" y=\"-2106.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1019.5,-2098.5 1074.5,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1047\" y=\"-2083.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1074.5,-2075.5 1074.5,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1126.5\" y=\"-2106.3\">(None, 35, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1074.5,-2098.5 1178.5,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1126.5\" y=\"-2083.3\">(None, 35, 300)</text>\n",
       "</g>\n",
       "<!-- 140033052462272 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140033052462272</title>\n",
       "<polygon fill=\"none\" points=\"870,-1992.5 870,-2038.5 1152,-2038.5 1152,-1992.5 870,-1992.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"931.5\" y=\"-2011.8\">conv1d_8: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"993,-1992.5 993,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1020.5\" y=\"-2023.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"993,-2015.5 1048,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1020.5\" y=\"-2000.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1048,-1992.5 1048,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1100\" y=\"-2023.3\">(None, 35, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1048,-2015.5 1152,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1100\" y=\"-2000.3\">(None, 35, 16)</text>\n",
       "</g>\n",
       "<!-- 140033052460312&#45;&gt;140033052462272 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140033052460312-&gt;140033052462272</title>\n",
       "<path d=\"M1011,-2075.3799C1011,-2067.1745 1011,-2057.7679 1011,-2048.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1014.5001,-2048.784 1011,-2038.784 1007.5001,-2048.784 1014.5001,-2048.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140033203871584 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140033203871584</title>\n",
       "<polygon fill=\"none\" points=\"872.5,-1909.5 872.5,-1955.5 1149.5,-1955.5 1149.5,-1909.5 872.5,-1909.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-1928.8\">dropout_7: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"997.5,-1909.5 997.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1025\" y=\"-1940.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"997.5,-1932.5 1052.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1025\" y=\"-1917.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1052.5,-1909.5 1052.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101\" y=\"-1940.3\">(None, 35, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1052.5,-1932.5 1149.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101\" y=\"-1917.3\">(None, 35, 16)</text>\n",
       "</g>\n",
       "<!-- 140033052462272&#45;&gt;140033203871584 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140033052462272-&gt;140033203871584</title>\n",
       "<path d=\"M1011,-1992.3799C1011,-1984.1745 1011,-1974.7679 1011,-1965.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1014.5001,-1965.784 1011,-1955.784 1007.5001,-1965.784 1014.5001,-1965.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140033203867944 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140033203867944</title>\n",
       "<polygon fill=\"none\" points=\"873.5,-1826.5 873.5,-1872.5 1148.5,-1872.5 1148.5,-1826.5 873.5,-1826.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-1845.8\">conv1d_9: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"996.5,-1826.5 996.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1024\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"996.5,-1849.5 1051.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1024\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1051.5,-1826.5 1051.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1100\" y=\"-1857.3\">(None, 35, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1051.5,-1849.5 1148.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1100\" y=\"-1834.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140033203871584&#45;&gt;140033203867944 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140033203871584-&gt;140033203867944</title>\n",
       "<path d=\"M1011,-1909.3799C1011,-1901.1745 1011,-1891.7679 1011,-1882.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1014.5001,-1882.784 1011,-1872.784 1007.5001,-1882.784 1014.5001,-1882.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218270536 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140030218270536</title>\n",
       "<polygon fill=\"none\" points=\"872.5,-1743.5 872.5,-1789.5 1149.5,-1789.5 1149.5,-1743.5 872.5,-1743.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-1762.8\">dropout_8: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"997.5,-1743.5 997.5,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1025\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"997.5,-1766.5 1052.5,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1025\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1052.5,-1743.5 1052.5,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101\" y=\"-1774.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1052.5,-1766.5 1149.5,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101\" y=\"-1751.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140033203867944&#45;&gt;140030218270536 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140033203867944-&gt;140030218270536</title>\n",
       "<path d=\"M1011,-1826.3799C1011,-1818.1745 1011,-1808.7679 1011,-1799.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1014.5001,-1799.784 1011,-1789.784 1007.5001,-1799.784 1014.5001,-1799.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218847400 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140030218847400</title>\n",
       "<polygon fill=\"none\" points=\"805,-1660.5 805,-1706.5 1217,-1706.5 1217,-1660.5 805,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"935\" y=\"-1679.8\">batch_normalization_7: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1065,-1660.5 1065,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1092.5\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1065,-1683.5 1120,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1092.5\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1120,-1660.5 1120,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1168.5\" y=\"-1691.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1120,-1683.5 1217,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1168.5\" y=\"-1668.3\">(None, 35, 32)</text>\n",
       "</g>\n",
       "<!-- 140030218270536&#45;&gt;140030218847400 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140030218270536-&gt;140030218847400</title>\n",
       "<path d=\"M1011,-1743.3799C1011,-1735.1745 1011,-1725.7679 1011,-1716.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1014.5001,-1716.784 1011,-1706.784 1007.5001,-1716.784 1014.5001,-1716.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218781360 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140030218781360</title>\n",
       "<polygon fill=\"none\" points=\"276,-1577.5 276,-1623.5 630,-1623.5 630,-1577.5 276,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"333.5\" y=\"-1596.8\">sa1: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"391,-1577.5 391,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"391,-1600.5 446,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"446,-1577.5 446,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"446,-1600.5 630,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"538\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030218847400&#45;&gt;140030218781360 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140030218847400-&gt;140030218781360</title>\n",
       "<path d=\"M856.3069,-1660.4901C782.225,-1649.4707 693.4453,-1636.2652 618.0639,-1625.0525\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"618.4002,-1621.5641 607.994,-1623.5547 617.3702,-1628.4879 618.4002,-1621.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218195520 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140030218195520</title>\n",
       "<polygon fill=\"none\" points=\"648,-1577.5 648,-1623.5 1002,-1623.5 1002,-1577.5 648,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"705.5\" y=\"-1596.8\">sa2: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"763,-1577.5 763,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"763,-1600.5 818,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"790.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"818,-1577.5 818,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"818,-1600.5 1002,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030218847400&#45;&gt;140030218195520 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140030218847400-&gt;140030218195520</title>\n",
       "<path d=\"M959.1886,-1660.3799C936.5087,-1650.2592 909.7331,-1638.311 886.0891,-1627.7602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"887.4141,-1624.5188 876.8558,-1623.6399 884.5615,-1630.9113 887.4141,-1624.5188\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218505856 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140030218505856</title>\n",
       "<polygon fill=\"none\" points=\"1020,-1577.5 1020,-1623.5 1374,-1623.5 1374,-1577.5 1020,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1077.5\" y=\"-1596.8\">sa3: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1135,-1577.5 1135,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1162.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1135,-1600.5 1190,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1162.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1190,-1577.5 1190,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1282\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1190,-1600.5 1374,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1282\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030218847400&#45;&gt;140030218505856 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140030218847400-&gt;140030218505856</title>\n",
       "<path d=\"M1062.8114,-1660.3799C1085.4913,-1650.2592 1112.2669,-1638.311 1135.9109,-1627.7602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1137.4385,-1630.9113 1145.1442,-1623.6399 1134.5859,-1624.5188 1137.4385,-1630.9113\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218662520 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140030218662520</title>\n",
       "<polygon fill=\"none\" points=\"1392,-1577.5 1392,-1623.5 1746,-1623.5 1746,-1577.5 1392,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1449.5\" y=\"-1596.8\">sa4: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1507,-1577.5 1507,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1534.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1507,-1600.5 1562,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1534.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1562,-1577.5 1562,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1654\" y=\"-1608.3\">(None, 35, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1562,-1600.5 1746,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1654\" y=\"-1585.3\">[(None, 35, 32), (35, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030218847400&#45;&gt;140030218662520 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140030218847400-&gt;140030218662520</title>\n",
       "<path d=\"M1165.6931,-1660.4901C1239.775,-1649.4707 1328.5547,-1636.2652 1403.9361,-1625.0525\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1404.6298,-1628.4879 1414.006,-1623.5547 1403.5998,-1621.5641 1404.6298,-1628.4879\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218663416 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>140030218663416</title>\n",
       "<polygon fill=\"none\" points=\"715,-1494.5 715,-1540.5 1307,-1540.5 1307,-1494.5 715,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"799\" y=\"-1513.8\">concatenate_3: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"883,-1494.5 883,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"883,-1517.5 938,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"938,-1494.5 938,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1122.5\" y=\"-1525.3\">[(None, 35, 32), (None, 35, 32), (None, 35, 32), (None, 35, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"938,-1517.5 1307,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1122.5\" y=\"-1502.3\">(None, 35, 128)</text>\n",
       "</g>\n",
       "<!-- 140030218781360&#45;&gt;140030218663416 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140030218781360-&gt;140030218663416</title>\n",
       "<path d=\"M607.6931,-1577.4901C681.775,-1566.4707 770.5547,-1553.2652 845.9361,-1542.0525\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"846.6298,-1545.4879 856.006,-1540.5547 845.5998,-1538.5641 846.6298,-1545.4879\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218195520&#45;&gt;140030218663416 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140030218195520-&gt;140030218663416</title>\n",
       "<path d=\"M876.8114,-1577.3799C899.4913,-1567.2592 926.2669,-1555.311 949.9109,-1544.7602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"951.4385,-1547.9113 959.1442,-1540.6399 948.5859,-1541.5188 951.4385,-1547.9113\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218505856&#45;&gt;140030218663416 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>140030218505856-&gt;140030218663416</title>\n",
       "<path d=\"M1145.1886,-1577.3799C1122.5087,-1567.2592 1095.7331,-1555.311 1072.0891,-1544.7602\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1073.4141,-1541.5188 1062.8558,-1540.6399 1070.5615,-1547.9113 1073.4141,-1541.5188\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030218662520&#45;&gt;140030218663416 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>140030218662520-&gt;140030218663416</title>\n",
       "<path d=\"M1414.3069,-1577.4901C1340.225,-1566.4707 1251.4453,-1553.2652 1176.0639,-1542.0525\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1176.4002,-1538.5641 1165.994,-1540.5547 1175.3702,-1545.4879 1176.4002,-1538.5641\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140033052460144 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>140033052460144</title>\n",
       "<polygon fill=\"none\" points=\"353.5,-1494.5 353.5,-1540.5 696.5,-1540.5 696.5,-1494.5 353.5,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456\" y=\"-1513.8\">input_headline_vector: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"558.5,-1494.5 558.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"586\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"558.5,-1517.5 613.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"586\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"613.5,-1494.5 613.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655\" y=\"-1525.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"613.5,-1517.5 696.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655\" y=\"-1502.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140030138704616 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>140030138704616</title>\n",
       "<polygon fill=\"none\" points=\"431,-1411.5 431,-1457.5 671,-1457.5 671,-1411.5 431,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"482\" y=\"-1430.8\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"533,-1411.5 533,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"560.5\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"533,-1434.5 588,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"560.5\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"588,-1411.5 588,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"629.5\" y=\"-1442.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"588,-1434.5 671,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"629.5\" y=\"-1419.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140033052460144&#45;&gt;140030138704616 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>140033052460144-&gt;140030138704616</title>\n",
       "<path d=\"M532.2424,-1494.3799C534.8407,-1486.0854 537.8236,-1476.5633 540.6348,-1467.5889\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"544.0568,-1468.373 543.7062,-1457.784 537.3769,-1466.2805 544.0568,-1468.373\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139772096 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>140030139772096</title>\n",
       "<polygon fill=\"none\" points=\"864.5,-1411.5 864.5,-1457.5 1153.5,-1457.5 1153.5,-1411.5 864.5,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"929.5\" y=\"-1430.8\">conv1d_10: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"994.5,-1411.5 994.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1022\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"994.5,-1434.5 1049.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1022\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1049.5,-1411.5 1049.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101.5\" y=\"-1442.3\">(None, 35, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1049.5,-1434.5 1153.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1101.5\" y=\"-1419.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140030218663416&#45;&gt;140030139772096 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>140030218663416-&gt;140030139772096</title>\n",
       "<path d=\"M1010.4429,-1494.3799C1010.2452,-1486.1745 1010.0185,-1476.7679 1009.8043,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1013.301,-1467.6968 1009.5611,-1457.784 1006.3031,-1467.8655 1013.301,-1467.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138814536 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>140030138814536</title>\n",
       "<polygon fill=\"none\" points=\"426.5,-1328.5 426.5,-1374.5 701.5,-1374.5 701.5,-1328.5 426.5,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-1347.8\">lambda_2: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"549.5,-1328.5 549.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"577\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"549.5,-1351.5 604.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"577\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"604.5,-1328.5 604.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653\" y=\"-1359.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"604.5,-1351.5 701.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653\" y=\"-1336.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140030138704616&#45;&gt;140030138814536 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>140030138704616-&gt;140030138814536</title>\n",
       "<path d=\"M554.6212,-1411.3799C555.9064,-1403.1745 557.3797,-1393.7679 558.772,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"562.2635,-1385.2052 560.3531,-1374.784 555.3478,-1384.1219 562.2635,-1385.2052\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138705624 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>140030138705624</title>\n",
       "<polygon fill=\"none\" points=\"864,-1328.5 864,-1374.5 1148,-1374.5 1148,-1328.5 864,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"926.5\" y=\"-1347.8\">dropout_9: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"989,-1328.5 989,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1016.5\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"989,-1351.5 1044,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1016.5\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1044,-1328.5 1044,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1096\" y=\"-1359.3\">(None, 35, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1044,-1351.5 1148,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1096\" y=\"-1336.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140030139772096&#45;&gt;140030138705624 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>140030139772096-&gt;140030138705624</title>\n",
       "<path d=\"M1008.1643,-1411.3799C1007.8678,-1403.1745 1007.5278,-1393.7679 1007.2065,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1010.7006,-1384.651 1006.8416,-1374.784 1003.7052,-1384.9039 1010.7006,-1384.651\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138818400 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>140030138818400</title>\n",
       "<polygon fill=\"none\" points=\"364,-1245.5 364,-1291.5 776,-1291.5 776,-1245.5 364,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"494\" y=\"-1264.8\">batch_normalization_9: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"624,-1245.5 624,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"624,-1268.5 679,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"651.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"679,-1245.5 679,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727.5\" y=\"-1276.3\">(None, 1, 256)</text>\n",
       "<polyline fill=\"none\" points=\"679,-1268.5 776,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727.5\" y=\"-1253.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 140030138814536&#45;&gt;140030138818400 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>140030138814536-&gt;140030138818400</title>\n",
       "<path d=\"M565.6713,-1328.3799C566.2645,-1320.1745 566.9445,-1310.7679 567.5871,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"571.0866,-1302.0104 568.3168,-1291.784 564.1048,-1301.5056 571.0866,-1302.0104\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138729136 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>140030138729136</title>\n",
       "<polygon fill=\"none\" points=\"794.5,-1245.5 794.5,-1291.5 1213.5,-1291.5 1213.5,-1245.5 794.5,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"924.5\" y=\"-1264.8\">batch_normalization_8: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1054.5,-1245.5 1054.5,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1082\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1054.5,-1268.5 1109.5,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1082\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1109.5,-1245.5 1109.5,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161.5\" y=\"-1276.3\">(None, 35, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1109.5,-1268.5 1213.5,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161.5\" y=\"-1253.3\">(None, 35, 256)</text>\n",
       "</g>\n",
       "<!-- 140030138705624&#45;&gt;140030138729136 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>140030138705624-&gt;140030138729136</title>\n",
       "<path d=\"M1005.4429,-1328.3799C1005.2452,-1320.1745 1005.0185,-1310.7679 1004.8043,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1008.301,-1301.6968 1004.5611,-1291.784 1001.3031,-1301.8655 1008.301,-1301.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138536400 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>140030138536400</title>\n",
       "<polygon fill=\"none\" points=\"796,-1162.5 796,-1208.5 1176,-1208.5 1176,-1162.5 796,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858.5\" y=\"-1181.8\">ca1: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"921,-1162.5 921,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"921,-1185.5 976,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"976,-1162.5 976,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1076\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"976,-1185.5 1176,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1076\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030138818400&#45;&gt;140030138536400 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>140030138818400-&gt;140030138536400</title>\n",
       "<path d=\"M685.3268,-1245.4901C739.7673,-1234.6282 804.8545,-1221.642 860.5271,-1210.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"861.327,-1213.9437 870.4489,-1208.5547 859.9573,-1207.079 861.327,-1213.9437\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138506040 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>140030138506040</title>\n",
       "<polygon fill=\"none\" points=\"1194,-1162.5 1194,-1208.5 1574,-1208.5 1574,-1162.5 1194,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1256.5\" y=\"-1181.8\">ca2: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"1319,-1162.5 1319,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1346.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1319,-1185.5 1374,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1346.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1374,-1162.5 1374,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1474\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1374,-1185.5 1574,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1474\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030138818400&#45;&gt;140030138506040 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>140030138818400-&gt;140030138506040</title>\n",
       "<path d=\"M776.0691,-1245.995C779.4034,-1245.6587 782.7156,-1245.3268 786,-1245 959.8912,-1227.6974 1006.1987,-1227.4458 1183.6822,-1209.1239\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1184.3052,-1212.5782 1193.8908,-1208.0652 1183.5831,-1205.6155 1184.3052,-1212.5782\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138505704 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>140030138505704</title>\n",
       "<polygon fill=\"none\" points=\"0,-1162.5 0,-1208.5 380,-1208.5 380,-1162.5 0,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-1181.8\">ca3: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"125,-1162.5 125,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"125,-1185.5 180,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"180,-1162.5 180,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"180,-1185.5 380,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030138818400&#45;&gt;140030138505704 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>140030138818400-&gt;140030138505704</title>\n",
       "<path d=\"M464.6534,-1245.4901C415.233,-1234.6956 356.2077,-1221.8033 305.5633,-1210.7415\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"306.068,-1207.2693 295.5515,-1208.5547 304.5743,-1214.108 306.068,-1207.2693\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030140746832 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>140030140746832</title>\n",
       "<polygon fill=\"none\" points=\"398,-1162.5 398,-1208.5 778,-1208.5 778,-1162.5 398,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"460.5\" y=\"-1181.8\">ca4: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"523,-1162.5 523,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"523,-1185.5 578,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"578,-1162.5 578,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678\" y=\"-1193.3\">[(None, 1, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"578,-1185.5 778,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678\" y=\"-1170.3\">[(None, 35, 256), (1, 35), (1,)]</text>\n",
       "</g>\n",
       "<!-- 140030138818400&#45;&gt;140030140746832 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>140030138818400-&gt;140030140746832</title>\n",
       "<path d=\"M575.014,-1245.3799C576.8128,-1237.0854 578.8778,-1227.5633 580.8241,-1218.5889\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"584.2515,-1219.2987 582.9505,-1208.784 577.4105,-1217.815 584.2515,-1219.2987\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138729136&#45;&gt;140030138536400 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>140030138729136-&gt;140030138536400</title>\n",
       "<path d=\"M998.986,-1245.3799C997.1872,-1237.0854 995.1222,-1227.5633 993.1759,-1218.5889\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"996.5895,-1217.815 991.0495,-1208.784 989.7485,-1219.2987 996.5895,-1217.815\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138729136&#45;&gt;140030138506040 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>140030138729136-&gt;140030138506040</title>\n",
       "<path d=\"M1109.3466,-1245.4901C1158.767,-1234.6956 1217.7923,-1221.8033 1268.4367,-1210.7415\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1269.4257,-1214.108 1278.4485,-1208.5547 1267.932,-1207.2693 1269.4257,-1214.108\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138729136&#45;&gt;140030138505704 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>140030138729136-&gt;140030138505704</title>\n",
       "<path d=\"M794.4885,-1245.9458C791.3038,-1245.6266 788.1393,-1245.3111 785,-1245 612.3988,-1227.8951 566.4607,-1227.3781 390.2452,-1209.1173\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"390.4181,-1205.6165 380.1095,-1208.0624 389.6934,-1212.5789 390.4181,-1205.6165\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138729136&#45;&gt;140030140746832 -->\n",
       "<g class=\"edge\" id=\"edge27\">\n",
       "<title>140030138729136-&gt;140030140746832</title>\n",
       "<path d=\"M888.6732,-1245.4901C834.2327,-1234.6282 769.1455,-1221.642 713.4729,-1210.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"714.0427,-1207.079 703.5511,-1208.5547 712.673,-1213.9437 714.0427,-1207.079\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030140917128 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>140030140917128</title>\n",
       "<polygon fill=\"none\" points=\"477.5,-1079.5 477.5,-1125.5 1096.5,-1125.5 1096.5,-1079.5 477.5,-1079.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-1098.8\">concatenate_4: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1079.5 645.5,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1102.5 700.5,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1079.5 700.5,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"898.5\" y=\"-1110.3\">[(None, 35, 256), (None, 35, 256), (None, 35, 256), (None, 35, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1102.5 1096.5,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"898.5\" y=\"-1087.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140030138536400&#45;&gt;140030140917128 -->\n",
       "<g class=\"edge\" id=\"edge28\">\n",
       "<title>140030138536400-&gt;140030140917128</title>\n",
       "<path d=\"M930.8317,-1162.4901C906.245,-1152.2353 877.1188,-1140.0872 851.5287,-1129.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"852.8524,-1126.1739 842.2756,-1125.5547 850.1577,-1132.6345 852.8524,-1126.1739\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138506040&#45;&gt;140030140917128 -->\n",
       "<g class=\"edge\" id=\"edge29\">\n",
       "<title>140030138506040-&gt;140030140917128</title>\n",
       "<path d=\"M1218.495,-1162.4901C1138.9908,-1151.4367 1043.6645,-1138.1837 962.8547,-1126.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"962.9548,-1123.4291 952.568,-1125.5187 961.9908,-1130.3625 962.9548,-1123.4291\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138505704&#45;&gt;140030140917128 -->\n",
       "<g class=\"edge\" id=\"edge30\">\n",
       "<title>140030138505704-&gt;140030140917128</title>\n",
       "<path d=\"M355.505,-1162.4901C435.0092,-1151.4367 530.3355,-1138.1837 611.1453,-1126.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"612.0092,-1130.3625 621.432,-1125.5187 611.0452,-1123.4291 612.0092,-1130.3625\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030140746832&#45;&gt;140030140917128 -->\n",
       "<g class=\"edge\" id=\"edge31\">\n",
       "<title>140030140746832-&gt;140030140917128</title>\n",
       "<path d=\"M643.1683,-1162.4901C667.755,-1152.2353 696.8812,-1140.0872 722.4713,-1129.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"723.8423,-1132.6345 731.7244,-1125.5547 721.1476,-1126.1739 723.8423,-1132.6345\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139464896 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>140030139464896</title>\n",
       "<polygon fill=\"none\" points=\"639,-996.5 639,-1042.5 935,-1042.5 935,-996.5 639,-996.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704.5\" y=\"-1015.8\">dropout_10: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"770,-996.5 770,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797.5\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"770,-1019.5 825,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797.5\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"825,-996.5 825,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-1027.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"825,-1019.5 935,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-1004.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140030140917128&#45;&gt;140030139464896 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>140030140917128-&gt;140030139464896</title>\n",
       "<path d=\"M787,-1079.3799C787,-1071.1745 787,-1061.7679 787,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-1052.784 787,-1042.784 783.5001,-1052.784 790.5001,-1052.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138503856 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>140030138503856</title>\n",
       "<polygon fill=\"none\" points=\"571.5,-913.5 571.5,-959.5 1002.5,-959.5 1002.5,-913.5 571.5,-913.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704.5\" y=\"-932.8\">batch_normalization_10: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-913.5 837.5,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"865\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-936.5 892.5,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"865\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"892.5,-913.5 892.5,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-944.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"892.5,-936.5 1002.5,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-921.3\">(None, 35, 1024)</text>\n",
       "</g>\n",
       "<!-- 140030139464896&#45;&gt;140030138503856 -->\n",
       "<g class=\"edge\" id=\"edge33\">\n",
       "<title>140030139464896-&gt;140030138503856</title>\n",
       "<path d=\"M787,-996.3799C787,-988.1745 787,-978.7679 787,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-969.784 787,-959.784 783.5001,-969.784 790.5001,-969.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030138591104 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>140030138591104</title>\n",
       "<polygon fill=\"none\" points=\"639.5,-830.5 639.5,-876.5 934.5,-876.5 934.5,-830.5 639.5,-830.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704.5\" y=\"-849.8\">conv1d_11: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"769.5,-830.5 769.5,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"769.5,-853.5 824.5,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"824.5,-830.5 824.5,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-861.3\">(None, 35, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"824.5,-853.5 934.5,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-838.3\">(None, 18, 256)</text>\n",
       "</g>\n",
       "<!-- 140030138503856&#45;&gt;140030138591104 -->\n",
       "<g class=\"edge\" id=\"edge34\">\n",
       "<title>140030138503856-&gt;140030138591104</title>\n",
       "<path d=\"M787,-913.3799C787,-905.1745 787,-895.7679 787,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-886.784 787,-876.784 783.5001,-886.784 790.5001,-886.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030140896480 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>140030140896480</title>\n",
       "<polygon fill=\"none\" points=\"642.5,-747.5 642.5,-793.5 931.5,-793.5 931.5,-747.5 642.5,-747.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"707.5\" y=\"-766.8\">conv1d_12: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"772.5,-747.5 772.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"800\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"772.5,-770.5 827.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"800\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"827.5,-747.5 827.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-778.3\">(None, 18, 256)</text>\n",
       "<polyline fill=\"none\" points=\"827.5,-770.5 931.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-755.3\">(None, 9, 256)</text>\n",
       "</g>\n",
       "<!-- 140030138591104&#45;&gt;140030140896480 -->\n",
       "<g class=\"edge\" id=\"edge35\">\n",
       "<title>140030138591104-&gt;140030140896480</title>\n",
       "<path d=\"M787,-830.3799C787,-822.1745 787,-812.7679 787,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-803.784 787,-793.784 783.5001,-803.784 790.5001,-803.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139587104 -->\n",
       "<g class=\"node\" id=\"node28\">\n",
       "<title>140030139587104</title>\n",
       "<polygon fill=\"none\" points=\"646,-664.5 646,-710.5 928,-710.5 928,-664.5 646,-664.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"711\" y=\"-683.8\">conv1d_13: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"776,-664.5 776,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776,-687.5 831,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"831,-664.5 831,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-695.3\">(None, 9, 256)</text>\n",
       "<polyline fill=\"none\" points=\"831,-687.5 928,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-672.3\">(None, 5, 256)</text>\n",
       "</g>\n",
       "<!-- 140030140896480&#45;&gt;140030139587104 -->\n",
       "<g class=\"edge\" id=\"edge36\">\n",
       "<title>140030140896480-&gt;140030139587104</title>\n",
       "<path d=\"M787,-747.3799C787,-739.1745 787,-729.7679 787,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-720.784 787,-710.784 783.5001,-720.784 790.5001,-720.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139660440 -->\n",
       "<g class=\"node\" id=\"node29\">\n",
       "<title>140030139660440</title>\n",
       "<polygon fill=\"none\" points=\"646,-581.5 646,-627.5 928,-627.5 928,-581.5 646,-581.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"711\" y=\"-600.8\">conv1d_14: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"776,-581.5 776,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776,-604.5 831,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"831,-581.5 831,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-612.3\">(None, 5, 256)</text>\n",
       "<polyline fill=\"none\" points=\"831,-604.5 928,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879.5\" y=\"-589.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140030139587104&#45;&gt;140030139660440 -->\n",
       "<g class=\"edge\" id=\"edge37\">\n",
       "<title>140030139587104-&gt;140030139660440</title>\n",
       "<path d=\"M787,-664.3799C787,-656.1745 787,-646.7679 787,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-637.784 787,-627.784 783.5001,-637.784 790.5001,-637.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139652976 -->\n",
       "<g class=\"node\" id=\"node30\">\n",
       "<title>140030139652976</title>\n",
       "<polygon fill=\"none\" points=\"645.5,-498.5 645.5,-544.5 928.5,-544.5 928.5,-498.5 645.5,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"711\" y=\"-517.8\">dropout_11: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"776.5,-498.5 776.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776.5,-521.5 831.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"831.5,-498.5 831.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-529.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"831.5,-521.5 928.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-506.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140030139660440&#45;&gt;140030139652976 -->\n",
       "<g class=\"edge\" id=\"edge38\">\n",
       "<title>140030139660440-&gt;140030139652976</title>\n",
       "<path d=\"M787,-581.3799C787,-573.1745 787,-563.7679 787,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-554.784 787,-544.784 783.5001,-554.784 790.5001,-554.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139587328 -->\n",
       "<g class=\"node\" id=\"node31\">\n",
       "<title>140030139587328</title>\n",
       "<polygon fill=\"none\" points=\"578,-415.5 578,-461.5 996,-461.5 996,-415.5 578,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"711\" y=\"-434.8\">batch_normalization_11: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"844,-415.5 844,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"844,-438.5 899,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"871.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"899,-415.5 899,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-446.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"899,-438.5 996,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-423.3\">(None, 3, 256)</text>\n",
       "</g>\n",
       "<!-- 140030139652976&#45;&gt;140030139587328 -->\n",
       "<g class=\"edge\" id=\"edge39\">\n",
       "<title>140030139652976-&gt;140030139587328</title>\n",
       "<path d=\"M787,-498.3799C787,-490.1745 787,-480.7679 787,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-471.784 787,-461.784 783.5001,-471.784 790.5001,-471.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139643440 -->\n",
       "<g class=\"node\" id=\"node32\">\n",
       "<title>140030139643440</title>\n",
       "<polygon fill=\"none\" points=\"549.5,-332.5 549.5,-378.5 1024.5,-378.5 1024.5,-332.5 549.5,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"711\" y=\"-351.8\">global_average_pooling1d_2: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"872.5,-332.5 872.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"900\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"872.5,-355.5 927.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"900\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"927.5,-332.5 927.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"976\" y=\"-363.3\">(None, 3, 256)</text>\n",
       "<polyline fill=\"none\" points=\"927.5,-355.5 1024.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"976\" y=\"-340.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140030139587328&#45;&gt;140030139643440 -->\n",
       "<g class=\"edge\" id=\"edge40\">\n",
       "<title>140030139587328-&gt;140030139643440</title>\n",
       "<path d=\"M787,-415.3799C787,-407.1745 787,-397.7679 787,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-388.784 787,-378.784 783.5001,-388.784 790.5001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030139202472 -->\n",
       "<g class=\"node\" id=\"node33\">\n",
       "<title>140030139202472</title>\n",
       "<polygon fill=\"none\" points=\"667,-249.5 667,-295.5 907,-295.5 907,-249.5 667,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-268.8\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"769,-249.5 769,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"796.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"769,-272.5 824,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"796.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"824,-249.5 824,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"865.5\" y=\"-280.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"824,-272.5 907,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"865.5\" y=\"-257.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140030139643440&#45;&gt;140030139202472 -->\n",
       "<g class=\"edge\" id=\"edge41\">\n",
       "<title>140030139643440-&gt;140030139202472</title>\n",
       "<path d=\"M787,-332.3799C787,-324.1745 787,-314.7679 787,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-305.784 787,-295.784 783.5001,-305.784 790.5001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030141291488 -->\n",
       "<g class=\"node\" id=\"node34\">\n",
       "<title>140030141291488</title>\n",
       "<polygon fill=\"none\" points=\"652.5,-166.5 652.5,-212.5 921.5,-212.5 921.5,-166.5 652.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-185.8\">dropout_12: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-166.5 783.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-189.5 838.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-166.5 838.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-197.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-189.5 921.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"880\" y=\"-174.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140030139202472&#45;&gt;140030141291488 -->\n",
       "<g class=\"edge\" id=\"edge42\">\n",
       "<title>140030139202472-&gt;140030141291488</title>\n",
       "<path d=\"M787,-249.3799C787,-241.1745 787,-231.7679 787,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-222.784 787,-212.784 783.5001,-222.784 790.5001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030141290984 -->\n",
       "<g class=\"node\" id=\"node35\">\n",
       "<title>140030141290984</title>\n",
       "<polygon fill=\"none\" points=\"585,-83.5 585,-129.5 989,-129.5 989,-83.5 585,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-102.8\">batch_normalization_12: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"851,-83.5 851,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"851,-106.5 906,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"906,-83.5 906,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-114.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"906,-106.5 989,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"947.5\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 140030141291488&#45;&gt;140030141290984 -->\n",
       "<g class=\"edge\" id=\"edge43\">\n",
       "<title>140030141291488-&gt;140030141290984</title>\n",
       "<path d=\"M787,-166.3799C787,-158.1745 787,-148.7679 787,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-139.784 787,-129.784 783.5001,-139.784 790.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140030141169224 -->\n",
       "<g class=\"node\" id=\"node36\">\n",
       "<title>140030141169224</title>\n",
       "<polygon fill=\"none\" points=\"625.5,-.5 625.5,-46.5 948.5,-46.5 948.5,-.5 625.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-19.8\">output_headline_vector: Dense</text>\n",
       "<polyline fill=\"none\" points=\"810.5,-.5 810.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"810.5,-23.5 865.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"838\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-.5 865.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"907\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-23.5 948.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"907\" y=\"-8.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140030141290984&#45;&gt;140030141169224 -->\n",
       "<g class=\"edge\" id=\"edge44\">\n",
       "<title>140030141290984-&gt;140030141169224</title>\n",
       "<path d=\"M787,-83.3799C787,-75.1745 787,-65.7679 787,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"790.5001,-56.784 787,-46.784 783.5001,-56.784 790.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp_sentence_vectors = Input(shape=(max_sentences, 300), name='sentence_vectors')\n",
    "    inp_headline_vector = Input(shape=(300,), name='input_headline_vector')\n",
    "    conv1 = Conv1D(filters=16,kernel_size=3,strides=1,activation='relu', padding='same')(inp_sentence_vectors)\n",
    "    conv1 = Dropout(0.5)(conv1)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=3,strides=1,activation='relu', padding='same')(conv1)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    sent_sa_feat_1, sent_beta_1, sent_gamma_1 = SelfAttention(int(conv2.shape[-1]), name = 'sa1')(conv2)\n",
    "    sent_sa_feat_2, sent_beta_2, sent_gamma_2 = SelfAttention(int(conv2.shape[-1]), name = 'sa2')(conv2)\n",
    "    sent_sa_feat_3, sent_beta_3, sent_gamma_3 = SelfAttention(int(conv2.shape[-1]), name = 'sa3')(conv2)\n",
    "    sent_sa_feat_4, sent_beta_4, sent_gamma_4 = SelfAttention(int(conv2.shape[-1]), name = 'sa4')(conv2)\n",
    "    concat1 = Concatenate()([sent_sa_feat_1,sent_sa_feat_2,sent_sa_feat_3,sent_sa_feat_4])\n",
    "    conv3 = Conv1D(filters=256,kernel_size=3, strides=1, activation='relu', padding='same')(concat1)\n",
    "    conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    headline = Dense(256, activation='relu')(inp_headline_vector)\n",
    "    headline = Lambda(lambda x:K.expand_dims(x, axis=1))(headline)\n",
    "    headline = BatchNormalization()(headline)\n",
    "    sent_hd_sa_feat_1, sent_hd_beta_1, sent_hd_gamma_1 = CrossAttention(int(conv3.shape[-1]), name = 'ca1')([headline,conv3])\n",
    "    sent_hd_sa_feat_2, sent_hd_beta_2, sent_hd_gamma_2 = CrossAttention(int(conv3.shape[-1]), name = 'ca2')([headline,conv3])\n",
    "    sent_hd_sa_feat_3, sent_hd_beta_3, sent_hd_gamma_3 = CrossAttention(int(conv3.shape[-1]), name = 'ca3')([headline,conv3])\n",
    "    sent_hd_sa_feat_4, sent_hd_beta_4, sent_hd_gamma_4 = CrossAttention(int(conv3.shape[-1]), name = 'ca4')([headline,conv3])  \n",
    "    concat3 = Concatenate()([sent_hd_sa_feat_1,sent_hd_sa_feat_2,sent_hd_sa_feat_3,sent_hd_sa_feat_4])\n",
    "    concat3 = Dropout(0.5)(concat3)\n",
    "    concat3 = BatchNormalization()(concat3)\n",
    "    conv5 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(concat3)\n",
    "    conv6 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "    conv7 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv6)\n",
    "    conv8 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv7)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    gap = GlobalAveragePooling1D()(conv8)\n",
    "#     repeat = RepeatVector(50)(gap)\n",
    "#     lstm = LSTM(256,return_sequences=True)(repeat)\n",
    "    dense1 = Dense(512,activation='relu')(gap)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    gen_hd_vector = Dense(300,activation='linear', name='output_headline_vector')(dense1)\n",
    "    model = Model([inp_sentence_vectors,inp_headline_vector],gen_hd_vector)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001,beta_1=0.0,beta_2=0.99),loss='mse')\n",
    "model.summary()\n",
    "# print('model params:',model.count_params())\n",
    "SVG(model_to_dot(model,show_layer_names=True,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now()\n",
    "mc = ModelCheckpoint('weights/dnf300_sa_sent_hd_vector.hdf5',save_best_only=True,save_weights_only=True)\n",
    "tb = TensorBoard(batch_size=32,log_dir='logs/dnf300_sa_sent_hd_vector/{0}'.format(dt.timestamp()),write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.2944 - val_loss: 0.2150\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 1.2715 - val_loss: 0.1970\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 1.2647 - val_loss: 0.1948\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 2s 431ms/step - loss: 1.2787 - val_loss: 0.1833\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 1.2404 - val_loss: 0.1787\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 1.2394 - val_loss: 0.1745\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 2s 515ms/step - loss: 1.2343 - val_loss: 0.1653\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 2s 568ms/step - loss: 1.2214 - val_loss: 0.1612\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 1.2206 - val_loss: 0.1590\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 1.2088 - val_loss: 0.1539\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 1.1876 - val_loss: 0.1479\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 2s 570ms/step - loss: 1.1997 - val_loss: 0.1452\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 1.1857 - val_loss: 0.1385\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 1.1794 - val_loss: 0.1338\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 1.1695 - val_loss: 0.1287\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 1.1504 - val_loss: 0.1259\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 1.1493 - val_loss: 0.1185\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 1.1708 - val_loss: 0.1175\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 1.1340 - val_loss: 0.1140\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 1.1351 - val_loss: 0.1138\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 1.1475 - val_loss: 0.1152\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 1.1211 - val_loss: 0.1109\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 1.1227 - val_loss: 0.1078\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 1.1121 - val_loss: 0.1031\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 1.0953 - val_loss: 0.1043\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 1.1027 - val_loss: 0.0967\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 2s 589ms/step - loss: 1.0817 - val_loss: 0.0958\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 1.0850 - val_loss: 0.0962\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 1.0672 - val_loss: 0.0920\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 1.0666 - val_loss: 0.0925\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 1.0677 - val_loss: 0.0915\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 1.0566 - val_loss: 0.0893\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 1.0564 - val_loss: 0.0869\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 1.0342 - val_loss: 0.0864\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 1.0506 - val_loss: 0.0852\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 1.0227 - val_loss: 0.0829\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 1.0256 - val_loss: 0.0830\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 1.0265 - val_loss: 0.0811\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 1.0147 - val_loss: 0.0812\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 1.0073 - val_loss: 0.0808\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.9942 - val_loss: 0.0775\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 1.0131 - val_loss: 0.0774\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 2s 500ms/step - loss: 0.9696 - val_loss: 0.0774\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.9838 - val_loss: 0.0729\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.9786 - val_loss: 0.0733\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.9628 - val_loss: 0.0765\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.9525 - val_loss: 0.0750\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.9575 - val_loss: 0.0755\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.9455 - val_loss: 0.0762\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.9304 - val_loss: 0.0743\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.9355 - val_loss: 0.0738\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 2s 562ms/step - loss: 0.9380 - val_loss: 0.0701\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 2s 564ms/step - loss: 0.9406 - val_loss: 0.0724\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.9301 - val_loss: 0.0728\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.9323 - val_loss: 0.0722\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.9222 - val_loss: 0.0752\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 0.8820 - val_loss: 0.0701\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.9075 - val_loss: 0.0692\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.8943 - val_loss: 0.0707\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 3s 860ms/step - loss: 0.8858 - val_loss: 0.0729\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.8871 - val_loss: 0.0681\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.8846 - val_loss: 0.0685\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.8797 - val_loss: 0.0696\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.8590 - val_loss: 0.0666\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.8622 - val_loss: 0.0667\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 3s 860ms/step - loss: 0.8591 - val_loss: 0.0676\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.8466 - val_loss: 0.0635\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 3s 793ms/step - loss: 0.8331 - val_loss: 0.0647\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.8505 - val_loss: 0.0672\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.8397 - val_loss: 0.0658\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.8167 - val_loss: 0.0633\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.8248 - val_loss: 0.0649\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.7997 - val_loss: 0.0643\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.8011 - val_loss: 0.0622\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.7941 - val_loss: 0.0630\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.7864 - val_loss: 0.0606\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.7856 - val_loss: 0.0625\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.7937 - val_loss: 0.0594\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.7777 - val_loss: 0.0615\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.7749 - val_loss: 0.0599\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.7628 - val_loss: 0.0609\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.7630 - val_loss: 0.0596\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 3s 822ms/step - loss: 0.7737 - val_loss: 0.0600\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.7520 - val_loss: 0.0564\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 2s 543ms/step - loss: 0.7565 - val_loss: 0.0554\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.7564 - val_loss: 0.0568\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.7470 - val_loss: 0.0538\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.7451 - val_loss: 0.0560\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.7365 - val_loss: 0.0543\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 3s 821ms/step - loss: 0.7238 - val_loss: 0.0523\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.7164 - val_loss: 0.0503\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 0.7118 - val_loss: 0.0511\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.7119 - val_loss: 0.0522\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.7087 - val_loss: 0.0515\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.7000 - val_loss: 0.0491\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.7082 - val_loss: 0.0486\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.7024 - val_loss: 0.0489\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 2s 579ms/step - loss: 0.6978 - val_loss: 0.0479\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.6967 - val_loss: 0.0479\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.6777 - val_loss: 0.0480\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.6773 - val_loss: 0.0471\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 2s 549ms/step - loss: 0.6652 - val_loss: 0.0451\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 2s 530ms/step - loss: 0.6643 - val_loss: 0.0480\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.6756 - val_loss: 0.0437\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.6652 - val_loss: 0.0424\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.6445 - val_loss: 0.0405\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.6591 - val_loss: 0.0431\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.6371 - val_loss: 0.0404\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.6400 - val_loss: 0.0397\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.6431 - val_loss: 0.0408\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.6344 - val_loss: 0.0398\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.6305 - val_loss: 0.0391\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.6378 - val_loss: 0.0395\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.6319 - val_loss: 0.0380\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.6163 - val_loss: 0.0396\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.6022 - val_loss: 0.0375\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 0.6103 - val_loss: 0.0397\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.6215 - val_loss: 0.0364\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.6178 - val_loss: 0.0366\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.6105 - val_loss: 0.0332\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.5926 - val_loss: 0.0337\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 3s 777ms/step - loss: 0.5874 - val_loss: 0.0324\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.5826 - val_loss: 0.0319\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.5790 - val_loss: 0.0323\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.5836 - val_loss: 0.0316\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.5817 - val_loss: 0.0321\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.5733 - val_loss: 0.0324\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.5825 - val_loss: 0.0308\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.5822 - val_loss: 0.0299\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 3s 795ms/step - loss: 0.5680 - val_loss: 0.0283\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.5642 - val_loss: 0.0278\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.5558 - val_loss: 0.0282\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.5463 - val_loss: 0.0283\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.5669 - val_loss: 0.0291\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.5600 - val_loss: 0.0289\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.5522 - val_loss: 0.0291\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.5361 - val_loss: 0.0273\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.5435 - val_loss: 0.0263\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.5412 - val_loss: 0.0279\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.5414 - val_loss: 0.0262\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.5326 - val_loss: 0.0254\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.5274 - val_loss: 0.0269\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.5274 - val_loss: 0.0246\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.5204 - val_loss: 0.0246\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.5229 - val_loss: 0.0238\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.5107 - val_loss: 0.0214\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.5048 - val_loss: 0.0264\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 0.5017 - val_loss: 0.0215\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.5096 - val_loss: 0.0221\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.5133 - val_loss: 0.0215\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.5072 - val_loss: 0.0217\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.4954 - val_loss: 0.0207\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.5037 - val_loss: 0.0230\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.4953 - val_loss: 0.0231\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.5002 - val_loss: 0.0225\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.4875 - val_loss: 0.0208\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4974 - val_loss: 0.0224\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 3s 781ms/step - loss: 0.4911 - val_loss: 0.0201\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.4747 - val_loss: 0.0209\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.4962 - val_loss: 0.0212\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.4847 - val_loss: 0.0213\n",
      "Epoch 162/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 605ms/step - loss: 0.4685 - val_loss: 0.0224\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.4707 - val_loss: 0.0191\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.4760 - val_loss: 0.0218\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 2s 554ms/step - loss: 0.4603 - val_loss: 0.0197\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.4738 - val_loss: 0.0201\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.4593 - val_loss: 0.0194\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.4659 - val_loss: 0.0194\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.4633 - val_loss: 0.0176\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.4714 - val_loss: 0.0184\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.4541 - val_loss: 0.0177\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.4490 - val_loss: 0.0177\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.4510 - val_loss: 0.0172\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.4381 - val_loss: 0.0178\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.4528 - val_loss: 0.0181\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.4384 - val_loss: 0.0176\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.4444 - val_loss: 0.0152\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.4338 - val_loss: 0.0162\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.4313 - val_loss: 0.0181\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.4234 - val_loss: 0.0151\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.4245 - val_loss: 0.0175\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.4330 - val_loss: 0.0166\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.4241 - val_loss: 0.0173\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.4205 - val_loss: 0.0147\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.4254 - val_loss: 0.0177\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.4208 - val_loss: 0.0174\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 2s 551ms/step - loss: 0.4348 - val_loss: 0.0161\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.4224 - val_loss: 0.0151\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.4281 - val_loss: 0.0165\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.4273 - val_loss: 0.0165\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.4084 - val_loss: 0.0161\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.4175 - val_loss: 0.0166\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 2s 555ms/step - loss: 0.4120 - val_loss: 0.0168\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.4148 - val_loss: 0.0149\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.4074 - val_loss: 0.0180\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.4123 - val_loss: 0.0170\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 2s 558ms/step - loss: 0.3944 - val_loss: 0.0151\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.4100 - val_loss: 0.0161\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.4012 - val_loss: 0.0144\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 3s 625ms/step - loss: 0.4022 - val_loss: 0.0159\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.4024 - val_loss: 0.0158\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.3960 - val_loss: 0.0144\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.3946 - val_loss: 0.0154\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.3923 - val_loss: 0.0137\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.3974 - val_loss: 0.0139\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.3788 - val_loss: 0.0145\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 2s 591ms/step - loss: 0.3767 - val_loss: 0.0147\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.3774 - val_loss: 0.0152\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.3827 - val_loss: 0.0140\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.3758 - val_loss: 0.0145\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.3771 - val_loss: 0.0138\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.3716 - val_loss: 0.0148\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.3755 - val_loss: 0.0154\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.3782 - val_loss: 0.0149\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.3800 - val_loss: 0.0136\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.3721 - val_loss: 0.0118\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.3699 - val_loss: 0.0121\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.3720 - val_loss: 0.0142\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.3651 - val_loss: 0.0132\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.3651 - val_loss: 0.0143\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.3619 - val_loss: 0.0149\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.3789 - val_loss: 0.0146\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.3650 - val_loss: 0.0156\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.3508 - val_loss: 0.0145\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.3632 - val_loss: 0.0130\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.3549 - val_loss: 0.0133\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.3573 - val_loss: 0.0138\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.3607 - val_loss: 0.0142\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.3430 - val_loss: 0.0141\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.3407 - val_loss: 0.0134\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.3447 - val_loss: 0.0114\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.3352 - val_loss: 0.0134\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.3434 - val_loss: 0.0130\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.3393 - val_loss: 0.0125\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.3370 - val_loss: 0.0144\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.3468 - val_loss: 0.0157\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.3319 - val_loss: 0.0156\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.3284 - val_loss: 0.0133\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.3370 - val_loss: 0.0143\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.3373 - val_loss: 0.0147\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.3167 - val_loss: 0.0118\n",
      "Epoch 242/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.3274 - val_loss: 0.0144\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.3234 - val_loss: 0.0115\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.3232 - val_loss: 0.0121\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.3203 - val_loss: 0.0126\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.3425 - val_loss: 0.0145\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.3179 - val_loss: 0.0131\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.3183 - val_loss: 0.0136\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.3214 - val_loss: 0.0118\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.3197 - val_loss: 0.0126\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.3263 - val_loss: 0.0118\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.3206 - val_loss: 0.0120\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.3125 - val_loss: 0.0122\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.3136 - val_loss: 0.0118\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.3010 - val_loss: 0.0113\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 2s 548ms/step - loss: 0.3034 - val_loss: 0.0131\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.3131 - val_loss: 0.0124\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.3085 - val_loss: 0.0159\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.3079 - val_loss: 0.0120\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.3149 - val_loss: 0.0134\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.2988 - val_loss: 0.0128\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.3006 - val_loss: 0.0103\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.3012 - val_loss: 0.0130\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.2940 - val_loss: 0.0114\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.3032 - val_loss: 0.0129\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 3s 778ms/step - loss: 0.3036 - val_loss: 0.0132\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.2831 - val_loss: 0.0125\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.3014 - val_loss: 0.0126\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.2849 - val_loss: 0.0120\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.2867 - val_loss: 0.0129\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.2887 - val_loss: 0.0114\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.2776 - val_loss: 0.0124\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.2765 - val_loss: 0.0134\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.2873 - val_loss: 0.0117\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.2927 - val_loss: 0.0103\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 2s 558ms/step - loss: 0.2872 - val_loss: 0.0107\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 2s 554ms/step - loss: 0.2839 - val_loss: 0.0140\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.2968 - val_loss: 0.0120\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 0.2922 - val_loss: 0.0125\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.2825 - val_loss: 0.0124\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.2701 - val_loss: 0.0127\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 2s 620ms/step - loss: 0.2766 - val_loss: 0.0117\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.2746 - val_loss: 0.0131\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.2751 - val_loss: 0.0134\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.2833 - val_loss: 0.0115\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.2713 - val_loss: 0.0114\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.2776 - val_loss: 0.0129\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.2601 - val_loss: 0.0135\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.2671 - val_loss: 0.0142\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.2584 - val_loss: 0.0125\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 2s 563ms/step - loss: 0.2666 - val_loss: 0.0118\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.2572 - val_loss: 0.0174\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.2528 - val_loss: 0.0148\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.2560 - val_loss: 0.0125\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.2584 - val_loss: 0.0152\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.2620 - val_loss: 0.0144\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.2778 - val_loss: 0.0159\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.2532 - val_loss: 0.0140\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 2s 562ms/step - loss: 0.2449 - val_loss: 0.0126\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.2629 - val_loss: 0.0108\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.2536 - val_loss: 0.0133\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.2579 - val_loss: 0.0118\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.2482 - val_loss: 0.0114\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.2470 - val_loss: 0.0100\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.2559 - val_loss: 0.0120\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.2456 - val_loss: 0.0116\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.2545 - val_loss: 0.0123\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.2332 - val_loss: 0.0112\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.2429 - val_loss: 0.0113\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.2390 - val_loss: 0.0120\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.2464 - val_loss: 0.0113\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.2474 - val_loss: 0.0141\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.2360 - val_loss: 0.0125\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.2459 - val_loss: 0.0119\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.2260 - val_loss: 0.0125\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 2s 596ms/step - loss: 0.2316 - val_loss: 0.0134\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.2545 - val_loss: 0.0130\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.2278 - val_loss: 0.0133\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.2406 - val_loss: 0.0136\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.2360 - val_loss: 0.0120\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.2360 - val_loss: 0.0138\n",
      "Epoch 322/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 661ms/step - loss: 0.2306 - val_loss: 0.0105\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.2402 - val_loss: 0.0114\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.2267 - val_loss: 0.0115\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.2416 - val_loss: 0.0115\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 3s 799ms/step - loss: 0.2170 - val_loss: 0.0115\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.2289 - val_loss: 0.0127\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.2225 - val_loss: 0.0126\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.2226 - val_loss: 0.0124\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.2070 - val_loss: 0.0106\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.2233 - val_loss: 0.0143\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.2013 - val_loss: 0.0112\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.2213 - val_loss: 0.0114\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.2192 - val_loss: 0.0116\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.2102 - val_loss: 0.0111\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.2047 - val_loss: 0.0134\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.1878 - val_loss: 0.0123\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.2125 - val_loss: 0.0109\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.2132 - val_loss: 0.0121\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.2044 - val_loss: 0.0133\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.2016 - val_loss: 0.0124\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.1906 - val_loss: 0.0105\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.2040 - val_loss: 0.0114\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 2s 585ms/step - loss: 0.1991 - val_loss: 0.0129\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.1982 - val_loss: 0.0112\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.2025 - val_loss: 0.0103\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.2115 - val_loss: 0.0115\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 2s 605ms/step - loss: 0.2008 - val_loss: 0.0099\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.1907 - val_loss: 0.0116\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 3s 802ms/step - loss: 0.1946 - val_loss: 0.0128\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.2098 - val_loss: 0.0113\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.2003 - val_loss: 0.0110\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.1672 - val_loss: 0.0102\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.1945 - val_loss: 0.0132\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.1742 - val_loss: 0.0113\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.1790 - val_loss: 0.0113\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.1940 - val_loss: 0.0120\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.1667 - val_loss: 0.0109\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.1856 - val_loss: 0.0108\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.1914 - val_loss: 0.0131\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.1684 - val_loss: 0.0105\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.1657 - val_loss: 0.0104\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.1395 - val_loss: 0.0102\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 2s 620ms/step - loss: 0.1495 - val_loss: 0.0123\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 0.1392 - val_loss: 0.0114\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.1562 - val_loss: 0.0122\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.1251 - val_loss: 0.0126\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.1469 - val_loss: 0.0128\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.1010 - val_loss: 0.0119\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0799 - val_loss: 0.0141\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 2s 596ms/step - loss: 0.0750 - val_loss: 0.0115\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0509 - val_loss: 0.0134\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0664 - val_loss: 0.0104\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0613 - val_loss: 0.0115\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0414 - val_loss: 0.0118\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0372 - val_loss: 0.0122\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0516 - val_loss: 0.0116\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0497 - val_loss: 0.0136\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0326 - val_loss: 0.0120\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0335 - val_loss: 0.0109\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0326 - val_loss: 0.0111\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0255 - val_loss: 0.0124\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0252 - val_loss: 0.0113\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0226 - val_loss: 0.0112\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 2s 557ms/step - loss: 0.0264 - val_loss: 0.0125\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 2s 591ms/step - loss: 0.0216 - val_loss: 0.0118\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0327 - val_loss: 0.0106\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0224 - val_loss: 0.0119\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0197 - val_loss: 0.0121\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0204 - val_loss: 0.0120\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0246 - val_loss: 0.0121\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0234 - val_loss: 0.0110\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0175 - val_loss: 0.0115\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0177 - val_loss: 0.0129\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0181 - val_loss: 0.0125\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0183 - val_loss: 0.0133\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 3s 808ms/step - loss: 0.0167 - val_loss: 0.0121\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0146 - val_loss: 0.0110\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0146 - val_loss: 0.0119\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0160 - val_loss: 0.0129\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0153 - val_loss: 0.0115\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0153 - val_loss: 0.0111\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0149 - val_loss: 0.0120\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0162 - val_loss: 0.0100\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0172 - val_loss: 0.0110\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0147 - val_loss: 0.0118\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 0.0139 - val_loss: 0.0105\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0129 - val_loss: 0.0123\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 2s 601ms/step - loss: 0.0143 - val_loss: 0.0113\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0137 - val_loss: 0.0108\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0140 - val_loss: 0.0116\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0130 - val_loss: 0.0094\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0137 - val_loss: 0.0117\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0121 - val_loss: 0.0092\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 2s 572ms/step - loss: 0.0118 - val_loss: 0.0106\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0132 - val_loss: 0.0126\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0130 - val_loss: 0.0108\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0118 - val_loss: 0.0111\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0117 - val_loss: 0.0160\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0119 - val_loss: 0.0102\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0121 - val_loss: 0.0118\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0116 - val_loss: 0.0096\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0139 - val_loss: 0.0115\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0147 - val_loss: 0.0120\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0101 - val_loss: 0.0117\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 2s 605ms/step - loss: 0.0117 - val_loss: 0.0111\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0115 - val_loss: 0.0104\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 2s 594ms/step - loss: 0.0127 - val_loss: 0.0102\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0097 - val_loss: 0.0124\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0103 - val_loss: 0.0086\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 2s 531ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 2s 531ms/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0132 - val_loss: 0.0115\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0205 - val_loss: 0.0094\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0111 - val_loss: 0.0096\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 2s 583ms/step - loss: 0.0105 - val_loss: 0.0134\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 2s 620ms/step - loss: 0.0098 - val_loss: 0.0114\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0119 - val_loss: 0.0111\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 3s 793ms/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0101 - val_loss: 0.0130\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 482/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0109 - val_loss: 0.0134\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0118 - val_loss: 0.0109\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0112 - val_loss: 0.0134\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0117 - val_loss: 0.0113\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0112 - val_loss: 0.0107\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 2s 545ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.0103 - val_loss: 0.0127\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0110 - val_loss: 0.0133\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 2s 568ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0103 - val_loss: 0.0138\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0105 - val_loss: 0.0119\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 2s 560ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0116 - val_loss: 0.0114\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 2s 548ms/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0110 - val_loss: 0.0094\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0134 - val_loss: 0.0107\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0125 - val_loss: 0.0105\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 3s 757ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0100 - val_loss: 0.0123\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 2s 622ms/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0107 - val_loss: 0.0131\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0096 - val_loss: 0.0116\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0107 - val_loss: 0.1040\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0112 - val_loss: 0.0097\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0120 - val_loss: 0.0116\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0109 - val_loss: 0.0091\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.0094 - val_loss: 0.0104\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0107 - val_loss: 0.0131\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 2s 547ms/step - loss: 0.0110 - val_loss: 0.0093\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0126 - val_loss: 0.0123\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0111 - val_loss: 0.0093\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 2s 544ms/step - loss: 0.0098 - val_loss: 0.0099\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0127 - val_loss: 0.0114\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0117 - val_loss: 0.0101\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0106 - val_loss: 0.0090\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 2s 561ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 2s 560ms/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0111 - val_loss: 0.0108\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0106 - val_loss: 0.0131\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 3s 810ms/step - loss: 0.0144 - val_loss: 0.0119\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 3s 850ms/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.0102 - val_loss: 0.0125\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0100 - val_loss: 0.0099\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0116 - val_loss: 0.0099\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 2s 591ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0113 - val_loss: 0.0104\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0100 - val_loss: 0.0116\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 642/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 3s 802ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0111 - val_loss: 0.0104\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0106 - val_loss: 0.0092\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0107 - val_loss: 0.0137\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 2s 561ms/step - loss: 0.0105 - val_loss: 0.0137\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0113 - val_loss: 0.0115\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 2s 590ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0098 - val_loss: 0.0112\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0098 - val_loss: 0.0115\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0101 - val_loss: 0.0114\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0103 - val_loss: 0.0129\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.0118 - val_loss: 0.0101\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0117 - val_loss: 0.0106\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0106 - val_loss: 0.0134\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0107 - val_loss: 0.0092\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 2s 555ms/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.0132 - val_loss: 0.0095\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 3s 796ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 3s 755ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0110 - val_loss: 0.0100\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0118 - val_loss: 0.0101\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0106 - val_loss: 0.0125\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 2s 560ms/step - loss: 0.0105 - val_loss: 0.0132\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 2s 580ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 3s 778ms/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 3s 789ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0105 - val_loss: 0.0096\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 3s 780ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0112 - val_loss: 0.0135\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 2s 585ms/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 2s 584ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0097 - val_loss: 0.0095\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 2s 585ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 802/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0107 - val_loss: 0.0105\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 2s 564ms/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 2s 577ms/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0118 - val_loss: 0.0116\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 4s 879ms/step - loss: 0.0108 - val_loss: 0.0138\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0096 - val_loss: 0.0109\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0116 - val_loss: 0.0091\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0107 - val_loss: 0.0100\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 3s 784ms/step - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.0102 - val_loss: 0.0086\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0096 - val_loss: 0.0124\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 3s 785ms/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0099 - val_loss: 0.0115\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.0106 - val_loss: 0.0117\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0113 - val_loss: 0.0103\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0095 - val_loss: 0.0114\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0100 - val_loss: 0.0115\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.0127 - val_loss: 0.0110\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0111 - val_loss: 0.0134\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0113 - val_loss: 0.0099\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0107 - val_loss: 0.0131\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0117 - val_loss: 0.0107\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0096 - val_loss: 0.0090\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0113 - val_loss: 0.0129\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 2s 564ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0107 - val_loss: 0.0118\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0100 - val_loss: 0.0107\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0104 - val_loss: 0.0142\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 2s 546ms/step - loss: 0.0101 - val_loss: 0.0118\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0103 - val_loss: 0.0108\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0110 - val_loss: 0.0096\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0108 - val_loss: 0.0124\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.0109 - val_loss: 0.0117\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.0105 - val_loss: 0.0091\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 2s 596ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0098 - val_loss: 0.0100\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 3s 819ms/step - loss: 0.0107 - val_loss: 0.0090\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0099 - val_loss: 0.0112\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0097 - val_loss: 0.0109\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0098 - val_loss: 0.0110\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0110 - val_loss: 0.0114\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 962/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 3s 775ms/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0110 - val_loss: 0.0094\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.0110 - val_loss: 0.0091\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0116 - val_loss: 0.0118\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.0106 - val_loss: 0.0087\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 3s 779ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0102 - val_loss: 0.0126\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 2s 621ms/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 2s 554ms/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0103 - val_loss: 0.0132\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0115 - val_loss: 0.0118\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 3s 789ms/step - loss: 0.0105 - val_loss: 0.0133\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0094 - val_loss: 0.0127\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0107 - val_loss: 0.0091\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 2s 538ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0102 - val_loss: 0.0118\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.0101 - val_loss: 0.0103\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 2s 605ms/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0100 - val_loss: 0.0114\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 2s 531ms/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0102 - val_loss: 0.0114\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0112 - val_loss: 0.0121\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0114 - val_loss: 0.0115\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0100 - val_loss: 0.0105\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1042/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0102 - val_loss: 0.0124\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0111 - val_loss: 0.0113\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 2s 589ms/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0093 - val_loss: 0.0130\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0114 - val_loss: 0.0118\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 2s 588ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 2s 594ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 2s 620ms/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0116 - val_loss: 0.0123\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 2s 620ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 3s 812ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 2s 546ms/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 0.0098 - val_loss: 0.0105\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 2s 530ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 2s 552ms/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 2s 589ms/step - loss: 0.0099 - val_loss: 0.0124\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 3s 783ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0120 - val_loss: 0.0107\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0099 - val_loss: 0.0124\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0122 - val_loss: 0.0119\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0098 - val_loss: 0.0111\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0113 - val_loss: 0.0100\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 2s 589ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 2s 581ms/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 2s 573ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 2s 537ms/step - loss: 0.0111 - val_loss: 0.0127\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0111 - val_loss: 0.0102\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0101 - val_loss: 0.0117\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0105 - val_loss: 0.0088\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 2s 550ms/step - loss: 0.0117 - val_loss: 0.0126\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0119 - val_loss: 0.0121\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0098 - val_loss: 0.0112\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0110 - val_loss: 0.0092\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0110 - val_loss: 0.0124\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0100 - val_loss: 0.0081\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0125 - val_loss: 0.0099\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0100 - val_loss: 0.0112\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 3s 781ms/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 3s 795ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.0101 - val_loss: 0.0092\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0119 - val_loss: 0.0112\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 3s 817ms/step - loss: 0.0102 - val_loss: 0.0085\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 3s 862ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 3s 862ms/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0110 - val_loss: 0.0094\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0103 - val_loss: 0.0104\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0104 - val_loss: 0.0130\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 2s 551ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0092 - val_loss: 0.0120\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 3s 830ms/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.0099 - val_loss: 0.0106\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 3s 783ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0119 - val_loss: 0.0105\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1200/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 779ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0100 - val_loss: 0.0112\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0103 - val_loss: 0.0098\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0112 - val_loss: 0.0127\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0105 - val_loss: 0.0125\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.0118 - val_loss: 0.0105\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0099 - val_loss: 0.0102\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0102 - val_loss: 0.0105\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0103 - val_loss: 0.0088\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 3s 786ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0096 - val_loss: 0.0115\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 3s 804ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 3s 814ms/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 2s 590ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 2s 575ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0104 - val_loss: 0.0085\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0104 - val_loss: 0.0111\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0110 - val_loss: 0.0088\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 3s 646ms/step - loss: 0.0112 - val_loss: 0.0101\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0101 - val_loss: 0.0125\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 3s 728ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 3s 802ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 3s 777ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0109 - val_loss: 0.0133\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 2s 584ms/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 2s 604ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 2s 590ms/step - loss: 0.0110 - val_loss: 0.0101\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 2s 613ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 2s 596ms/step - loss: 0.0123 - val_loss: 0.0092\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 2s 575ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0106 - val_loss: 0.0094\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0115 - val_loss: 0.0101\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 2s 602ms/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1310/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 2s 513ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 2s 605ms/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.0103 - val_loss: 0.0115\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 3s 631ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 3s 777ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0105 - val_loss: 0.0112\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 2s 596ms/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0108 - val_loss: 0.0127\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0107 - val_loss: 0.0088\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0101 - val_loss: 0.0123\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0101 - val_loss: 0.0111\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.0108 - val_loss: 0.0112\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0103 - val_loss: 0.0124\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 3s 780ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 3s 837ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 2s 557ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 1358/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0106 - val_loss: 0.0130\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 2s 594ms/step - loss: 0.0103 - val_loss: 0.0130\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0104 - val_loss: 0.0115\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0107 - val_loss: 0.0130\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 3s 793ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 2s 581ms/step - loss: 0.0095 - val_loss: 0.0096\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 3s 747ms/step - loss: 0.0098 - val_loss: 0.0088\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 3s 860ms/step - loss: 0.0095 - val_loss: 0.0097\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 3s 735ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0108 - val_loss: 0.0100\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0107 - val_loss: 0.0129\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 3s 703ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0114 - val_loss: 0.0105\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 3s 630ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0097 - val_loss: 0.0101\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.0116 - val_loss: 0.0110\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0095 - val_loss: 0.0103\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0115 - val_loss: 0.0114\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0120 - val_loss: 0.0100\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 2s 559ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0100 - val_loss: 0.0118\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0115 - val_loss: 0.0123\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0114 - val_loss: 0.0106\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0108 - val_loss: 0.0090\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0116 - val_loss: 0.0100\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0101 - val_loss: 0.0095\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 2s 558ms/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 2s 546ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0099 - val_loss: 0.0128\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0123 - val_loss: 0.0107\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 3s 746ms/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0112 - val_loss: 0.0126\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 2s 551ms/step - loss: 0.0104 - val_loss: 0.0133\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0106 - val_loss: 0.0127\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0111 - val_loss: 0.0109\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0116 - val_loss: 0.0112\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 3s 732ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0108 - val_loss: 0.0094\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 3s 629ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0104 - val_loss: 0.0138\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 2s 575ms/step - loss: 0.0110 - val_loss: 0.0108\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0102 - val_loss: 0.0088\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 2s 560ms/step - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 2s 603ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 2s 597ms/step - loss: 0.0115 - val_loss: 0.0132\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 2s 610ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 3s 757ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 3s 777ms/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0117 - val_loss: 0.0119\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0113 - val_loss: 0.0119\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0104 - val_loss: 0.0130\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 2s 598ms/step - loss: 0.0099 - val_loss: 0.0125\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0114 - val_loss: 0.0119\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1516/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 771ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 3s 793ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 3s 691ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0108 - val_loss: 0.0125\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 2s 582ms/step - loss: 0.0101 - val_loss: 0.0126\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0096 - val_loss: 0.0116\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0105 - val_loss: 0.0115\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0115 - val_loss: 0.0109\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.0107 - val_loss: 0.0111\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0098 - val_loss: 0.0118\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.0109 - val_loss: 0.0101\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 2s 543ms/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0105 - val_loss: 0.0097\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 3s 649ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 3s 642ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0105 - val_loss: 0.0130\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0100 - val_loss: 0.0123\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0100 - val_loss: 0.0124\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 3s 692ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 2s 570ms/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 3s 651ms/step - loss: 0.0109 - val_loss: 0.0128\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0101 - val_loss: 0.0104\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0103 - val_loss: 0.0133\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 3s 729ms/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 3s 853ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0111 - val_loss: 0.0115\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0098 - val_loss: 0.0116\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0098 - val_loss: 0.0110\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0095 - val_loss: 0.0117\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 2s 600ms/step - loss: 0.0104 - val_loss: 0.0101\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0116 - val_loss: 0.0102\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0108 - val_loss: 0.0113\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 2s 551ms/step - loss: 0.0109 - val_loss: 0.0118\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 2s 604ms/step - loss: 0.0105 - val_loss: 0.0130\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0107 - val_loss: 0.0136\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 0.0104 - val_loss: 0.0125\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 3s 656ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 3s 702ms/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0102 - val_loss: 0.0088\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.0109 - val_loss: 0.0102\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.0101 - val_loss: 0.0129\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 3s 715ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0104 - val_loss: 0.0104\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0110 - val_loss: 0.0111\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 3s 777ms/step - loss: 0.0095 - val_loss: 0.0113\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 3s 727ms/step - loss: 0.0106 - val_loss: 0.0100\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 0.0110 - val_loss: 0.0093\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0097 - val_loss: 0.0104\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0107 - val_loss: 0.0098\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0101 - val_loss: 0.0087\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.0110 - val_loss: 0.0097\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0112 - val_loss: 0.0104\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 3s 628ms/step - loss: 0.0113 - val_loss: 0.0121\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 3s 632ms/step - loss: 0.0104 - val_loss: 0.0127\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0099 - val_loss: 0.0105\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0111 - val_loss: 0.0114\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0107 - val_loss: 0.0107\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.0109 - val_loss: 0.0098\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 3s 633ms/step - loss: 0.0100 - val_loss: 0.0110\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 3s 694ms/step - loss: 0.0103 - val_loss: 0.0106\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0095 - val_loss: 0.0127\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 2s 618ms/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0099 - val_loss: 0.0092\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0105 - val_loss: 0.0087\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0117 - val_loss: 0.0091\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 2s 559ms/step - loss: 0.0108 - val_loss: 0.0114\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 3s 643ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 2s 590ms/step - loss: 0.0093 - val_loss: 0.0111\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0100 - val_loss: 0.0123\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 3s 737ms/step - loss: 0.0116 - val_loss: 0.0130\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 3s 731ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0096 - val_loss: 0.0131\n",
      "Epoch 1674/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 3s 719ms/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.0106 - val_loss: 0.0126\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 3s 693ms/step - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 3s 830ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 3s 701ms/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 3s 790ms/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0104 - val_loss: 0.0097\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0118 - val_loss: 0.0108\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 3s 717ms/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 2s 599ms/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.0108 - val_loss: 0.0116\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.0098 - val_loss: 0.0116\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 3s 699ms/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0115 - val_loss: 0.0093\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 0.0114 - val_loss: 0.0100\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 3s 655ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 3s 722ms/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 3s 639ms/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 3s 800ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.0113 - val_loss: 0.0097\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 2s 611ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 2s 617ms/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.0099 - val_loss: 0.0117\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 3s 625ms/step - loss: 0.0109 - val_loss: 0.0125\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 2s 616ms/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0115 - val_loss: 0.0105\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0110 - val_loss: 0.0102\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0112 - val_loss: 0.0110\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0104 - val_loss: 0.0132\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 3s 775ms/step - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 3s 739ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0107 - val_loss: 0.0087\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0113 - val_loss: 0.0126\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 3s 745ms/step - loss: 0.0102 - val_loss: 0.0115\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0104 - val_loss: 0.0118\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 3s 718ms/step - loss: 0.0109 - val_loss: 0.0095\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0108 - val_loss: 0.0095\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 3s 680ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 3s 782ms/step - loss: 0.0104 - val_loss: 0.0112\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 3s 757ms/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0110 - val_loss: 0.0083\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 3s 659ms/step - loss: 0.0114 - val_loss: 0.0117\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 3s 741ms/step - loss: 0.0103 - val_loss: 0.0110\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 3s 648ms/step - loss: 0.0105 - val_loss: 0.0093\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 2s 607ms/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 3s 661ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 3s 650ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 2s 612ms/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0104 - val_loss: 0.0114\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0110 - val_loss: 0.0135\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 2s 609ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 3s 716ms/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 2s 592ms/step - loss: 0.0107 - val_loss: 0.0134\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 3s 641ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0116 - val_loss: 0.0109\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 3s 636ms/step - loss: 0.0099 - val_loss: 0.0131\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 3s 669ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 2s 578ms/step - loss: 0.0107 - val_loss: 0.0117\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 3s 705ms/step - loss: 0.0108 - val_loss: 0.0110\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 3s 734ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 3s 721ms/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 3s 689ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 3s 709ms/step - loss: 0.0099 - val_loss: 0.0113\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 3s 794ms/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 3s 781ms/step - loss: 0.0104 - val_loss: 0.0090\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 3s 720ms/step - loss: 0.0107 - val_loss: 0.0109\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0097 - val_loss: 0.0115\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0091 - val_loss: 0.0110\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 2s 619ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0112 - val_loss: 0.0102\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 0.0116 - val_loss: 0.0096\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 3s 755ms/step - loss: 0.0109 - val_loss: 0.0093\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 3s 780ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0113 - val_loss: 0.0084\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 3s 749ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 2s 615ms/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0105 - val_loss: 0.0116\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 3s 704ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0105 - val_loss: 0.0143\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 3s 634ms/step - loss: 0.0112 - val_loss: 0.0100\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 3s 695ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 3s 750ms/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 3s 736ms/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 2s 624ms/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 2s 608ms/step - loss: 0.0118 - val_loss: 0.0121\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 3s 658ms/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.0106 - val_loss: 0.0115\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.0105 - val_loss: 0.0100\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 3s 635ms/step - loss: 0.0103 - val_loss: 0.0112\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 2s 553ms/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 3s 638ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 3s 662ms/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 3s 637ms/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0107 - val_loss: 0.0091\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 3s 712ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 3s 707ms/step - loss: 0.0101 - val_loss: 0.0093\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 3s 710ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 3s 682ms/step - loss: 0.0107 - val_loss: 0.0113\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 3s 730ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 3s 640ms/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 3s 645ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1832/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 558ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 3s 711ms/step - loss: 0.0108 - val_loss: 0.0092\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0113 - val_loss: 0.0085\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 3s 788ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 3s 708ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 3s 652ms/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 3s 674ms/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0100 - val_loss: 0.0127\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 3s 657ms/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 2s 594ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0112 - val_loss: 0.0095\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 3s 713ms/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 2s 606ms/step - loss: 0.0105 - val_loss: 0.0109\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 3s 626ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 3s 780ms/step - loss: 0.0103 - val_loss: 0.0091\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 3s 752ms/step - loss: 0.0110 - val_loss: 0.0095\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 3s 644ms/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 0.0106 - val_loss: 0.0094\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 3s 627ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 2s 614ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 3s 663ms/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 3s 698ms/step - loss: 0.0115 - val_loss: 0.0110\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 3s 725ms/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 3s 726ms/step - loss: 0.0098 - val_loss: 0.0092\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 3s 724ms/step - loss: 0.0106 - val_loss: 0.0123\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 3s 757ms/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 3s 819ms/step - loss: 0.0108 - val_loss: 0.0098\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 3s 685ms/step - loss: 0.0109 - val_loss: 0.0103\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 2s 524ms/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 2s 593ms/step - loss: 0.0101 - val_loss: 0.0112\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 2s 543ms/step - loss: 0.0105 - val_loss: 0.0126\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 2s 552ms/step - loss: 0.0107 - val_loss: 0.0108\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.0102 - val_loss: 0.0130\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 2s 451ms/step - loss: 0.0116 - val_loss: 0.0111\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 2s 525ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 2s 475ms/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 2s 437ms/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 2s 426ms/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0114 - val_loss: 0.0111\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 2s 426ms/step - loss: 0.0105 - val_loss: 0.0129\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 2s 408ms/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 2s 424ms/step - loss: 0.0110 - val_loss: 0.0127\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 0.0112 - val_loss: 0.0106\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 0.0110 - val_loss: 0.0133\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0108 - val_loss: 0.0118\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 0.0107 - val_loss: 0.0094\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0106 - val_loss: 0.0111\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0105 - val_loss: 0.0110\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 2s 402ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 0.0098 - val_loss: 0.0102\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 2s 427ms/step - loss: 0.0096 - val_loss: 0.0098\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 2s 398ms/step - loss: 0.0101 - val_loss: 0.0113\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 0.0111 - val_loss: 0.0119\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0117 - val_loss: 0.0121\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 2s 426ms/step - loss: 0.0107 - val_loss: 0.0116\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 2s 422ms/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0104 - val_loss: 0.0106\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 2s 426ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 2s 399ms/step - loss: 0.0118 - val_loss: 0.0110\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 2s 431ms/step - loss: 0.0105 - val_loss: 0.0105\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0109 - val_loss: 0.0100\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0101 - val_loss: 0.0125\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0122 - val_loss: 0.0114\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0104 - val_loss: 0.0116\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 2s 401ms/step - loss: 0.0103 - val_loss: 0.0114\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 2s 428ms/step - loss: 0.0109 - val_loss: 0.0109\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 0.0106 - val_loss: 0.0109\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 2s 424ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 2s 422ms/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0102 - val_loss: 0.0095\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0107 - val_loss: 0.0097\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 2s 421ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0109 - val_loss: 0.0097\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 2s 404ms/step - loss: 0.0099 - val_loss: 0.0094\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0100 - val_loss: 0.0113\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 0.0107 - val_loss: 0.0110\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0100 - val_loss: 0.0101\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0104 - val_loss: 0.0110\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 2s 413ms/step - loss: 0.0101 - val_loss: 0.0116\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 2s 424ms/step - loss: 0.0111 - val_loss: 0.0117\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 2s 405ms/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0105 - val_loss: 0.0124\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0110 - val_loss: 0.0130\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 2s 406ms/step - loss: 0.0109 - val_loss: 0.0099\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0109 - val_loss: 0.0114\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 0.0105 - val_loss: 0.0106\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0111 - val_loss: 0.0097\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 2s 397ms/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0106 - val_loss: 0.0097\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0108 - val_loss: 0.0109\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 2s 412ms/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0107 - val_loss: 0.0104\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0103 - val_loss: 0.0116\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 2s 422ms/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 2s 410ms/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 2s 416ms/step - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0102 - val_loss: 0.0117\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 2s 423ms/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 2s 417ms/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 0.0106 - val_loss: 0.0116\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 2s 433ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 2s 420ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 2s 391ms/step - loss: 0.0107 - val_loss: 0.0102\n",
      "Epoch 1990/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 409ms/step - loss: 0.0105 - val_loss: 0.0118\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 2s 426ms/step - loss: 0.0105 - val_loss: 0.0107\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 0.0101 - val_loss: 0.0102\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0111 - val_loss: 0.0120\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 2s 414ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 2s 396ms/step - loss: 0.0101 - val_loss: 0.0128\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 2s 425ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 2s 418ms/step - loss: 0.0112 - val_loss: 0.0120\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 2s 422ms/step - loss: 0.0102 - val_loss: 0.0127\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0106 - val_loss: 0.0098\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 2s 415ms/step - loss: 0.0108 - val_loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(tdg, callbacks=[mc,tb], initial_epoch=0\n",
    "                           ,steps_per_epoch=train_steps_per_epoch\n",
    "                           ,validation_data=vdg\n",
    "                           ,validation_steps=val_steps_per_epoch\n",
    "                           ,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# acc = hist.history['acc']\n",
    "# loss = hist.history['loss']\n",
    "\n",
    "# # Create count of the number of epochs\n",
    "# epoch_count = range(1, len(acc) + 1)\n",
    "\n",
    "# # Visualize loss history\n",
    "# # plt.plot(epoch_count, acc, 'b-')\n",
    "# fig, ax = plt.subplots(ncols=2,sharex=True)\n",
    "# ax[0].plot(epoch_count, loss, 'r--')\n",
    "# ax[0].legend(['Loss'])\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].set_ylabel('Loss')\n",
    "# ax[1].plot(epoch_count, acc, 'b-')\n",
    "# ax[1].legend(['Accuracy'])\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].set_xlabel('Accuracy')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<generator..., use_multiprocessing=True, steps=5)`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011516106873750686"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/dnf300_sa_sent_hd_vector.hdf5')\n",
    "model.evaluate_generator(test_dg,steps=5,pickle_safe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(test_dg)\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = np.random.randint(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 : reddit users declare war on hillary’s paid internet trolls\n",
      "0 : wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news\n",
      "98 : top aide: hillary ‘still not perfect in her head’, wikileaks\n",
      "30 : nsa whistleblower says dnc email hack was not by russia, but by us intelligence | alternative\n",
      "96 : us officials try to scare voters with terror threat\n",
      "35 : reddit users declare war on hillary’s paid internet trolls\n",
      "14 : department of homeland security chairman officially indicts hillary clinton of treason\n",
      "19 : leaked 2013 trump tax return shows he paid over 40 million in taxes\n",
      "14 : department of homeland security chairman officially indicts hillary clinton of treason\n",
      "19 : leaked 2013 trump tax return shows he paid over 40 million in taxes\n",
      "97 : hillary sold weapons to isis, wikileaks confirms\n",
      "39 : doj's loretta lynch tried to squash comey's letter to congress\n",
      "95 : us threatens military hacks on russia’s electric, communications grids over election\n",
      "18 : clinton camp demands 'compliant citizenry' for master plan\n",
      "21 : isis leader calls for american muslim voters to support hillary clinton\n",
      "6 : clinton received debate questions week before debate\n",
      "20 : the clinton foundation has purchased over $137 million of illegal arms and ammunition\n",
      "18 : clinton camp demands 'compliant citizenry' for master plan\n",
      "38 : hillary’s (islamic) america is already here where ‘muslim no-go zones’ are popping up all over michiganistan\n",
      "0 : wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news\n",
      "10 : physician confirms hillary clinton has parkinson's disease\n",
      "20 : the clinton foundation has purchased over $137 million of illegal arms and ammunition\n",
      "11 : fbi agent suspected in hillary email leaks found dead in apparent murder-suicide\n",
      "1 : hillary clinton wore secret earpiece during first presidential debate?\n",
      "4 : fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes\n",
      "98 : top aide: hillary ‘still not perfect in her head’, wikileaks\n",
      "9 : obama declares his family will move to canada if trump is elected\n",
      "1 : hillary clinton wore secret earpiece during first presidential debate?\n",
      "16 : george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\n",
      "4 : fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes\n",
      "36 : hillary clinton’s sudden move of $1.8 billion to qatar central bank stuns financial world\n",
      "6 : clinton received debate questions week before debate\n",
      "25 : erdoğan: us, the founder of isis\n",
      "37 : julian assange makes very suspect post election announcement, seeks pardon from trump\n",
      "26 : wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting\n",
      "30 : nsa whistleblower says dnc email hack was not by russia, but by us intelligence | alternative\n",
      "8 : hillary clinton cut her tax bill by 'donating' $1 million to herself via the clinton foundation?\n",
      "9 : obama declares his family will move to canada if trump is elected\n",
      "0 : wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news\n",
      "10 : physician confirms hillary clinton has parkinson's disease\n",
      "19 : leaked 2013 trump tax return shows he paid over 40 million in taxes\n",
      "14 : department of homeland security chairman officially indicts hillary clinton of treason\n",
      "31 : fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case\n",
      "2 : president obama confirms he will refuse to leave office if trump is elected\n",
      "5 : hillary clinton wore 'secret earpiece' during commander-in-chief forum\n",
      "26 : wikileaks: hillary got $12 million for clinton charity as quid pro quo for morocco meeting\n",
      "27 : trump accuses obama, hillary clinton of founding daesh\n",
      "25 : erdoğan: us, the founder of isis\n",
      "13 : hillary clinton in 2013: 'i would like to see people like donald trump run for office\n",
      "28 : he’s never sold an original painting until now…and this one’s going in the white house\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x['headline'])):\n",
    "    \n",
    "    print(x['article_id'][i],':',x['headline'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinton received debate questions week before debate'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = 15\n",
    "display(x['headline'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The first presidential debate was held and Hillary Clinton was proclaimed the winner by the media.',\n",
       " 'Indeed Clinton was able to turn in a strong debate performance, but did she do so fairly?',\n",
       " 'Multiple reports and leaked information from inside the Clinton camp claim that the Clinton campaign was given the entire set of debate questions an entire week before the actual debate.',\n",
       " 'Earlier last week an NBC intern was seen hand delivering a package to Clinton’s campaign headquarters, according to sources.',\n",
       " 'The package was not given to secretarial staff, as would normally happen, but the intern was instead ushered into the personal office of Clinton campaign manager Robert Mook.',\n",
       " 'Members of the Clinton press corps from several media organizations were in attendance at the time, and a reporter from Fox News recognized the intern, but said he was initially confused because the NBC intern was dressed like a Fed Ex employee.',\n",
       " 'The reporter from Fox questioned campaign staff about the intern, but campaign staff at first claimed ignorance and then claimed that it was just a Fed Ex employee who had already left.',\n",
       " 'No reporters present who had seen the intern dressed as a Fed Ex employee go into Mook’s office saw him leave by the same front entrance.',\n",
       " 'The Fox reporter who recognized the intern also immediately looked outside of the campaign headquarters and noted that there were no Fed Ex vehicles parked outside.',\n",
       " 'Clinton seemed to have scripted responses ready for every question she was asked at the first debate.',\n",
       " 'She had facts and numbers memorized for specific questions that it is very doubtful she would have had without being furnished the questions beforehand.',\n",
       " 'The entire mainstream media has specifically been trying to portray Trump as a racist and a poor candidate.',\n",
       " 'By furnishing Clinton with the debate questions NBC certainly hoped to make Clinton appear much more knowledgeable and competent than Trump.',\n",
       " 'And though it is unlikely that anyone will be able to conclusively prove that Clinton was given the debate questions, it seems both logical and likely.']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['sentences'][test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 32)       1568        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 35, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 32)       128         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 256)      98560       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 35, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 256)       1024        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 256)      1024        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 32)       1568        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 35, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 32)       128         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 256)      98560       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 35, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 256)       1024        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 256)      1024        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 32)       1568        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 35, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 32)       128         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 256)      98560       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 35, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 256)       1024        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 256)      1024        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 35, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 16)       14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 32)       1568        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 35, 32)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 32)       128         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 35, 32), (35 2377        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 35, 128)      0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 256)      98560       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 35, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 256)       1024        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 256)      1024        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 35, 256), (1 148033      batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(model.inputs,model.get_layer(name='ca1').output)\n",
    "model_2 = Model(model.inputs,model.get_layer(name='ca2').output)\n",
    "model_3 = Model(model.inputs,model.get_layer(name='ca3').output)\n",
    "model_4 = Model(model.inputs,model.get_layer(name='ca4').output)\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g1,g2,g3,g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 12,  3, 13,  4])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_N = 5\n",
    "t = b[0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.081066"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "b[0][:len(x['sentences'][test_idx])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'clinton received debate questions week before debate'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2, 9, 10, 12, 13]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x['article_id'][test_idx])\n",
    "display(x['headline'][test_idx])\n",
    "display(x['claims'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : Multiple reports and leaked information from inside the Clinton camp claim that the Clinton campaign was given the entire set of debate questions an entire week before the actual debate.\n",
      "12 : By furnishing Clinton with the debate questions NBC certainly hoped to make Clinton appear much more knowledgeable and competent than Trump.\n",
      "3 : Earlier last week an NBC intern was seen hand delivering a package to Clinton’s campaign headquarters, according to sources.\n",
      "13 : And though it is unlikely that anyone will be able to conclusively prove that Clinton was given the debate questions, it seems both logical and likely.\n",
      "4 : The package was not given to secretarial staff, as would normally happen, but the intern was instead ushered into the personal office of Clinton campaign manager Robert Mook.\n"
     ]
    }
   ],
   "source": [
    "for s in t:\n",
    "    if s>=len(x['sentences'][test_idx]):continue\n",
    "    print(s,':',x['sentences'][test_idx][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.081066"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "h_s_attended_vector = b[0][:len(x['sentences'][test_idx])]\n",
    "h_s_attended_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s_attended_vector = pd.DataFrame(h_s_attended_vector)\n",
    "\n",
    "\n",
    "xw = df_h_s_attended_vector.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xw_scaled = min_max_scaler.fit_transform(xw)\n",
    "df_h_s_attended_vector = pd.DataFrame(xw_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5bd4368550>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(2.0,7.0)})\n",
    "sns.heatmap(df_h_s_attended_vector, annot=True, cmap='YlGnBu', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa1 (SelfAttention)          [(None, 35, 32), (35, 35) 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa2 (SelfAttention)          [(None, 35, 32), (35, 35) 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa3 (SelfAttention)          [(None, 35, 32), (35, 35) 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 16)            14416     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 35, 32)            128       \n",
      "_________________________________________________________________\n",
      "sa4 (SelfAttention)          [(None, 35, 32), (35, 35) 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s1 = Model(model.inputs,model.get_layer(name='sa1').output)\n",
    "model_s2 = Model(model.inputs,model.get_layer(name='sa2').output)\n",
    "model_s3 = Model(model.inputs,model.get_layer(name='sa3').output)\n",
    "model_s4 = Model(model.inputs,model.get_layer(name='sa4').output)\n",
    "model_s1.summary()\n",
    "model_s2.summary()\n",
    "model_s3.summary()\n",
    "model_s4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sb1, sg1 = model_s1.predict([x['sentence_vectors'],x['input_headline_vector']])\n",
    "_, sb2, sg2 = model_s2.predict(x)\n",
    "_, sb3, sg3 = model_s3.predict(x)\n",
    "_, sb4, sg4 = model_s4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg1,sg2, sg3, sg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = sb1[test_idx]+sb2[test_idx]+sb3[test_idx]+sb4[test_idx]\n",
    "sb = sb[:len(x['sentences'][test_idx]),:len(x['sentences'][test_idx])]\n",
    "\n",
    "sb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = pd.DataFrame(sb)\n",
    "\n",
    "\n",
    "zx = df_sb.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "zx_scaled = min_max_scaler.fit_transform(zx)\n",
    "df_sb = pd.DataFrame(zx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5bd413bac8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(14.0,12.0)})\n",
    "sns.heatmap(pd.DataFrame(df_sb),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first presidential debate was held and Hillary Clinton was proclaimed the winner by the media.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indeed Clinton was able to turn in a strong debate performance, but did she do so fairly?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple reports and leaked information from inside the Clinton camp claim that the Clinton campaign was given the entire set of debate questions an entire week before the actual debate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Earlier last week an NBC intern was seen hand delivering a package to Clinton’s campaign headquarters, according to sources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The package was not given to secretarial staff, as would normally happen, but the intern was instead ushered into the personal office of Clinton campaign manager Robert Mook.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Members of the Clinton press corps from several media organizations were in attendance at the time, and a reporter from Fox News recognized the intern, but said he was initially confused because the NBC intern was dressed like a Fed Ex employee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The reporter from Fox questioned campaign staff about the intern, but campaign staff at first claimed ignorance and then claimed that it was just a Fed Ex employee who had already left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No reporters present who had seen the intern dressed as a Fed Ex employee go into Mook’s office saw him leave by the same front entrance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Fox reporter who recognized the intern also immediately looked outside of the campaign headquarters and noted that there were no Fed Ex vehicles parked outside.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clinton seemed to have scripted responses ready for every question she was asked at the first debate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>She had facts and numbers memorized for specific questions that it is very doubtful she would have had without being furnished the questions beforehand.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The entire mainstream media has specifically been trying to portray Trump as a racist and a poor candidate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>By furnishing Clinton with the debate questions NBC certainly hoped to make Clinton appear much more knowledgeable and competent than Trump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>And though it is unlikely that anyone will be able to conclusively prove that Clinton was given the debate questions, it seems both logical and likely.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        0\n",
       "0   The first presidential debate was held and Hillary Clinton was proclaimed the winner by the media.                                                                                                                                                   \n",
       "1   Indeed Clinton was able to turn in a strong debate performance, but did she do so fairly?                                                                                                                                                            \n",
       "2   Multiple reports and leaked information from inside the Clinton camp claim that the Clinton campaign was given the entire set of debate questions an entire week before the actual debate.                                                           \n",
       "3   Earlier last week an NBC intern was seen hand delivering a package to Clinton’s campaign headquarters, according to sources.                                                                                                                         \n",
       "4   The package was not given to secretarial staff, as would normally happen, but the intern was instead ushered into the personal office of Clinton campaign manager Robert Mook.                                                                       \n",
       "5   Members of the Clinton press corps from several media organizations were in attendance at the time, and a reporter from Fox News recognized the intern, but said he was initially confused because the NBC intern was dressed like a Fed Ex employee.\n",
       "6   The reporter from Fox questioned campaign staff about the intern, but campaign staff at first claimed ignorance and then claimed that it was just a Fed Ex employee who had already left.                                                            \n",
       "7   No reporters present who had seen the intern dressed as a Fed Ex employee go into Mook’s office saw him leave by the same front entrance.                                                                                                            \n",
       "8   The Fox reporter who recognized the intern also immediately looked outside of the campaign headquarters and noted that there were no Fed Ex vehicles parked outside.                                                                                 \n",
       "9   Clinton seemed to have scripted responses ready for every question she was asked at the first debate.                                                                                                                                                \n",
       "10  She had facts and numbers memorized for specific questions that it is very doubtful she would have had without being furnished the questions beforehand.                                                                                             \n",
       "11  The entire mainstream media has specifically been trying to portray Trump as a racist and a poor candidate.                                                                                                                                          \n",
       "12  By furnishing Clinton with the debate questions NBC certainly hoped to make Clinton appear much more knowledgeable and competent than Trump.                                                                                                         \n",
       "13  And though it is unlikely that anyone will be able to conclusively prove that Clinton was given the debate questions, it seems both logical and likely.                                                                                              "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x['sentences'][test_idx])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 95, 47)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hd_tp_cdc = pd.read_csv('evaluation_set/cdc_ibm/headline_topic_mapping.csv')\n",
    "df_ar_cl_cdc = pd.read_csv('evaluation_set/cdc_ibm/article_claim_mapping.csv')\n",
    "# df_hd_tp_dnf = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50.json')\n",
    "with open('evaluation_set/cdc_ibm/articles.p', 'rb') as fp:\n",
    "    articlesCDC = pickle.load(fp)\n",
    "with open('evaluation_set/cdc_ibm/article_vectors.p', 'rb') as fp:\n",
    "    article_vectorsCDC = pickle.load(fp)\n",
    "\n",
    "test_titles = []\n",
    "for ar in df_ar_cl_cdc.Article.unique():\n",
    "    if len(df_ar_cl_cdc[df_ar_cl_cdc.Article==ar]['Claim'].values)>8:\n",
    "        test_titles.append(ar)\n",
    "        \n",
    "titles = sorted(df_hd_tp_cdc.Title.unique())\n",
    "non_test_titles = list(set(titles)-set(test_titles))\n",
    "val_titles = non_test_titles[380:]\n",
    "train_titles = titles[:380]\n",
    "len(train_titles),len(val_titles),len(test_titles)\n",
    "# df_hd_tp_cdc.keys(),df_ar_cl_cdc.keys(), len(articles.keys()), len(article_vectors.keys()), df_hd_tp_dnf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_cdc(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds, ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        hd = dataframe[dataframe.Title==idx]['Headline'].values[0].lower()\n",
    "        hds.append(hd)\n",
    "        ar_id = dataframe[dataframe.Title==idx]['article Id'].values[0]\n",
    "        cl = df_ar_cl_cdc[df_ar_cl_cdc.Article==idx]['Claim'].values\n",
    "        ar_claims.append(cl)\n",
    "        sentences=articlesCDC[ar_id]\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((35,300))\n",
    "        vectors = article_vectorsCDC[ar_id]\n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            \n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "                \n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdgCDC = datagen_cdc(train_batchsize,df_hd_tp_cdc,mode='train')\n",
    "vdgCDC = datagen_cdc(val_batchsize,df_hd_tp_cdc,mode='val')\n",
    "test_dgCDC = datagen_cdc(test_batchsize,df_hd_tp_cdc,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 265 to array axis with dimension 35",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-3a6619f138d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxCDC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myCDC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dgCDC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxCDC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-298-1b69097d0132>\u001b[0m in \u001b[0;36mdatagen_cdc\u001b[0;34m(batchsize, dataframe, mode)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_vectorsCDC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mar_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mar_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mar_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 265 to array axis with dimension 35"
     ]
    }
   ],
   "source": [
    "xCDC,yCDC = next(test_dgCDC)\n",
    "xCDC.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected sentence_vectors to have shape (35, 300) but got array with shape (1300, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-b440e680f30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxCDC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxCDC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxCDC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                              'argument.')\n\u001b[1;32m   1146\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected sentence_vectors to have shape (35, 300) but got array with shape (1300, 300)"
     ]
    }
   ],
   "source": [
    "\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(xCDC)\n",
    "_, b2, g2 = model_2.predict(xCDC)\n",
    "_, b3, g3 = model_3.predict(xCDC)\n",
    "_, b4, g4 = model_4.predict(xCDC)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in range(len(xCDC['headline'])):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(xCDC['claims'][test_idx])\n",
    "    print(claims)\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    pred = b[0][:len(xCDC['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "    \n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for p in pred:\n",
    "        if p in claims:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    for c in claims:\n",
    "        if c not in pred:\n",
    "            fn+=1\n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "#     print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "    break\n",
    "#     counter+=1\n",
    "#     if counter==5:\n",
    "#         break\n",
    "    print(\"----------------------------\")\n",
    "#     for s in t:\n",
    "#         if s>=len(x['sentences'][test_idx]):continue\n",
    "#         x['sentences'][test_idx][s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , article id: 35 ,# sentences: 4 : 0.5 1.0\n",
      "----------------------------\n",
      "1 , article id: 0 ,# sentences: 20 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "2 , article id: 98 ,# sentences: 11 : 0.2 0.25\n",
      "----------------------------\n",
      "3 , article id: 30 ,# sentences: 9 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "4 , article id: 96 ,# sentences: 8 : 0.8 0.8\n",
      "----------------------------\n",
      "5 , article id: 35 ,# sentences: 4 : 0.5 1.0\n",
      "----------------------------\n",
      "6 , article id: 14 ,# sentences: 22 : 0.0 0.0\n",
      "----------------------------\n",
      "7 , article id: 19 ,# sentences: 13 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "8 , article id: 14 ,# sentences: 22 : 0.0 0.0\n",
      "----------------------------\n",
      "9 , article id: 19 ,# sentences: 13 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "10 , article id: 97 ,# sentences: 4 : 0.75 1.0\n",
      "----------------------------\n",
      "11 , article id: 39 ,# sentences: 8 : 0.4 0.5\n",
      "----------------------------\n",
      "12 , article id: 95 ,# sentences: 10 : 0.2 0.25\n",
      "----------------------------\n",
      "13 , article id: 18 ,# sentences: 8 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "14 , article id: 21 ,# sentences: 6 : 0.6 0.75\n",
      "----------------------------\n",
      "15 , article id: 6 ,# sentences: 14 : 0.6 0.6\n",
      "----------------------------\n",
      "16 , article id: 20 ,# sentences: 5 : 0.8 1.0\n",
      "----------------------------\n",
      "17 , article id: 18 ,# sentences: 8 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "18 , article id: 38 ,# sentences: 10 : 0.2 0.25\n",
      "----------------------------\n",
      "19 , article id: 0 ,# sentences: 20 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "20 , article id: 10 ,# sentences: 17 : 0.2 0.25\n",
      "----------------------------\n",
      "21 , article id: 20 ,# sentences: 5 : 0.8 1.0\n",
      "----------------------------\n",
      "22 , article id: 11 ,# sentences: 19 : 0.0 0.0\n",
      "----------------------------\n",
      "23 , article id: 1 ,# sentences: 9 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "24 , article id: 4 ,# sentences: 35 : 0.2 0.2\n",
      "----------------------------\n",
      "25 , article id: 98 ,# sentences: 11 : 0.2 0.25\n",
      "----------------------------\n",
      "26 , article id: 9 ,# sentences: 13 : 0.6 1.0\n",
      "----------------------------\n",
      "27 , article id: 1 ,# sentences: 9 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "28 , article id: 16 ,# sentences: 10 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "29 , article id: 4 ,# sentences: 35 : 0.2 0.2\n",
      "----------------------------\n",
      "30 , article id: 36 ,# sentences: 11 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "31 , article id: 6 ,# sentences: 14 : 0.6 0.6\n",
      "----------------------------\n",
      "32 , article id: 25 ,# sentences: 2 : 1.0 1.0\n",
      "----------------------------\n",
      "33 , article id: 37 ,# sentences: 9 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "34 , article id: 26 ,# sentences: 22 : 0.0 0.0\n",
      "----------------------------\n",
      "35 , article id: 30 ,# sentences: 9 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "36 , article id: 8 ,# sentences: 7 : 0.4 0.6666666666666666\n",
      "----------------------------\n",
      "37 , article id: 9 ,# sentences: 13 : 0.6 1.0\n",
      "----------------------------\n",
      "38 , article id: 0 ,# sentences: 20 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "39 , article id: 10 ,# sentences: 17 : 0.2 0.25\n",
      "----------------------------\n",
      "40 , article id: 19 ,# sentences: 13 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "41 , article id: 14 ,# sentences: 22 : 0.0 0.0\n",
      "----------------------------\n",
      "42 , article id: 31 ,# sentences: 12 : 0.0 0.0\n",
      "----------------------------\n",
      "43 , article id: 2 ,# sentences: 18 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "44 , article id: 5 ,# sentences: 21 : 0.2 0.3333333333333333\n",
      "----------------------------\n",
      "45 , article id: 26 ,# sentences: 22 : 0.0 0.0\n",
      "----------------------------\n",
      "46 , article id: 27 ,# sentences: 17 : 0.0 0.0\n",
      "----------------------------\n",
      "47 , article id: 25 ,# sentences: 2 : 1.0 1.0\n",
      "----------------------------\n",
      "48 , article id: 13 ,# sentences: 12 : 0.8 0.8\n",
      "----------------------------\n",
      "49 , article id: 28 ,# sentences: 12 : 0.0 0.0\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.335, 0.45899999999999996, 0.3873173803526448)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in range(len(x['headline'])):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(x['claims'][test_idx])\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    pred = b[0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "    \n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for p in pred:\n",
    "        if p in claims:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    for c in claims:\n",
    "        if c not in pred:\n",
    "            fn+=1\n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "    print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "#     counter+=1\n",
    "#     if counter==5:\n",
    "#         break\n",
    "    print(\"----------------------------\")\n",
    "#     for s in t:\n",
    "#         if s>=len(x['sentences'][test_idx]):continue\n",
    "#         x['sentences'][test_idx][s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.382, 0.488, 0.4285425287356322)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(ps), np.average(rs), 2*np.average(ps)*np.average(rs)/(np.average(ps)+ np.average(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(pred)+list(claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 6])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
