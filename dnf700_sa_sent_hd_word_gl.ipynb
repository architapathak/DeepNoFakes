{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from spacy.lang.en import English\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.layers import BatchNormalization, Lambda, Concatenate, Dropout, Conv1D, MaxPooling1D, Input, TimeDistributed, Dense, LSTM, RepeatVector, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from AttentionModules import SelfAttention, CrossAttention\n",
    "import sys,os\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['authors', 'claim_ids', 'evidence', 'headline', 'id', 'reason',\n",
       "        'claims', 'type', 'urls'],\n",
       "       dtype='object'),\n",
       " Index(['authors', 'headline', 'id', 'type', 'urls'], dtype='object'),\n",
       " 705,\n",
       " 705)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnf700 = pd.read_json('evaluation_set/deepnofakes/dnf_700/initial.json')\n",
    "dnf_eval = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V4.json')\n",
    "dnf_eval.columns = ['authors','claim_ids', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls'] \n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_array_id.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_700/dnf700_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_array_id.p', 'rb') as fp:\n",
    "    articles300 = pickle.load(fp)\n",
    "with open('evaluation_set/deepnofakes/dnf_300/cleaned/cleaned_dnf300_sent_vector_array_id.p', 'rb') as fp:\n",
    "    article_vectors300 = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "dnf_eval.keys(), dnf700.keys(), len(articles.keys()), len(article_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_splits = 5\n",
    "kf = KFold(n_splits=num_splits)\n",
    "train_batchsize = 32\n",
    "val_batchsize = 32\n",
    "test_batchsize = 50\n",
    "train_steps_per_epoch = 4\n",
    "val_steps_per_epoch = 1\n",
    "epochs = 2000\n",
    "max_sentences = 500\n",
    "# for idx in articles.keys():\n",
    "#     num = len(articles[idx])\n",
    "#     if num>=max_sentences:\n",
    "#         max_sentences = num\n",
    "        \n",
    "max_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [authors, headline, id, type, urls]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdl = \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\"\n",
    "hdl = \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\"\n",
    "# hdl = \"If You Thought The Trump Child Rape Case In NY Couldn’t Get Much Worse — You Were Wrong\"\n",
    "# hdl = \"California Set to Let Public Schools Teach Primarily in Spanish\"\n",
    "dnf700[dnf700.headline==hdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = sorted(dnf700.headline.unique())\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(VIDEO) Female College Students Protesting Because ‘Trump is a Rapist’',\n",
       " 'Assange Confirms: WikiLeaks Didn’t Get Emails From Russian Govt',\n",
       " 'BREAKING: Fraudulent Clinton Votes Discovered By The Tens Of Thousands',\n",
       " \"Clinton Camp Demands 'Compliant Citizenry' for Master Plan\",\n",
       " 'Clinton Received Debate Questions Week Before Debate',\n",
       " \"DOJ's Loretta Lynch Tried To Squash Comey's Letter To Congress\",\n",
       " 'Department of Homeland Security Chairman Officially Indicts Hillary Clinton of Treason',\n",
       " 'Developing: Obama WH admits that Hillary gave ISIS $400 million on accident',\n",
       " 'Erdoğan: US, the founder of ISIS',\n",
       " \"FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Healthcare Begins With A Bombshell! » 100percentfedUp.com\",\n",
       " 'FBI Agent Suspected in Hillary Email Leaks Found Dead in Apparent Murder-Suicide',\n",
       " 'FBI Director Comey’s ‘Leaked’ Memo Explains Why He’s Reopening the Clinton Email Case',\n",
       " 'FBI director received millions from Clinton Foundation, his brother’s law firm does Clinton’s taxes',\n",
       " 'Former NATO Chief: We Need US as ‘World’s Policeman’',\n",
       " \"George Soros: Trump Will Win Popular Vote by a Landslide but Clinton Victory a 'Done Deal'\",\n",
       " 'HE’S NEVER SOLD AN ORIGINAL PAINTING UNTIL NOW…And This One’s Going In The White House',\n",
       " 'HILLARY’S (Islamic) AMERICA IS ALREADY HERE where ‘Muslim NO-GO ZONES’ are popping up all over Michiganistan',\n",
       " \"Hillary Clinton Cut Her Tax Bill by 'Donating' $1 Million to Herself via the Clinton Foundation?\",\n",
       " 'Hillary Clinton Used Hand Signals to Rig Debate?',\n",
       " \"Hillary Clinton Wore 'Secret Earpiece' During Commander-in-Chief Forum\",\n",
       " 'Hillary Clinton Wore Secret Earpiece During First Presidential Debate?',\n",
       " \"Hillary Clinton in 2013: 'I Would Like to See People Like Donald Trump Run for Office\",\n",
       " \"Hillary Clinton's 'Sudden Move' of $1.8 Billion to Qatar Central Bank Stuns Financial World\",\n",
       " 'Hillary Clinton’s Sudden Move Of $1.8 Billion To Qatar Central Bank Stuns Financial World',\n",
       " 'Hillary Friend Bribed FBI Agent and His Wife',\n",
       " 'Hillary Personally Ordered ‘Donald Duck’ Troll Campaign',\n",
       " 'Hillary Sold Weapons To ISIS, Wikileaks Confirms',\n",
       " 'ISIS Leader Calls for American Muslim Voters to Support Hillary Clinton',\n",
       " 'Jill Stein Endorsed Donald Trump',\n",
       " 'Julian Assange Makes VERY Suspect Post Election Announcement, Seeks Pardon From Trump',\n",
       " 'KREMLIN: Putin Congratulates Trump, Hopes to Work Together Major Issues',\n",
       " 'LOL! BRITISH WIFE Of LIB ACTOR Who Said: There Will Never Be A President Donald Trump…Warns Americans About President-Elect Trump [VIDEO]',\n",
       " 'Leaked 2013 Trump Tax Return Shows He Paid Over 40 Million in Taxes',\n",
       " 'NSA Whistleblower Says DNC Email Hack Was Not by Russia, but by US Intelligence | Alternative',\n",
       " 'Obama Declares His Family Will Move to Canada If Trump Is Elected',\n",
       " 'Pentagon Officials Furious After Clinton Announces US Response Time for Nuclear Launch During Debate',\n",
       " 'Pentagon Seeks Another $6 Billion for Overseas Troop Deployments',\n",
       " \"Physician Confirms Hillary Clinton Has Parkinson's Disease\",\n",
       " 'President Obama Confirms He Will Refuse to Leave Office If Trump Is Elected',\n",
       " 'Reddit Users Declare War On Hillary’s Paid Internet Trolls',\n",
       " \"Ted Cruz Said 'If Something Happens to Hillary' He'll 'Run as a Democrat Against Trump'\",\n",
       " 'The Clinton Foundation has purchased over $137 million of illegal arms and ammunition',\n",
       " 'Top aide: Hillary ‘still not perfect in her head’, Wikileaks',\n",
       " 'Trump accuses Obama, Hillary Clinton of founding Daesh',\n",
       " 'US Officials See No Link Between Trump and Russia',\n",
       " 'US Officials Try to Scare Voters With Terror Threat',\n",
       " 'US Threatens Military Hacks on Russia’s Electric, Communications Grids Over Election',\n",
       " 'WIKILEAKS: Hillary Got $12 Million for Clinton Charity As Quid Pro Quo For Morocco Meeting',\n",
       " 'WikiLeaks CONFIRMS Hillary Sold Weapons to ISIS... Then Drops Another BOMBSHELL! Breaking News',\n",
       " 'WikiLeaks: Hillary Clinton knew Saudi, Qatar were funding ISIS – but still took their money for Foundation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_titles = sorted(dnf_eval.headline.unique())\n",
    "len(test_titles)\n",
    "test_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_test_titles = np.array(list(set(titles)-set(test_titles)))\n",
    "len(non_test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for train_index, val_index in kf.split(non_test_titles):\n",
    "    indices.append([train_index,val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522] [523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
      " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
      " 649 650 651 652]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(523, 130, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_index, val_index = indices[np.random.randint(0,num_splits)]\n",
    "print(train_index,val_index)\n",
    "val_titles = non_test_titles[val_index]\n",
    "train_titles = non_test_titles[train_index]\n",
    "len(train_titles),len(val_titles),len(test_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    sentencizer = English()\n",
    "    sentencizer.add_pipe(sentencizer.create_pipe('sentencizer'))\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    return sentencizer, nlp\n",
    "sentencizer, nlp = load_spacy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf(batchsize,dataframe,mode):\n",
    "    counter=0\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "    while True:\n",
    "        if mode=='train':\n",
    "            idx=np.random.choice(train_titles)\n",
    "        elif mode=='val':\n",
    "            idx=np.random.choice(val_titles)\n",
    "        elif mode=='test':\n",
    "            idx=np.random.choice(test_titles)\n",
    "        idx = idx.strip()\n",
    "        \n",
    "            \n",
    "#         cl = dataframe[dataframe.Article==idx]['Claim'].values\n",
    "#         sentences=articles[ar_id]\n",
    "#         print(len(sentences))\n",
    "        if mode=='test':\n",
    "            hd = dnf_eval[dnf_eval.headline==idx]['headline'].values[0].lower()\n",
    "            ar_id = dnf_eval[dnf_eval.headline==idx]['id'].values[0]\n",
    "            cl = dnf_eval[dnf_eval.headline==idx]['claims'].values[0]\n",
    "            ar_claims.append(cl)\n",
    "            sentences = articles300[ar_id]\n",
    "            vectors = article_vectors300[ar_id]\n",
    "        else:\n",
    "            try:\n",
    "                hd = dataframe[dataframe.headline==idx]['headline'].values[0].lower()\n",
    "                ar_id = dataframe[dataframe.headline==idx]['id'].values[0]\n",
    "                ar_claims.append('None')\n",
    "                sentences=articles[ar_id]\n",
    "                vectors = article_vectors[ar_id]\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                print(idx)\n",
    "            \n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "#         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "        \n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "        counter+=1\n",
    "        if counter==batchsize:\n",
    "            inputs = {\n",
    "                'article_id': np.array(ar_ids)\n",
    "                ,'headline': np.array(hds)\n",
    "                ,'sentence_vectors' : np.array(ar_sents)\n",
    "                ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "                ,'claims':np.array(ar_claims)\n",
    "                ,'sentences':np.array(ar_sentences)\n",
    "            }\n",
    "            outputs = {\n",
    "                'headline_token_classes': np.array(ar_head_classes)\n",
    "                ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "            }\n",
    "            yield inputs,outputs\n",
    "            ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "            counter=0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = datagen_dnf(train_batchsize,dnf700,mode='train')\n",
    "vdg = datagen_dnf(val_batchsize,dnf700,mode='val')\n",
    "test_dg = datagen_dnf(test_batchsize,dnf700,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(test_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['sentence_vectors'].shape, x['headline_vector'].shape, y['headline_token_classes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 500, 1024)    0           ca1[0][0]                        \n",
      "                                                                 ca2[0][0]                        \n",
      "                                                                 ca3[0][0]                        \n",
      "                                                                 ca4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 500, 1024)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 500, 1024)    4096        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 250, 256)     786688      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 125, 256)     196864      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 63, 256)      196864      conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 32, 256)      196864      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 256)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 256)      1024        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 50, 256)      0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50, 256)      525312      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 50, 256)      0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 256)      1024        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "headline_token_classes (TimeDis (None, 50, 20000)    5140000     batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,844,152\n",
      "Trainable params: 7,839,992\n",
      "Non-trainable params: 4,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2213pt\" viewBox=\"0.00 0.00 1810.50 2213.00\" width=\"1811pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 2209)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-2209 1806.5,-2209 1806.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139980986084656 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139980986084656</title>\n",
       "<polygon fill=\"none\" points=\"857,-2158.5 857,-2204.5 1198,-2204.5 1198,-2158.5 857,-2158.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"945\" y=\"-2177.8\">sentence_vectors: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1033,-2158.5 1033,-2204.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1060.5\" y=\"-2189.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1033,-2181.5 1088,-2181.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1060.5\" y=\"-2166.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1088,-2158.5 1088,-2204.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1143\" y=\"-2189.3\">(None, 500, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1088,-2181.5 1198,-2181.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1143\" y=\"-2166.3\">(None, 500, 300)</text>\n",
       "</g>\n",
       "<!-- 139980986084880 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139980986084880</title>\n",
       "<polygon fill=\"none\" points=\"883.5,-2075.5 883.5,-2121.5 1171.5,-2121.5 1171.5,-2075.5 883.5,-2075.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"945\" y=\"-2094.8\">conv1d_1: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1006.5,-2075.5 1006.5,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1034\" y=\"-2106.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1006.5,-2098.5 1061.5,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1034\" y=\"-2083.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1061.5,-2075.5 1061.5,-2121.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-2106.3\">(None, 500, 300)</text>\n",
       "<polyline fill=\"none\" points=\"1061.5,-2098.5 1171.5,-2098.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-2083.3\">(None, 500, 16)</text>\n",
       "</g>\n",
       "<!-- 139980986084656&#45;&gt;139980986084880 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139980986084656-&gt;139980986084880</title>\n",
       "<path d=\"M1027.5,-2158.3799C1027.5,-2150.1745 1027.5,-2140.7679 1027.5,-2131.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-2131.784 1027.5,-2121.784 1024.0001,-2131.784 1031.0001,-2131.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980986118664 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139980986118664</title>\n",
       "<polygon fill=\"none\" points=\"885.5,-1992.5 885.5,-2038.5 1169.5,-2038.5 1169.5,-1992.5 885.5,-1992.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-2011.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1992.5 1010.5,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-2023.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-2015.5 1065.5,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-2000.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1992.5 1065.5,-2038.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-2023.3\">(None, 500, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-2015.5 1169.5,-2015.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-2000.3\">(None, 500, 16)</text>\n",
       "</g>\n",
       "<!-- 139980986084880&#45;&gt;139980986118664 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139980986084880-&gt;139980986118664</title>\n",
       "<path d=\"M1027.5,-2075.3799C1027.5,-2067.1745 1027.5,-2057.7679 1027.5,-2048.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-2048.784 1027.5,-2038.784 1024.0001,-2048.784 1031.0001,-2048.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980986118328 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139980986118328</title>\n",
       "<polygon fill=\"none\" points=\"886.5,-1909.5 886.5,-1955.5 1168.5,-1955.5 1168.5,-1909.5 886.5,-1909.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1928.8\">conv1d_2: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1009.5,-1909.5 1009.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1037\" y=\"-1940.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1009.5,-1932.5 1064.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1037\" y=\"-1917.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1064.5,-1909.5 1064.5,-1955.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-1940.3\">(None, 500, 16)</text>\n",
       "<polyline fill=\"none\" points=\"1064.5,-1932.5 1168.5,-1932.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1116.5\" y=\"-1917.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 139980986118664&#45;&gt;139980986118328 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139980986118664-&gt;139980986118328</title>\n",
       "<path d=\"M1027.5,-1992.3799C1027.5,-1984.1745 1027.5,-1974.7679 1027.5,-1965.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1965.784 1027.5,-1955.784 1024.0001,-1965.784 1031.0001,-1965.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980986250632 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139980986250632</title>\n",
       "<polygon fill=\"none\" points=\"885.5,-1826.5 885.5,-1872.5 1169.5,-1872.5 1169.5,-1826.5 885.5,-1826.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1845.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1826.5 1010.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1010.5,-1849.5 1065.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1038\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1826.5 1065.5,-1872.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1857.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1065.5,-1849.5 1169.5,-1849.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1117.5\" y=\"-1834.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 139980986118328&#45;&gt;139980986250632 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139980986118328-&gt;139980986250632</title>\n",
       "<path d=\"M1027.5,-1909.3799C1027.5,-1901.1745 1027.5,-1891.7679 1027.5,-1882.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1882.784 1027.5,-1872.784 1024.0001,-1882.784 1031.0001,-1882.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980986252872 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139980986252872</title>\n",
       "<polygon fill=\"none\" points=\"818,-1743.5 818,-1789.5 1237,-1789.5 1237,-1743.5 818,-1743.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"948\" y=\"-1762.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1078,-1743.5 1078,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1105.5\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1078,-1766.5 1133,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1105.5\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1133,-1743.5 1133,-1789.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185\" y=\"-1774.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1133,-1766.5 1237,-1766.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1185\" y=\"-1751.3\">(None, 500, 32)</text>\n",
       "</g>\n",
       "<!-- 139980986250632&#45;&gt;139980986252872 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139980986250632-&gt;139980986252872</title>\n",
       "<path d=\"M1027.5,-1826.3799C1027.5,-1818.1745 1027.5,-1808.7679 1027.5,-1799.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1031.0001,-1799.784 1027.5,-1789.784 1024.0001,-1799.784 1031.0001,-1799.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980985507120 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139980985507120</title>\n",
       "<polygon fill=\"none\" points=\"252.5,-1660.5 252.5,-1706.5 626.5,-1706.5 626.5,-1660.5 252.5,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-1679.8\">sa1: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"367.5,-1660.5 367.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"395\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"367.5,-1683.5 422.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"395\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"422.5,-1660.5 422.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.5\" y=\"-1691.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"422.5,-1683.5 626.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524.5\" y=\"-1668.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980986252872&#45;&gt;139980985507120 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139980986252872-&gt;139980985507120</title>\n",
       "<path d=\"M864.4901,-1743.4901C786.1844,-1732.4367 692.2952,-1719.1837 612.7036,-1707.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"612.9631,-1704.4508 602.572,-1706.5187 611.9847,-1711.3821 612.9631,-1704.4508\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980984966784 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139980984966784</title>\n",
       "<polygon fill=\"none\" points=\"644.5,-1660.5 644.5,-1706.5 1018.5,-1706.5 1018.5,-1660.5 644.5,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"702\" y=\"-1679.8\">sa2: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"759.5,-1660.5 759.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"759.5,-1683.5 814.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"814.5,-1660.5 814.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-1691.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"814.5,-1683.5 1018.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"916.5\" y=\"-1668.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980986252872&#45;&gt;139980984966784 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139980986252872-&gt;139980984966784</title>\n",
       "<path d=\"M973.1634,-1743.4901C949.0535,-1733.2803 920.5118,-1721.1938 895.3877,-1710.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"896.5156,-1707.2313 885.9423,-1706.5547 893.7859,-1713.6771 896.5156,-1707.2313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980983688832 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139980983688832</title>\n",
       "<polygon fill=\"none\" points=\"1036.5,-1660.5 1036.5,-1706.5 1410.5,-1706.5 1410.5,-1660.5 1036.5,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-1679.8\">sa3: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1151.5,-1660.5 1151.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1179\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1151.5,-1683.5 1206.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1179\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1206.5,-1660.5 1206.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1308.5\" y=\"-1691.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1206.5,-1683.5 1410.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1308.5\" y=\"-1668.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980986252872&#45;&gt;139980983688832 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139980986252872-&gt;139980983688832</title>\n",
       "<path d=\"M1081.8366,-1743.4901C1105.9465,-1733.2803 1134.4882,-1721.1938 1159.6123,-1710.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1161.2141,-1713.6771 1169.0577,-1706.5547 1158.4844,-1707.2313 1161.2141,-1713.6771\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980982946672 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>139980982946672</title>\n",
       "<polygon fill=\"none\" points=\"1428.5,-1660.5 1428.5,-1706.5 1802.5,-1706.5 1802.5,-1660.5 1428.5,-1660.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1486\" y=\"-1679.8\">sa4: SelfAttention</text>\n",
       "<polyline fill=\"none\" points=\"1543.5,-1660.5 1543.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1571\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1543.5,-1683.5 1598.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1571\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1598.5,-1660.5 1598.5,-1706.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1700.5\" y=\"-1691.3\">(None, 500, 32)</text>\n",
       "<polyline fill=\"none\" points=\"1598.5,-1683.5 1802.5,-1683.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1700.5\" y=\"-1668.3\">[(None, 500, 32), (500, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980986252872&#45;&gt;139980982946672 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139980986252872-&gt;139980982946672</title>\n",
       "<path d=\"M1190.5099,-1743.4901C1268.8156,-1732.4367 1362.7048,-1719.1837 1442.2964,-1707.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1443.0153,-1711.3821 1452.428,-1706.5187 1442.0369,-1704.4508 1443.0153,-1711.3821\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980982252880 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>139980982252880</title>\n",
       "<polygon fill=\"none\" points=\"718,-1577.5 718,-1623.5 1337,-1623.5 1337,-1577.5 718,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"802\" y=\"-1596.8\">concatenate_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"886,-1577.5 886,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"886,-1600.5 941,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"913.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"941,-1577.5 941,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1139\" y=\"-1608.3\">[(None, 500, 32), (None, 500, 32), (None, 500, 32), (None, 500, 32)]</text>\n",
       "<polyline fill=\"none\" points=\"941,-1600.5 1337,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1139\" y=\"-1585.3\">(None, 500, 128)</text>\n",
       "</g>\n",
       "<!-- 139980985507120&#45;&gt;139980982252880 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>139980985507120-&gt;139980982252880</title>\n",
       "<path d=\"M602.5099,-1660.4901C680.8156,-1649.4367 774.7048,-1636.1837 854.2964,-1624.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"855.0153,-1628.3821 864.428,-1623.5187 854.0369,-1621.4508 855.0153,-1628.3821\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980984966784&#45;&gt;139980982252880 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>139980984966784-&gt;139980982252880</title>\n",
       "<path d=\"M885.8366,-1660.4901C909.9465,-1650.2803 938.4882,-1638.1938 963.6123,-1627.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"965.2141,-1630.6771 973.0577,-1623.5547 962.4844,-1624.2313 965.2141,-1630.6771\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980983688832&#45;&gt;139980982252880 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>139980983688832-&gt;139980982252880</title>\n",
       "<path d=\"M1169.1634,-1660.4901C1145.0535,-1650.2803 1116.5118,-1638.1938 1091.3877,-1627.5545\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1092.5156,-1624.2313 1081.9423,-1623.5547 1089.7859,-1630.6771 1092.5156,-1624.2313\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980982946672&#45;&gt;139980982252880 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>139980982946672-&gt;139980982252880</title>\n",
       "<path d=\"M1452.4901,-1660.4901C1374.1844,-1649.4367 1280.2952,-1636.1837 1200.7036,-1624.9488\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1200.9631,-1621.4508 1190.572,-1623.5187 1199.9847,-1628.3821 1200.9631,-1621.4508\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980986084824 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>139980986084824</title>\n",
       "<polygon fill=\"none\" points=\"357,-1577.5 357,-1623.5 700,-1623.5 700,-1577.5 357,-1577.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459.5\" y=\"-1596.8\">input_headline_vector: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"562,-1577.5 562,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"562,-1600.5 617,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"617,-1577.5 617,-1623.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658.5\" y=\"-1608.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"617,-1600.5 700,-1600.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658.5\" y=\"-1585.3\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 139980981504264 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>139980981504264</title>\n",
       "<polygon fill=\"none\" points=\"439.5,-1494.5 439.5,-1540.5 679.5,-1540.5 679.5,-1494.5 439.5,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"490.5\" y=\"-1513.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1494.5 541.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1517.5 596.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"569\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"596.5,-1494.5 596.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638\" y=\"-1525.3\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"596.5,-1517.5 679.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"638\" y=\"-1502.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 139980986084824&#45;&gt;139980981504264 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>139980986084824-&gt;139980981504264</title>\n",
       "<path d=\"M537.1352,-1577.3799C540.2665,-1568.9962 543.8662,-1559.3584 547.2495,-1550.2996\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"550.5834,-1551.3766 550.8036,-1540.784 544.0259,-1548.9273 550.5834,-1551.3766\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980985236672 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>139980985236672</title>\n",
       "<polygon fill=\"none\" points=\"881.5,-1494.5 881.5,-1540.5 1169.5,-1540.5 1169.5,-1494.5 881.5,-1494.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"943\" y=\"-1513.8\">conv1d_3: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"1004.5,-1494.5 1004.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1032\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1004.5,-1517.5 1059.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1032\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1059.5,-1494.5 1059.5,-1540.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1114.5\" y=\"-1525.3\">(None, 500, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1059.5,-1517.5 1169.5,-1517.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1114.5\" y=\"-1502.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 139980982252880&#45;&gt;139980985236672 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>139980982252880-&gt;139980985236672</title>\n",
       "<path d=\"M1026.9429,-1577.3799C1026.7452,-1569.1745 1026.5185,-1559.7679 1026.3043,-1550.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1029.801,-1550.6968 1026.0611,-1540.784 1022.8031,-1550.8655 1029.801,-1550.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980811536 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>139980980811536</title>\n",
       "<polygon fill=\"none\" points=\"437,-1411.5 437,-1457.5 712,-1457.5 712,-1411.5 437,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"498.5\" y=\"-1430.8\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"560,-1411.5 560,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"560,-1434.5 615,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"615,-1411.5 615,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"663.5\" y=\"-1442.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"615,-1434.5 712,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"663.5\" y=\"-1419.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 139980981504264&#45;&gt;139980980811536 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>139980981504264-&gt;139980980811536</title>\n",
       "<path d=\"M563.6783,-1494.3799C565.1612,-1486.1745 566.8612,-1476.7679 568.4677,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"571.9578,-1468.2471 570.292,-1457.784 565.0693,-1467.0021 571.9578,-1468.2471\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395072 -->\n",
       "<g class=\"node\" id=\"node16\">\n",
       "<title>139980981395072</title>\n",
       "<polygon fill=\"none\" points=\"876.5,-1411.5 876.5,-1457.5 1166.5,-1457.5 1166.5,-1411.5 876.5,-1411.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"939\" y=\"-1430.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"1001.5,-1411.5 1001.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1029\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1001.5,-1434.5 1056.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1029\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1056.5,-1411.5 1056.5,-1457.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-1442.3\">(None, 500, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1056.5,-1434.5 1166.5,-1434.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1111.5\" y=\"-1419.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 139980985236672&#45;&gt;139980981395072 -->\n",
       "<g class=\"edge\" id=\"edge17\">\n",
       "<title>139980985236672-&gt;139980981395072</title>\n",
       "<path d=\"M1024.3858,-1494.3799C1023.9903,-1486.1745 1023.537,-1476.7679 1023.1086,-1467.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1026.5995,-1467.6039 1022.6221,-1457.784 1019.6076,-1467.9409 1026.5995,-1467.6039\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980250384 -->\n",
       "<g class=\"node\" id=\"node17\">\n",
       "<title>139980980250384</title>\n",
       "<polygon fill=\"none\" points=\"376.5,-1328.5 376.5,-1374.5 788.5,-1374.5 788.5,-1328.5 376.5,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-1347.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"636.5,-1328.5 636.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"664\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"636.5,-1351.5 691.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"664\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"691.5,-1328.5 691.5,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"740\" y=\"-1359.3\">(None, 1, 256)</text>\n",
       "<polyline fill=\"none\" points=\"691.5,-1351.5 788.5,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"740\" y=\"-1336.3\">(None, 1, 256)</text>\n",
       "</g>\n",
       "<!-- 139980980811536&#45;&gt;139980980250384 -->\n",
       "<g class=\"edge\" id=\"edge18\">\n",
       "<title>139980980811536-&gt;139980980250384</title>\n",
       "<path d=\"M576.7284,-1411.3799C577.5193,-1403.1745 578.426,-1393.7679 579.2828,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"582.7801,-1385.0737 580.2558,-1374.784 575.8124,-1384.4021 582.7801,-1385.0737\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395128 -->\n",
       "<g class=\"node\" id=\"node18\">\n",
       "<title>139980981395128</title>\n",
       "<polygon fill=\"none\" points=\"807,-1328.5 807,-1374.5 1232,-1374.5 1232,-1328.5 807,-1328.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"937\" y=\"-1347.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1067,-1328.5 1067,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094.5\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1067,-1351.5 1122,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094.5\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1122,-1328.5 1122,-1374.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1177\" y=\"-1359.3\">(None, 500, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1122,-1351.5 1232,-1351.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1177\" y=\"-1336.3\">(None, 500, 256)</text>\n",
       "</g>\n",
       "<!-- 139980981395072&#45;&gt;139980981395128 -->\n",
       "<g class=\"edge\" id=\"edge19\">\n",
       "<title>139980981395072-&gt;139980981395128</title>\n",
       "<path d=\"M1020.9429,-1411.3799C1020.7452,-1403.1745 1020.5185,-1393.7679 1020.3043,-1384.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1023.801,-1384.6968 1020.0611,-1374.784 1016.8031,-1384.8655 1023.801,-1384.6968\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980064040 -->\n",
       "<g class=\"node\" id=\"node19\">\n",
       "<title>139980980064040</title>\n",
       "<polygon fill=\"none\" points=\"810,-1245.5 810,-1291.5 1197,-1291.5 1197,-1245.5 810,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872.5\" y=\"-1264.8\">ca1: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"935,-1245.5 935,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"962.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"935,-1268.5 990,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"962.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"990,-1245.5 990,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093.5\" y=\"-1276.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"990,-1268.5 1197,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1093.5\" y=\"-1253.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980980250384&#45;&gt;139980980064040 -->\n",
       "<g class=\"edge\" id=\"edge20\">\n",
       "<title>139980980250384-&gt;139980980064040</title>\n",
       "<path d=\"M699.2129,-1328.4901C754.3078,-1317.6282 820.1773,-1304.642 876.519,-1293.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"877.4259,-1296.9229 886.5601,-1291.5547 876.0719,-1290.0551 877.4259,-1296.9229\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980979937008 -->\n",
       "<g class=\"node\" id=\"node20\">\n",
       "<title>139980979937008</title>\n",
       "<polygon fill=\"none\" points=\"1215,-1245.5 1215,-1291.5 1602,-1291.5 1602,-1245.5 1215,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1277.5\" y=\"-1264.8\">ca2: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"1340,-1245.5 1340,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1367.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1340,-1268.5 1395,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1367.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1395,-1245.5 1395,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1498.5\" y=\"-1276.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"1395,-1268.5 1602,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1498.5\" y=\"-1253.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980980250384&#45;&gt;139980979937008 -->\n",
       "<g class=\"edge\" id=\"edge22\">\n",
       "<title>139980980250384-&gt;139980979937008</title>\n",
       "<path d=\"M788.5682,-1328.986C791.9028,-1328.6525 795.2153,-1328.3236 798.5,-1328 976.3881,-1310.477 1023.7095,-1310.5124 1204.8542,-1292.1557\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1205.3964,-1295.6187 1214.9903,-1291.1236 1204.6872,-1288.6547 1205.3964,-1295.6187\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980978654064 -->\n",
       "<g class=\"node\" id=\"node21\">\n",
       "<title>139980978654064</title>\n",
       "<polygon fill=\"none\" points=\"0,-1245.5 0,-1291.5 387,-1291.5 387,-1245.5 0,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-1264.8\">ca3: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"125,-1245.5 125,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"125,-1268.5 180,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"152.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"180,-1245.5 180,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-1276.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"180,-1268.5 387,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283.5\" y=\"-1253.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980980250384&#45;&gt;139980978654064 -->\n",
       "<g class=\"edge\" id=\"edge24\">\n",
       "<title>139980980250384-&gt;139980978654064</title>\n",
       "<path d=\"M474.6584,-1328.4901C423.962,-1317.6731 363.3923,-1304.7495 311.4764,-1293.6723\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"312.0616,-1290.2185 301.5514,-1291.5547 310.6009,-1297.0644 312.0616,-1290.2185\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980977423752 -->\n",
       "<g class=\"node\" id=\"node22\">\n",
       "<title>139980977423752</title>\n",
       "<polygon fill=\"none\" points=\"405,-1245.5 405,-1291.5 792,-1291.5 792,-1245.5 405,-1245.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"467.5\" y=\"-1264.8\">ca4: CrossAttention</text>\n",
       "<polyline fill=\"none\" points=\"530,-1245.5 530,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557.5\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"530,-1268.5 585,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557.5\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"585,-1245.5 585,-1291.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"688.5\" y=\"-1276.3\">[(None, 1, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"585,-1268.5 792,-1268.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"688.5\" y=\"-1253.3\">[(None, 500, 256), (1, 500), (1,)]</text>\n",
       "</g>\n",
       "<!-- 139980980250384&#45;&gt;139980977423752 -->\n",
       "<g class=\"edge\" id=\"edge26\">\n",
       "<title>139980980250384-&gt;139980977423752</title>\n",
       "<path d=\"M586.9569,-1328.3799C588.5386,-1320.1745 590.352,-1310.7679 592.0656,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"595.5553,-1302.2658 594.0115,-1291.784 588.6818,-1300.9407 595.5553,-1302.2658\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395128&#45;&gt;139980980064040 -->\n",
       "<g class=\"edge\" id=\"edge21\">\n",
       "<title>139980981395128-&gt;139980980064040</title>\n",
       "<path d=\"M1015.0431,-1328.3799C1013.4614,-1320.1745 1011.648,-1310.7679 1009.9344,-1301.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1013.3182,-1300.9407 1007.9885,-1291.784 1006.4447,-1302.2658 1013.3182,-1300.9407\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395128&#45;&gt;139980979937008 -->\n",
       "<g class=\"edge\" id=\"edge23\">\n",
       "<title>139980981395128-&gt;139980979937008</title>\n",
       "<path d=\"M1127.3416,-1328.4901C1178.038,-1317.6731 1238.6077,-1304.7495 1290.5236,-1293.6723\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"1291.3991,-1297.0644 1300.4486,-1291.5547 1289.9384,-1290.2185 1291.3991,-1297.0644\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395128&#45;&gt;139980978654064 -->\n",
       "<g class=\"edge\" id=\"edge25\">\n",
       "<title>139980981395128-&gt;139980978654064</title>\n",
       "<path d=\"M806.8249,-1328.9162C803.6957,-1328.6071 800.5859,-1328.3015 797.5,-1328 622.1938,-1310.873 575.6096,-1310.3786 397.0043,-1292.1432\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"397.3151,-1288.6568 387.0102,-1291.1184 396.601,-1295.6203 397.3151,-1288.6568\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980981395128&#45;&gt;139980977423752 -->\n",
       "<g class=\"edge\" id=\"edge27\">\n",
       "<title>139980981395128-&gt;139980977423752</title>\n",
       "<path d=\"M902.7871,-1328.4901C847.6922,-1317.6282 781.8227,-1304.642 725.481,-1293.5343\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"725.9281,-1290.0551 715.4399,-1291.5547 724.5741,-1296.9229 725.9281,-1290.0551\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139982920030864 -->\n",
       "<g class=\"node\" id=\"node23\">\n",
       "<title>139982920030864</title>\n",
       "<polygon fill=\"none\" points=\"477.5,-1162.5 477.5,-1208.5 1123.5,-1208.5 1123.5,-1162.5 477.5,-1162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-1181.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1162.5 645.5,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"645.5,-1185.5 700.5,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"673\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1162.5 700.5,-1208.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"912\" y=\"-1193.3\">[(None, 500, 256), (None, 500, 256), (None, 500, 256), (None, 500, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"700.5,-1185.5 1123.5,-1185.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"912\" y=\"-1170.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 139980980064040&#45;&gt;139982920030864 -->\n",
       "<g class=\"edge\" id=\"edge28\">\n",
       "<title>139980980064040-&gt;139982920030864</title>\n",
       "<path d=\"M947.2228,-1245.4901C922.1419,-1235.2353 892.4302,-1223.0872 866.3257,-1212.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"867.4675,-1209.0996 856.8867,-1208.5547 864.8183,-1215.579 867.4675,-1209.0996\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980979937008&#45;&gt;139982920030864 -->\n",
       "<g class=\"edge\" id=\"edge29\">\n",
       "<title>139980979937008-&gt;139982920030864</title>\n",
       "<path d=\"M1239.9455,-1245.4901C1158.8115,-1234.4142 1061.4981,-1221.1297 979.0922,-1209.8802\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"979.5002,-1206.4035 969.1187,-1208.5187 978.5534,-1213.3392 979.5002,-1206.4035\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980978654064&#45;&gt;139982920030864 -->\n",
       "<g class=\"edge\" id=\"edge30\">\n",
       "<title>139980978654064-&gt;139982920030864</title>\n",
       "<path d=\"M361.7773,-1245.4901C442.7778,-1234.4142 539.9312,-1221.1297 622.2015,-1209.8802\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"622.725,-1213.3413 632.1586,-1208.5187 621.7766,-1206.4058 622.725,-1213.3413\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980977423752&#45;&gt;139982920030864 -->\n",
       "<g class=\"edge\" id=\"edge31\">\n",
       "<title>139980977423752-&gt;139982920030864</title>\n",
       "<path d=\"M654.5,-1245.4901C679.4573,-1235.2353 709.0226,-1223.0872 734.9986,-1212.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"736.4717,-1215.5927 744.3911,-1208.5547 733.8112,-1209.1179 736.4717,-1215.5927\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980226704 -->\n",
       "<g class=\"node\" id=\"node24\">\n",
       "<title>139980980226704</title>\n",
       "<polygon fill=\"none\" points=\"652,-1079.5 652,-1125.5 949,-1125.5 949,-1079.5 652,-1079.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-1098.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"777,-1079.5 777,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"777,-1102.5 832,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"832,-1079.5 832,-1125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-1110.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"832,-1102.5 949,-1102.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-1087.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 139982920030864&#45;&gt;139980980226704 -->\n",
       "<g class=\"edge\" id=\"edge32\">\n",
       "<title>139982920030864-&gt;139980980226704</title>\n",
       "<path d=\"M800.5,-1162.3799C800.5,-1154.1745 800.5,-1144.7679 800.5,-1135.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-1135.784 800.5,-1125.784 797.0001,-1135.784 804.0001,-1135.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980227768 -->\n",
       "<g class=\"node\" id=\"node25\">\n",
       "<title>139980980227768</title>\n",
       "<polygon fill=\"none\" points=\"584.5,-996.5 584.5,-1042.5 1016.5,-1042.5 1016.5,-996.5 584.5,-996.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-1015.8\">batch_normalization_4: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"844.5,-996.5 844.5,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"844.5,-1019.5 899.5,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"872\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"899.5,-996.5 899.5,-1042.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-1027.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"899.5,-1019.5 1016.5,-1019.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-1004.3\">(None, 500, 1024)</text>\n",
       "</g>\n",
       "<!-- 139980980226704&#45;&gt;139980980227768 -->\n",
       "<g class=\"edge\" id=\"edge33\">\n",
       "<title>139980980226704-&gt;139980980227768</title>\n",
       "<path d=\"M800.5,-1079.3799C800.5,-1071.1745 800.5,-1061.7679 800.5,-1052.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-1052.784 800.5,-1042.784 797.0001,-1052.784 804.0001,-1052.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980980227432 -->\n",
       "<g class=\"node\" id=\"node26\">\n",
       "<title>139980980227432</title>\n",
       "<polygon fill=\"none\" points=\"653,-913.5 653,-959.5 948,-959.5 948,-913.5 653,-913.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-932.8\">conv1d_4: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"776,-913.5 776,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776,-936.5 831,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"803.5\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"831,-913.5 831,-959.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-944.3\">(None, 500, 1024)</text>\n",
       "<polyline fill=\"none\" points=\"831,-936.5 948,-936.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-921.3\">(None, 250, 256)</text>\n",
       "</g>\n",
       "<!-- 139980980227768&#45;&gt;139980980227432 -->\n",
       "<g class=\"edge\" id=\"edge34\">\n",
       "<title>139980980227768-&gt;139980980227432</title>\n",
       "<path d=\"M800.5,-996.3799C800.5,-988.1745 800.5,-978.7679 800.5,-969.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-969.784 800.5,-959.784 797.0001,-969.784 804.0001,-969.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980975913784 -->\n",
       "<g class=\"node\" id=\"node27\">\n",
       "<title>139980975913784</title>\n",
       "<polygon fill=\"none\" points=\"656.5,-830.5 656.5,-876.5 944.5,-876.5 944.5,-830.5 656.5,-830.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-849.8\">conv1d_5: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-830.5 779.5,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-853.5 834.5,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-830.5 834.5,-876.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-861.3\">(None, 250, 256)</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-853.5 944.5,-853.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-838.3\">(None, 125, 256)</text>\n",
       "</g>\n",
       "<!-- 139980980227432&#45;&gt;139980975913784 -->\n",
       "<g class=\"edge\" id=\"edge35\">\n",
       "<title>139980980227432-&gt;139980975913784</title>\n",
       "<path d=\"M800.5,-913.3799C800.5,-905.1745 800.5,-895.7679 800.5,-886.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-886.784 800.5,-876.784 797.0001,-886.784 804.0001,-886.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974779808 -->\n",
       "<g class=\"node\" id=\"node28\">\n",
       "<title>139980974779808</title>\n",
       "<polygon fill=\"none\" points=\"656.5,-747.5 656.5,-793.5 944.5,-793.5 944.5,-747.5 656.5,-747.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718\" y=\"-766.8\">conv1d_6: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-747.5 779.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"779.5,-770.5 834.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"807\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-747.5 834.5,-793.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-778.3\">(None, 125, 256)</text>\n",
       "<polyline fill=\"none\" points=\"834.5,-770.5 944.5,-770.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-755.3\">(None, 63, 256)</text>\n",
       "</g>\n",
       "<!-- 139980975913784&#45;&gt;139980974779808 -->\n",
       "<g class=\"edge\" id=\"edge36\">\n",
       "<title>139980975913784-&gt;139980974779808</title>\n",
       "<path d=\"M800.5,-830.3799C800.5,-822.1745 800.5,-812.7679 800.5,-803.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-803.784 800.5,-793.784 797.0001,-803.784 804.0001,-803.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974896296 -->\n",
       "<g class=\"node\" id=\"node29\">\n",
       "<title>139980974896296</title>\n",
       "<polygon fill=\"none\" points=\"659.5,-664.5 659.5,-710.5 941.5,-710.5 941.5,-664.5 659.5,-664.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-683.8\">conv1d_7: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-664.5 782.5,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"782.5,-687.5 837.5,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"810\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-664.5 837.5,-710.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-695.3\">(None, 63, 256)</text>\n",
       "<polyline fill=\"none\" points=\"837.5,-687.5 941.5,-687.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"889.5\" y=\"-672.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974779808&#45;&gt;139980974896296 -->\n",
       "<g class=\"edge\" id=\"edge37\">\n",
       "<title>139980974779808-&gt;139980974896296</title>\n",
       "<path d=\"M800.5,-747.3799C800.5,-739.1745 800.5,-729.7679 800.5,-720.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-720.784 800.5,-710.784 797.0001,-720.784 804.0001,-720.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974623712 -->\n",
       "<g class=\"node\" id=\"node30\">\n",
       "<title>139980974623712</title>\n",
       "<polygon fill=\"none\" points=\"658.5,-581.5 658.5,-627.5 942.5,-627.5 942.5,-581.5 658.5,-581.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-600.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-581.5 783.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-604.5 838.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-581.5 838.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-612.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-604.5 942.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-589.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974896296&#45;&gt;139980974623712 -->\n",
       "<g class=\"edge\" id=\"edge38\">\n",
       "<title>139980974896296-&gt;139980974623712</title>\n",
       "<path d=\"M800.5,-664.3799C800.5,-656.1745 800.5,-646.7679 800.5,-637.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-637.784 800.5,-627.784 797.0001,-637.784 804.0001,-637.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974705128 -->\n",
       "<g class=\"node\" id=\"node31\">\n",
       "<title>139980974705128</title>\n",
       "<polygon fill=\"none\" points=\"591,-498.5 591,-544.5 1010,-544.5 1010,-498.5 591,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-517.8\">batch_normalization_5: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"851,-498.5 851,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"851,-521.5 906,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"906,-498.5 906,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-529.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"906,-521.5 1010,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-506.3\">(None, 32, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974623712&#45;&gt;139980974705128 -->\n",
       "<g class=\"edge\" id=\"edge39\">\n",
       "<title>139980974623712-&gt;139980974705128</title>\n",
       "<path d=\"M800.5,-581.3799C800.5,-573.1745 800.5,-563.7679 800.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-554.784 800.5,-544.784 797.0001,-554.784 804.0001,-554.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974621136 -->\n",
       "<g class=\"node\" id=\"node32\">\n",
       "<title>139980974621136</title>\n",
       "<polygon fill=\"none\" points=\"559.5,-415.5 559.5,-461.5 1041.5,-461.5 1041.5,-415.5 559.5,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-434.8\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"882.5,-415.5 882.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"882.5,-438.5 937.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"937.5,-415.5 937.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"989.5\" y=\"-446.3\">(None, 32, 256)</text>\n",
       "<polyline fill=\"none\" points=\"937.5,-438.5 1041.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"989.5\" y=\"-423.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974705128&#45;&gt;139980974621136 -->\n",
       "<g class=\"edge\" id=\"edge40\">\n",
       "<title>139980974705128-&gt;139980974621136</title>\n",
       "<path d=\"M800.5,-498.3799C800.5,-490.1745 800.5,-480.7679 800.5,-471.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-471.784 800.5,-461.784 797.0001,-471.784 804.0001,-471.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974299008 -->\n",
       "<g class=\"node\" id=\"node33\">\n",
       "<title>139980974299008</title>\n",
       "<polygon fill=\"none\" points=\"628.5,-332.5 628.5,-378.5 972.5,-378.5 972.5,-332.5 628.5,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-351.8\">repeat_vector_1: RepeatVector</text>\n",
       "<polyline fill=\"none\" points=\"813.5,-332.5 813.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"841\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"813.5,-355.5 868.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"841\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"868.5,-332.5 868.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920.5\" y=\"-363.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"868.5,-355.5 972.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920.5\" y=\"-340.3\">(None, 50, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974621136&#45;&gt;139980974299008 -->\n",
       "<g class=\"edge\" id=\"edge41\">\n",
       "<title>139980974621136-&gt;139980974299008</title>\n",
       "<path d=\"M800.5,-415.3799C800.5,-407.1745 800.5,-397.7679 800.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-388.784 800.5,-378.784 797.0001,-388.784 804.0001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980974297384 -->\n",
       "<g class=\"node\" id=\"node34\">\n",
       "<title>139980974297384</title>\n",
       "<polygon fill=\"none\" points=\"672,-249.5 672,-295.5 929,-295.5 929,-249.5 672,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-268.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"770,-249.5 770,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"770,-272.5 825,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"797.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"825,-249.5 825,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"877\" y=\"-280.3\">(None, 50, 256)</text>\n",
       "<polyline fill=\"none\" points=\"825,-272.5 929,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"877\" y=\"-257.3\">(None, 50, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974299008&#45;&gt;139980974297384 -->\n",
       "<g class=\"edge\" id=\"edge42\">\n",
       "<title>139980974299008-&gt;139980974297384</title>\n",
       "<path d=\"M800.5,-332.3799C800.5,-324.1745 800.5,-314.7679 800.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-305.784 800.5,-295.784 797.0001,-305.784 804.0001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980973658856 -->\n",
       "<g class=\"node\" id=\"node35\">\n",
       "<title>139980973658856</title>\n",
       "<polygon fill=\"none\" points=\"658.5,-166.5 658.5,-212.5 942.5,-212.5 942.5,-166.5 658.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-185.8\">dropout_6: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-166.5 783.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"783.5,-189.5 838.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"811\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-166.5 838.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-197.3\">(None, 50, 256)</text>\n",
       "<polyline fill=\"none\" points=\"838.5,-189.5 942.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"890.5\" y=\"-174.3\">(None, 50, 256)</text>\n",
       "</g>\n",
       "<!-- 139980974297384&#45;&gt;139980973658856 -->\n",
       "<g class=\"edge\" id=\"edge43\">\n",
       "<title>139980974297384-&gt;139980973658856</title>\n",
       "<path d=\"M800.5,-249.3799C800.5,-241.1745 800.5,-231.7679 800.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-222.784 800.5,-212.784 797.0001,-222.784 804.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980972138000 -->\n",
       "<g class=\"node\" id=\"node36\">\n",
       "<title>139980972138000</title>\n",
       "<polygon fill=\"none\" points=\"591,-83.5 591,-129.5 1010,-129.5 1010,-83.5 591,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721\" y=\"-102.8\">batch_normalization_6: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"851,-83.5 851,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"851,-106.5 906,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"878.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"906,-83.5 906,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-114.3\">(None, 50, 256)</text>\n",
       "<polyline fill=\"none\" points=\"906,-106.5 1010,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"958\" y=\"-91.3\">(None, 50, 256)</text>\n",
       "</g>\n",
       "<!-- 139980973658856&#45;&gt;139980972138000 -->\n",
       "<g class=\"edge\" id=\"edge44\">\n",
       "<title>139980973658856-&gt;139980972138000</title>\n",
       "<path d=\"M800.5,-166.3799C800.5,-158.1745 800.5,-148.7679 800.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-139.784 800.5,-129.784 797.0001,-139.784 804.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139980971223976 -->\n",
       "<g class=\"node\" id=\"node37\">\n",
       "<title>139980971223976</title>\n",
       "<polygon fill=\"none\" points=\"546,-.5 546,-46.5 1055,-46.5 1055,-.5 546,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"714.5\" y=\"-19.8\">headline_token_classes(dense_2): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"883,-.5 883,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"883,-23.5 938,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"938,-.5 938,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"996.5\" y=\"-31.3\">(None, 50, 256)</text>\n",
       "<polyline fill=\"none\" points=\"938,-23.5 1055,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"996.5\" y=\"-8.3\">(None, 50, 20000)</text>\n",
       "</g>\n",
       "<!-- 139980972138000&#45;&gt;139980971223976 -->\n",
       "<g class=\"edge\" id=\"edge45\">\n",
       "<title>139980972138000-&gt;139980971223976</title>\n",
       "<path d=\"M800.5,-83.3799C800.5,-75.1745 800.5,-65.7679 800.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"804.0001,-56.784 800.5,-46.784 797.0001,-56.784 804.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp_sentence_vectors = Input(shape=(max_sentences, 300), name='sentence_vectors')\n",
    "    inp_headline_vector = Input(shape=(300,), name='input_headline_vector')\n",
    "    conv1 = Conv1D(filters=16,kernel_size=3,strides=1,activation='relu', padding='same')(inp_sentence_vectors)\n",
    "    conv1 = Dropout(0.5)(conv1)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=3,strides=1,activation='relu', padding='same')(conv1)\n",
    "    conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    sent_sa_feat_1, sent_beta_1, sent_gamma_1 = SelfAttention(int(conv2.shape[-1]), name = 'sa1')(conv2)\n",
    "    sent_sa_feat_2, sent_beta_2, sent_gamma_2 = SelfAttention(int(conv2.shape[-1]), name = 'sa2')(conv2)\n",
    "    sent_sa_feat_3, sent_beta_3, sent_gamma_3 = SelfAttention(int(conv2.shape[-1]), name = 'sa3')(conv2)\n",
    "    sent_sa_feat_4, sent_beta_4, sent_gamma_4 = SelfAttention(int(conv2.shape[-1]), name = 'sa4')(conv2)\n",
    "    concat1 = Concatenate()([sent_sa_feat_1,sent_sa_feat_2,sent_sa_feat_3,sent_sa_feat_4])\n",
    "    conv3 = Conv1D(filters=256,kernel_size=3, strides=1, activation='relu', padding='same')(concat1)\n",
    "    conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    headline = Dense(256, activation='relu')(inp_headline_vector)\n",
    "    headline = Lambda(lambda x:K.expand_dims(x, axis=1))(headline)\n",
    "    headline = BatchNormalization()(headline)\n",
    "    sent_hd_sa_feat_1, sent_hd_beta_1, sent_hd_gamma_1 = CrossAttention(int(conv3.shape[-1]), name = 'ca1')([headline,conv3])\n",
    "    sent_hd_sa_feat_2, sent_hd_beta_2, sent_hd_gamma_2 = CrossAttention(int(conv3.shape[-1]), name = 'ca2')([headline,conv3])\n",
    "    sent_hd_sa_feat_3, sent_hd_beta_3, sent_hd_gamma_3 = CrossAttention(int(conv3.shape[-1]), name = 'ca3')([headline,conv3])\n",
    "    sent_hd_sa_feat_4, sent_hd_beta_4, sent_hd_gamma_4 = CrossAttention(int(conv3.shape[-1]), name = 'ca4')([headline,conv3])  \n",
    "    concat3 = Concatenate()([sent_hd_sa_feat_1,sent_hd_sa_feat_2,sent_hd_sa_feat_3,sent_hd_sa_feat_4])\n",
    "    concat3 = Dropout(0.5)(concat3)\n",
    "    concat3 = BatchNormalization()(concat3)\n",
    "    conv5 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(concat3)\n",
    "    conv6 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv5)\n",
    "    conv7 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv6)\n",
    "    conv8 = Conv1D(filters=256,kernel_size=3, strides=2, activation='relu', padding='same')(conv7)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    gap = GlobalAveragePooling1D()(conv8)\n",
    "    repeat = RepeatVector(50)(gap)\n",
    "    lstm1 = LSTM(256,return_sequences=True, activation='relu')(repeat)\n",
    "    lstm1 = Dropout(0.5)(lstm1)\n",
    "    lstm1 = BatchNormalization()(lstm1)\n",
    "    gen_hd_vector = TimeDistributed(Dense(20000,activation='softmax'), name='headline_token_classes')(lstm1)\n",
    "    model = Model([inp_sentence_vectors,inp_headline_vector],gen_hd_vector)\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001,beta_1=0.0,beta_2=0.99),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "# print('model params:',model.count_params())\n",
    "SVG(model_to_dot(model,show_layer_names=True,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.now()\n",
    "mc = ModelCheckpoint('weights/dnf700_sa_sent_hd_word_gl.hdf5',save_best_only=True,save_weights_only=True)\n",
    "tb = TensorBoard(batch_size=32,log_dir='logs/dnf700_sa_sent_hd_word_gl/{0}'.format(dt.timestamp()),write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 34s 9s/step - loss: 9.9134 - acc: 0.0000e+00 - val_loss: 9.7867 - val_acc: 0.0000e+00\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 2s 386ms/step - loss: 9.9081 - acc: 0.0000e+00 - val_loss: 9.7842 - val_acc: 0.0000e+00\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 2s 452ms/step - loss: 9.8995 - acc: 1.5625e-04 - val_loss: 9.7852 - val_acc: 0.0000e+00\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 2s 457ms/step - loss: 9.8916 - acc: 1.5625e-04 - val_loss: 9.7462 - val_acc: 0.0219\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8841 - acc: 7.8125e-04 - val_loss: 9.7367 - val_acc: 0.0213\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 9.8761 - acc: 0.0017 - val_loss: 9.6989 - val_acc: 0.6894\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8687 - acc: 0.0048 - val_loss: 9.7474 - val_acc: 0.6375\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8459 - acc: 0.0217 - val_loss: 9.7009 - val_acc: 0.7100\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 9.8552 - acc: 0.0252 - val_loss: 9.8486 - val_acc: 0.0400\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8203 - acc: 0.0498 - val_loss: 9.8468 - val_acc: 6.2500e-04\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8101 - acc: 0.0583 - val_loss: 9.8391 - val_acc: 0.0213\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.8120 - acc: 0.0550 - val_loss: 9.8332 - val_acc: 0.0000e+00\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.8040 - acc: 0.0613 - val_loss: 9.8177 - val_acc: 0.0425\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.7769 - acc: 0.0819 - val_loss: 9.8182 - val_acc: 0.0225\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.7574 - acc: 0.1050 - val_loss: 9.8106 - val_acc: 0.0000e+00\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.7434 - acc: 0.1100 - val_loss: 9.8047 - val_acc: 0.0000e+00\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 9.7491 - acc: 0.1319 - val_loss: 9.7230 - val_acc: 0.0000e+00\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.7248 - acc: 0.1556 - val_loss: 10.6800 - val_acc: 0.0113\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 9.7157 - acc: 0.1603 - val_loss: 9.7777 - val_acc: 0.0825\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.7096 - acc: 0.1856 - val_loss: 10.2679 - val_acc: 0.0500\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.7090 - acc: 0.1734 - val_loss: 10.1897 - val_acc: 0.3175\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.6673 - acc: 0.2283 - val_loss: 9.5078 - val_acc: 0.6712\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.6771 - acc: 0.1944 - val_loss: 9.3587 - val_acc: 0.6888\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.6320 - acc: 0.2487 - val_loss: 9.2920 - val_acc: 0.6506\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.6282 - acc: 0.2403 - val_loss: 9.1744 - val_acc: 0.6294\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.6021 - acc: 0.2516 - val_loss: 9.1501 - val_acc: 0.6263\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.5775 - acc: 0.2820 - val_loss: 8.8522 - val_acc: 0.5956\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.5862 - acc: 0.2661 - val_loss: 9.2266 - val_acc: 0.6888\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.5979 - acc: 0.2434 - val_loss: 9.1498 - val_acc: 0.6862\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.5622 - acc: 0.2839 - val_loss: 9.3342 - val_acc: 0.6806\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.5775 - acc: 0.2723 - val_loss: 9.3121 - val_acc: 0.6944\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.5036 - acc: 0.3175 - val_loss: 9.1140 - val_acc: 0.6737\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.4884 - acc: 0.2920 - val_loss: 9.0946 - val_acc: 0.6888\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.4533 - acc: 0.3359 - val_loss: 8.8109 - val_acc: 0.6938\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.4619 - acc: 0.3194 - val_loss: 8.6319 - val_acc: 0.6831\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.4731 - acc: 0.3187 - val_loss: 8.4792 - val_acc: 0.6419\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.4430 - acc: 0.3452 - val_loss: 8.4164 - val_acc: 0.6325\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.4032 - acc: 0.3659 - val_loss: 8.9745 - val_acc: 0.6513\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.4773 - acc: 0.3355 - val_loss: 8.8092 - val_acc: 0.6675\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.5694 - acc: 0.3314 - val_loss: 8.7231 - val_acc: 0.6562\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.4630 - acc: 0.3528 - val_loss: 8.4750 - val_acc: 0.6444\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.4531 - acc: 0.3336 - val_loss: 8.6576 - val_acc: 0.6594\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.4379 - acc: 0.3497 - val_loss: 8.6979 - val_acc: 0.6525\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.3742 - acc: 0.3622 - val_loss: 9.0209 - val_acc: 0.6506\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.2669 - acc: 0.3816 - val_loss: 9.1742 - val_acc: 0.6431\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.3981 - acc: 0.3716 - val_loss: 9.2070 - val_acc: 0.6469\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.4230 - acc: 0.3702 - val_loss: 9.0467 - val_acc: 0.6406\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.3546 - acc: 0.3828 - val_loss: 8.8560 - val_acc: 0.6569\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.2038 - acc: 0.3847 - val_loss: 9.0115 - val_acc: 0.6469\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.2587 - acc: 0.4100 - val_loss: 9.0247 - val_acc: 0.6488\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 9.2599 - acc: 0.4139 - val_loss: 8.9440 - val_acc: 0.6544\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.1822 - acc: 0.4028 - val_loss: 8.9982 - val_acc: 0.6431\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.1935 - acc: 0.4113 - val_loss: 8.8799 - val_acc: 0.6419\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 8s 2s/step - loss: 9.1609 - acc: 0.4267 - val_loss: 8.4753 - val_acc: 0.6394\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.1667 - acc: 0.4042 - val_loss: 8.3682 - val_acc: 0.6500\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.2447 - acc: 0.4284 - val_loss: 9.1448 - val_acc: 0.6594\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 9.0547 - acc: 0.4463 - val_loss: 8.9538 - val_acc: 0.6413\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 8.9962 - acc: 0.4520 - val_loss: 8.6771 - val_acc: 0.6525\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.0816 - acc: 0.4284 - val_loss: 8.3707 - val_acc: 0.6550\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.1787 - acc: 0.4252 - val_loss: 8.9484 - val_acc: 0.6419\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.2285 - acc: 0.4811 - val_loss: 8.9069 - val_acc: 0.6350\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.3383 - acc: 0.4695 - val_loss: 8.6322 - val_acc: 0.6413\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.3195 - acc: 0.4675 - val_loss: 8.9862 - val_acc: 0.6425\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.3084 - acc: 0.4659 - val_loss: 9.2226 - val_acc: 0.6456\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 9.2958 - acc: 0.4891 - val_loss: 9.2479 - val_acc: 0.6375\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 9.2243 - acc: 0.4948 - val_loss: 9.1131 - val_acc: 0.6506\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 9.0365 - acc: 0.4897 - val_loss: 8.6781 - val_acc: 0.6181\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 8.9545 - acc: 0.4850 - val_loss: 9.2578 - val_acc: 0.6131\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.9695 - acc: 0.4875 - val_loss: 9.4230 - val_acc: 0.6137\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 9.0956 - acc: 0.5122 - val_loss: 9.3419 - val_acc: 0.6212\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 9.0214 - acc: 0.4616 - val_loss: 9.4510 - val_acc: 0.6031\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 9.0116 - acc: 0.5205 - val_loss: 9.1656 - val_acc: 0.6438\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.8791 - acc: 0.4958 - val_loss: 9.2392 - val_acc: 0.6431\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.9875 - acc: 0.5005 - val_loss: 9.1676 - val_acc: 0.6469\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.8082 - acc: 0.5027 - val_loss: 9.3374 - val_acc: 0.6419\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.8170 - acc: 0.4933 - val_loss: 9.2790 - val_acc: 0.6288\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.7322 - acc: 0.4970 - val_loss: 9.2486 - val_acc: 0.6256\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.7082 - acc: 0.4986 - val_loss: 9.2129 - val_acc: 0.6481\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.7540 - acc: 0.5303 - val_loss: 9.1161 - val_acc: 0.6581\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.6850 - acc: 0.5294 - val_loss: 9.0920 - val_acc: 0.6494\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 9.0085 - acc: 0.5325 - val_loss: 9.1134 - val_acc: 0.6431\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.6246 - acc: 0.5023 - val_loss: 9.1480 - val_acc: 0.6463\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.6644 - acc: 0.5245 - val_loss: 9.0593 - val_acc: 0.6513\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.7946 - acc: 0.5311 - val_loss: 9.1465 - val_acc: 0.6456\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.5234 - acc: 0.5253 - val_loss: 9.0514 - val_acc: 0.6525\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.4612 - acc: 0.5163 - val_loss: 9.1729 - val_acc: 0.6256\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.7742 - acc: 0.5516 - val_loss: 9.0116 - val_acc: 0.6525\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.7443 - acc: 0.5564 - val_loss: 8.8984 - val_acc: 0.6550\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.7324 - acc: 0.5734 - val_loss: 8.9345 - val_acc: 0.6381\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.4102 - acc: 0.5302 - val_loss: 9.1110 - val_acc: 0.6350\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.3975 - acc: 0.4983 - val_loss: 9.0571 - val_acc: 0.6544\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.3793 - acc: 0.5530 - val_loss: 9.0136 - val_acc: 0.6419\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.7988 - acc: 0.5758 - val_loss: 8.2106 - val_acc: 0.6325\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.6299 - acc: 0.5673 - val_loss: 8.2642 - val_acc: 0.6494\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.2648 - acc: 0.5216 - val_loss: 8.3648 - val_acc: 0.6431\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.3172 - acc: 0.5328 - val_loss: 8.5013 - val_acc: 0.6531\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.4591 - acc: 0.5423 - val_loss: 8.6392 - val_acc: 0.6488\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.3190 - acc: 0.5623 - val_loss: 8.5822 - val_acc: 0.6569\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.1603 - acc: 0.5627 - val_loss: 8.8490 - val_acc: 0.6125\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.2067 - acc: 0.5711 - val_loss: 8.6436 - val_acc: 0.6187\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 8.1682 - acc: 0.5455 - val_loss: 8.7691 - val_acc: 0.6250\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 8.3477 - acc: 0.5916 - val_loss: 8.7189 - val_acc: 0.6119\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.1830 - acc: 0.5800 - val_loss: 8.4785 - val_acc: 0.6181\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.4001 - acc: 0.5953 - val_loss: 8.8044 - val_acc: 0.6187\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 8.0353 - acc: 0.5814 - val_loss: 8.7472 - val_acc: 0.6363\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.1777 - acc: 0.5808 - val_loss: 8.8466 - val_acc: 0.6313\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 7.9907 - acc: 0.5637 - val_loss: 8.9698 - val_acc: 0.6144\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 7.8275 - acc: 0.5727 - val_loss: 8.7676 - val_acc: 0.6425\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 8.0767 - acc: 0.5866 - val_loss: 8.7589 - val_acc: 0.6388\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.7825 - acc: 0.5523 - val_loss: 8.8272 - val_acc: 0.6319\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.8746 - acc: 0.5773 - val_loss: 8.8448 - val_acc: 0.6331\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.1224 - acc: 0.6039 - val_loss: 8.8546 - val_acc: 0.6344\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 8.1876 - acc: 0.5952 - val_loss: 9.0149 - val_acc: 0.6244\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.9051 - acc: 0.5742 - val_loss: 9.0859 - val_acc: 0.6250\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 7.7135 - acc: 0.5778 - val_loss: 9.0175 - val_acc: 0.6406\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.5853 - acc: 0.5920 - val_loss: 9.2040 - val_acc: 0.6144\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 8.1317 - acc: 0.6216 - val_loss: 9.0373 - val_acc: 0.5719\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 8.0101 - acc: 0.6131 - val_loss: 8.8462 - val_acc: 0.6350\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 8.1084 - acc: 0.6006 - val_loss: 8.7905 - val_acc: 0.6413\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 7.9998 - acc: 0.6206 - val_loss: 8.9755 - val_acc: 0.6306\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 8.0102 - acc: 0.6098 - val_loss: 8.9660 - val_acc: 0.6394\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.9868 - acc: 0.6116 - val_loss: 9.0500 - val_acc: 0.6281\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.9439 - acc: 0.6106 - val_loss: 9.0056 - val_acc: 0.6325\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.9323 - acc: 0.6048 - val_loss: 9.0583 - val_acc: 0.6369\n",
      "Epoch 125/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 7.9358 - acc: 0.5942 - val_loss: 9.0764 - val_acc: 0.6313\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.8481 - acc: 0.5827 - val_loss: 9.1406 - val_acc: 0.6300\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.8430 - acc: 0.5908 - val_loss: 9.1611 - val_acc: 0.6325\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.7365 - acc: 0.6083 - val_loss: 9.1505 - val_acc: 0.6388\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.7278 - acc: 0.5908 - val_loss: 9.1621 - val_acc: 0.6325\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.6572 - acc: 0.6070 - val_loss: 9.1436 - val_acc: 0.6294\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.6408 - acc: 0.5906 - val_loss: 9.2143 - val_acc: 0.6313\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.6414 - acc: 0.5795 - val_loss: 9.2332 - val_acc: 0.6237\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.5484 - acc: 0.5731 - val_loss: 9.0049 - val_acc: 0.6344\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.5092 - acc: 0.5991 - val_loss: 9.3536 - val_acc: 0.6181\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.4695 - acc: 0.5945 - val_loss: 9.1523 - val_acc: 0.6250\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.4496 - acc: 0.5863 - val_loss: 9.2455 - val_acc: 0.6162\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.4058 - acc: 0.5873 - val_loss: 9.1897 - val_acc: 0.6237\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.3966 - acc: 0.5900 - val_loss: 9.1406 - val_acc: 0.6225\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 7.3274 - acc: 0.5842 - val_loss: 9.2092 - val_acc: 0.6069\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.2763 - acc: 0.5970 - val_loss: 9.1787 - val_acc: 0.5938\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.2349 - acc: 0.5958 - val_loss: 9.2385 - val_acc: 0.5888\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.1871 - acc: 0.5891 - val_loss: 9.2777 - val_acc: 0.5806\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.1662 - acc: 0.6050 - val_loss: 9.3201 - val_acc: 0.5856\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.1687 - acc: 0.6012 - val_loss: 9.5676 - val_acc: 0.5619\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.1135 - acc: 0.5991 - val_loss: 9.7171 - val_acc: 0.5113\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.1079 - acc: 0.5934 - val_loss: 9.8892 - val_acc: 0.4250\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.0762 - acc: 0.6027 - val_loss: 9.8257 - val_acc: 0.4575\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 7.0348 - acc: 0.5877 - val_loss: 9.7959 - val_acc: 0.3913\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.9866 - acc: 0.6100 - val_loss: 9.5699 - val_acc: 0.5244\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.9656 - acc: 0.6133 - val_loss: 8.9347 - val_acc: 0.5819\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 6.9403 - acc: 0.6048 - val_loss: 8.9233 - val_acc: 0.5756\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.9368 - acc: 0.6089 - val_loss: 8.8061 - val_acc: 0.5300\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.8641 - acc: 0.6156 - val_loss: 8.8030 - val_acc: 0.5525\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.8035 - acc: 0.6072 - val_loss: 8.8067 - val_acc: 0.5525\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 6.7812 - acc: 0.6155 - val_loss: 8.6607 - val_acc: 0.5362\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.8053 - acc: 0.5961 - val_loss: 8.6764 - val_acc: 0.5531\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.7434 - acc: 0.6125 - val_loss: 8.7005 - val_acc: 0.5706\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.6663 - acc: 0.6177 - val_loss: 8.5367 - val_acc: 0.5544\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 6.6836 - acc: 0.6173 - val_loss: 8.6400 - val_acc: 0.5906\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.6891 - acc: 0.6169 - val_loss: 8.5015 - val_acc: 0.5437\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.6128 - acc: 0.6161 - val_loss: 8.5547 - val_acc: 0.5994\n",
      "Epoch 162/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.5929 - acc: 0.6112 - val_loss: 8.5803 - val_acc: 0.5800\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.5572 - acc: 0.6197 - val_loss: 8.6035 - val_acc: 0.5725\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.5458 - acc: 0.6194 - val_loss: 8.5385 - val_acc: 0.5788\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 6.4510 - acc: 0.6216 - val_loss: 8.6109 - val_acc: 0.5544\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.4841 - acc: 0.6195 - val_loss: 8.4352 - val_acc: 0.5856\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.4091 - acc: 0.6269 - val_loss: 8.4817 - val_acc: 0.5806\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.4179 - acc: 0.6216 - val_loss: 8.5155 - val_acc: 0.5850\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.3447 - acc: 0.6334 - val_loss: 8.4520 - val_acc: 0.6137\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.2710 - acc: 0.6272 - val_loss: 8.3729 - val_acc: 0.6050\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.2769 - acc: 0.6309 - val_loss: 8.4287 - val_acc: 0.6106\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.2656 - acc: 0.6169 - val_loss: 8.5564 - val_acc: 0.5925\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.2127 - acc: 0.6231 - val_loss: 8.4787 - val_acc: 0.6000\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 6.1870 - acc: 0.6320 - val_loss: 8.3832 - val_acc: 0.6100\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.2009 - acc: 0.6223 - val_loss: 8.4274 - val_acc: 0.5881\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.1035 - acc: 0.6353 - val_loss: 8.3760 - val_acc: 0.6056\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.1013 - acc: 0.6278 - val_loss: 8.1938 - val_acc: 0.6181\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.0160 - acc: 0.6328 - val_loss: 8.3828 - val_acc: 0.5994\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 6.0642 - acc: 0.6306 - val_loss: 8.3075 - val_acc: 0.6181\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 6.0041 - acc: 0.6394 - val_loss: 8.3209 - val_acc: 0.6131\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 5.9418 - acc: 0.6397 - val_loss: 8.1889 - val_acc: 0.6125\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.9753 - acc: 0.6412 - val_loss: 8.1650 - val_acc: 0.6237\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 5.9001 - acc: 0.6337 - val_loss: 8.2392 - val_acc: 0.6162\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.9135 - acc: 0.6314 - val_loss: 8.1931 - val_acc: 0.6206\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.9095 - acc: 0.6344 - val_loss: 10.9279 - val_acc: 0.4219\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 5.9215 - acc: 0.6436 - val_loss: 8.0268 - val_acc: 0.6294\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 5.8341 - acc: 0.6367 - val_loss: 8.1659 - val_acc: 0.6162\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.7419 - acc: 0.6392 - val_loss: 8.1171 - val_acc: 0.6244\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 5.6712 - acc: 0.6497 - val_loss: 8.0883 - val_acc: 0.6256\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.6753 - acc: 0.6378 - val_loss: 8.2390 - val_acc: 0.6125\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.5689 - acc: 0.6456 - val_loss: 8.1071 - val_acc: 0.6181\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 5.7258 - acc: 0.6338 - val_loss: 8.0252 - val_acc: 0.6237\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 5.5994 - acc: 0.6491 - val_loss: 7.9845 - val_acc: 0.6356\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 5.5701 - acc: 0.6444 - val_loss: 7.9757 - val_acc: 0.6363\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 5.5225 - acc: 0.6437 - val_loss: 8.0382 - val_acc: 0.6369\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.4785 - acc: 0.6558 - val_loss: 8.0473 - val_acc: 0.6306\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 5.4109 - acc: 0.6552 - val_loss: 7.9651 - val_acc: 0.6394\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.3378 - acc: 0.6595 - val_loss: 7.9310 - val_acc: 0.6237\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.4104 - acc: 0.6439 - val_loss: 7.9850 - val_acc: 0.6331\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.2941 - acc: 0.6614 - val_loss: 8.0979 - val_acc: 0.6231\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 5.2762 - acc: 0.6594 - val_loss: 7.9750 - val_acc: 0.6306\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.3147 - acc: 0.6508 - val_loss: 8.0799 - val_acc: 0.6263\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.2759 - acc: 0.6566 - val_loss: 8.1504 - val_acc: 0.6237\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.2233 - acc: 0.6558 - val_loss: 8.0650 - val_acc: 0.6212\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.2497 - acc: 0.6498 - val_loss: 7.8093 - val_acc: 0.6388\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.1494 - acc: 0.6584 - val_loss: 7.9201 - val_acc: 0.6331\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.1150 - acc: 0.6605 - val_loss: 7.9060 - val_acc: 0.6431\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 5.0357 - acc: 0.6708 - val_loss: 7.7707 - val_acc: 0.6500\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.9941 - acc: 0.6720 - val_loss: 7.8709 - val_acc: 0.6456\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.9977 - acc: 0.6655 - val_loss: 8.3445 - val_acc: 0.6019\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.9272 - acc: 0.6747 - val_loss: 8.0692 - val_acc: 0.6313\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.9212 - acc: 0.6733 - val_loss: 7.9330 - val_acc: 0.6500\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.9200 - acc: 0.6673 - val_loss: 7.8289 - val_acc: 0.6538\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.8374 - acc: 0.6748 - val_loss: 7.8419 - val_acc: 0.6394\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.8019 - acc: 0.6781 - val_loss: 7.9135 - val_acc: 0.6481\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.7851 - acc: 0.6716 - val_loss: 8.0999 - val_acc: 0.6331\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.7423 - acc: 0.6741 - val_loss: 8.3171 - val_acc: 0.6169\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.7289 - acc: 0.6731 - val_loss: 7.6396 - val_acc: 0.6594\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.7013 - acc: 0.6742 - val_loss: 7.7657 - val_acc: 0.6494\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.7104 - acc: 0.6717 - val_loss: 7.7324 - val_acc: 0.6612\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.6409 - acc: 0.6809 - val_loss: 7.5753 - val_acc: 0.6656\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.5867 - acc: 0.6777 - val_loss: 7.6526 - val_acc: 0.6650\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.6071 - acc: 0.6738 - val_loss: 6.9716 - val_acc: 0.6669\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 4.5967 - acc: 0.6741 - val_loss: 7.1100 - val_acc: 0.6694\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.5008 - acc: 0.6845 - val_loss: 7.0364 - val_acc: 0.6488\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.5021 - acc: 0.6838 - val_loss: 7.2491 - val_acc: 0.6587\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.4575 - acc: 0.6750 - val_loss: 7.3957 - val_acc: 0.6662\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.4271 - acc: 0.6792 - val_loss: 7.4335 - val_acc: 0.6712\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.3848 - acc: 0.6680 - val_loss: 7.6441 - val_acc: 0.6806\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.3033 - acc: 0.6878 - val_loss: 7.3269 - val_acc: 0.6762\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.3787 - acc: 0.6695 - val_loss: 7.3456 - val_acc: 0.6800\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.3417 - acc: 0.6731 - val_loss: 7.4278 - val_acc: 0.6881\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 4.2418 - acc: 0.6845 - val_loss: 7.2854 - val_acc: 0.6712\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 4.2024 - acc: 0.6870 - val_loss: 6.7743 - val_acc: 0.6806\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 4.1374 - acc: 0.6933 - val_loss: 7.2643 - val_acc: 0.6794\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.0784 - acc: 0.6984 - val_loss: 7.1083 - val_acc: 0.6856\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.1023 - acc: 0.6878 - val_loss: 6.4132 - val_acc: 0.6975\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.0968 - acc: 0.6850 - val_loss: 6.3625 - val_acc: 0.6956\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 4.0311 - acc: 0.6942 - val_loss: 6.4890 - val_acc: 0.6837\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.9769 - acc: 0.7030 - val_loss: 6.5922 - val_acc: 0.7094\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.0113 - acc: 0.6841 - val_loss: 8.4810 - val_acc: 0.5650\n",
      "Epoch 242/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 4.0147 - acc: 0.7039 - val_loss: 5.1463 - val_acc: 0.7075\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.8840 - acc: 0.7088 - val_loss: 5.5763 - val_acc: 0.6881\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.9300 - acc: 0.6933 - val_loss: 5.3958 - val_acc: 0.7019\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.8674 - acc: 0.7036 - val_loss: 5.7195 - val_acc: 0.7038\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.9570 - acc: 0.6897 - val_loss: 6.0347 - val_acc: 0.6975\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.8098 - acc: 0.7002 - val_loss: 6.0982 - val_acc: 0.6988\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.8003 - acc: 0.7030 - val_loss: 6.0987 - val_acc: 0.6925\n",
      "Epoch 249/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 3.7934 - acc: 0.6977 - val_loss: 5.9387 - val_acc: 0.7000\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.7571 - acc: 0.7033 - val_loss: 6.1660 - val_acc: 0.7044\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.7060 - acc: 0.7053 - val_loss: 6.0067 - val_acc: 0.7275\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 3.9343 - acc: 0.7000 - val_loss: 6.1091 - val_acc: 0.7100\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.6473 - acc: 0.7084 - val_loss: 5.7986 - val_acc: 0.7237\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.5966 - acc: 0.7145 - val_loss: 5.8239 - val_acc: 0.7206\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.6860 - acc: 0.7003 - val_loss: 5.8286 - val_acc: 0.7200\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.6120 - acc: 0.7027 - val_loss: 5.8479 - val_acc: 0.7150\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.5228 - acc: 0.7108 - val_loss: 5.9559 - val_acc: 0.7019\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.6240 - acc: 0.6966 - val_loss: 5.9291 - val_acc: 0.7088\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.5233 - acc: 0.7086 - val_loss: 5.9742 - val_acc: 0.7119\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.5345 - acc: 0.6952 - val_loss: 5.9541 - val_acc: 0.7156\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.5743 - acc: 0.7003 - val_loss: 6.0035 - val_acc: 0.7025\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.3567 - acc: 0.7105 - val_loss: 6.1040 - val_acc: 0.6969\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.3835 - acc: 0.7098 - val_loss: 6.0839 - val_acc: 0.7088\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.4633 - acc: 0.7041 - val_loss: 5.8666 - val_acc: 0.7169\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.3308 - acc: 0.7131 - val_loss: 5.7246 - val_acc: 0.7094\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 3.3975 - acc: 0.7045 - val_loss: 5.7868 - val_acc: 0.7225\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.2961 - acc: 0.7103 - val_loss: 5.8683 - val_acc: 0.7063\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.2496 - acc: 0.7122 - val_loss: 5.5995 - val_acc: 0.7156\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.0893 - acc: 0.7241 - val_loss: 5.7465 - val_acc: 0.7131\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.2695 - acc: 0.7109 - val_loss: 5.7800 - val_acc: 0.7287\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.1912 - acc: 0.7080 - val_loss: 5.8464 - val_acc: 0.7131\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.1064 - acc: 0.7234 - val_loss: 5.7846 - val_acc: 0.6956\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.2699 - acc: 0.7211 - val_loss: 5.4458 - val_acc: 0.6706\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.1028 - acc: 0.7222 - val_loss: 6.4759 - val_acc: 0.6069\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.0556 - acc: 0.7233 - val_loss: 5.1595 - val_acc: 0.6806\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.1304 - acc: 0.7094 - val_loss: 5.7054 - val_acc: 0.6681\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 3.0676 - acc: 0.7189 - val_loss: 5.3115 - val_acc: 0.6644\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 3.0394 - acc: 0.7166 - val_loss: 6.3950 - val_acc: 0.6100\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.0690 - acc: 0.7159 - val_loss: 5.8635 - val_acc: 0.6400\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.1087 - acc: 0.7109 - val_loss: 6.1452 - val_acc: 0.6169\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 3.0029 - acc: 0.7158 - val_loss: 6.5482 - val_acc: 0.5981\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 3.0303 - acc: 0.7139 - val_loss: 7.4217 - val_acc: 0.5269\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9611 - acc: 0.7217 - val_loss: 6.9640 - val_acc: 0.5675\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9617 - acc: 0.7172 - val_loss: 6.5523 - val_acc: 0.5900\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.0037 - acc: 0.7216 - val_loss: 6.6328 - val_acc: 0.5962\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.9254 - acc: 0.7245 - val_loss: 6.4599 - val_acc: 0.6094\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8842 - acc: 0.7198 - val_loss: 7.0691 - val_acc: 0.5562\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9118 - acc: 0.7233 - val_loss: 6.3329 - val_acc: 0.6231\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9712 - acc: 0.7177 - val_loss: 6.1745 - val_acc: 0.6244\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.9267 - acc: 0.7186 - val_loss: 5.8853 - val_acc: 0.6381\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.8362 - acc: 0.7288 - val_loss: 6.0730 - val_acc: 0.6331\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8741 - acc: 0.7228 - val_loss: 5.4347 - val_acc: 0.6725\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8590 - acc: 0.7273 - val_loss: 6.4953 - val_acc: 0.5881\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7626 - acc: 0.7337 - val_loss: 5.9094 - val_acc: 0.6319\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8013 - acc: 0.7247 - val_loss: 6.7858 - val_acc: 0.5669\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.7754 - acc: 0.7308 - val_loss: 6.6953 - val_acc: 0.5863\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8109 - acc: 0.7247 - val_loss: 6.7776 - val_acc: 0.5813\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8146 - acc: 0.7272 - val_loss: 7.0804 - val_acc: 0.5562\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7347 - acc: 0.7358 - val_loss: 8.1006 - val_acc: 0.5056\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7974 - acc: 0.7247 - val_loss: 8.0425 - val_acc: 0.4969\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7159 - acc: 0.7348 - val_loss: 8.1483 - val_acc: 0.4837\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7946 - acc: 0.7262 - val_loss: 8.4691 - val_acc: 0.4563\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.7698 - acc: 0.7263 - val_loss: 7.5769 - val_acc: 0.5194\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8121 - acc: 0.7239 - val_loss: 7.0395 - val_acc: 0.5569\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.8306 - acc: 0.7125 - val_loss: 8.1688 - val_acc: 0.4844\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7252 - acc: 0.7294 - val_loss: 7.0626 - val_acc: 0.5575\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7433 - acc: 0.7338 - val_loss: 6.7624 - val_acc: 0.5581\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8109 - acc: 0.7409 - val_loss: 5.0860 - val_acc: 0.6669\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8461 - acc: 0.7209 - val_loss: 6.8807 - val_acc: 0.5612\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.7457 - acc: 0.7263 - val_loss: 6.1765 - val_acc: 0.6062\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7996 - acc: 0.7225 - val_loss: 6.5608 - val_acc: 0.5569\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7053 - acc: 0.7350 - val_loss: 6.1216 - val_acc: 0.6081\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7144 - acc: 0.7317 - val_loss: 6.4120 - val_acc: 0.5925\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7934 - acc: 0.7392 - val_loss: 7.6603 - val_acc: 0.5000\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8835 - acc: 0.7380 - val_loss: 4.5657 - val_acc: 0.7063\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.6906 - acc: 0.7339 - val_loss: 4.6628 - val_acc: 0.7019\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.6496 - acc: 0.7331 - val_loss: 4.8587 - val_acc: 0.7019\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.7276 - acc: 0.7308 - val_loss: 4.8240 - val_acc: 0.6963\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.6257 - acc: 0.7302 - val_loss: 4.7643 - val_acc: 0.7188\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7220 - acc: 0.7253 - val_loss: 5.0041 - val_acc: 0.7131\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7226 - acc: 0.7231 - val_loss: 4.8525 - val_acc: 0.7188\n",
      "Epoch 322/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6460 - acc: 0.7327 - val_loss: 4.9066 - val_acc: 0.7075\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.6798 - acc: 0.7327 - val_loss: 4.6429 - val_acc: 0.7269\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6202 - acc: 0.7317 - val_loss: 4.7731 - val_acc: 0.7312\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.6512 - acc: 0.7269 - val_loss: 4.9181 - val_acc: 0.7063\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.6784 - acc: 0.7334 - val_loss: 4.7905 - val_acc: 0.7125\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7053 - acc: 0.7284 - val_loss: 4.6209 - val_acc: 0.7188\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6045 - acc: 0.7263 - val_loss: 4.9209 - val_acc: 0.6981\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.5390 - acc: 0.7377 - val_loss: 4.7232 - val_acc: 0.7188\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.5259 - acc: 0.7345 - val_loss: 5.1534 - val_acc: 0.7019\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6659 - acc: 0.7167 - val_loss: 4.9488 - val_acc: 0.7031\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5200 - acc: 0.7372 - val_loss: 3.9521 - val_acc: 0.7494\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 2.6396 - acc: 0.7198 - val_loss: 4.2415 - val_acc: 0.7219\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.8688 - acc: 0.7308 - val_loss: 4.5322 - val_acc: 0.7356\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 3.3092 - acc: 0.7144 - val_loss: 4.6877 - val_acc: 0.7069\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 3.0460 - acc: 0.7308 - val_loss: 4.4963 - val_acc: 0.7144\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9391 - acc: 0.7322 - val_loss: 4.2627 - val_acc: 0.7175\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 3.0278 - acc: 0.7262 - val_loss: 4.2639 - val_acc: 0.7169\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.9601 - acc: 0.7244 - val_loss: 4.1845 - val_acc: 0.7312\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8495 - acc: 0.7386 - val_loss: 4.2236 - val_acc: 0.7331\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8919 - acc: 0.7286 - val_loss: 4.1781 - val_acc: 0.7356\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8171 - acc: 0.7300 - val_loss: 4.3055 - val_acc: 0.7269\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7948 - acc: 0.7350 - val_loss: 4.2848 - val_acc: 0.7381\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.6796 - acc: 0.7431 - val_loss: 4.3075 - val_acc: 0.7294\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7913 - acc: 0.7289 - val_loss: 4.4143 - val_acc: 0.7094\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6672 - acc: 0.7433 - val_loss: 4.2864 - val_acc: 0.7188\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7018 - acc: 0.7330 - val_loss: 4.1439 - val_acc: 0.7350\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7386 - acc: 0.7319 - val_loss: 4.4866 - val_acc: 0.6994\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.6828 - acc: 0.7358 - val_loss: 4.5112 - val_acc: 0.7100\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6911 - acc: 0.7352 - val_loss: 3.7222 - val_acc: 0.7156\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6606 - acc: 0.7416 - val_loss: 3.7190 - val_acc: 0.7063\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.8699 - acc: 0.7367 - val_loss: 3.4364 - val_acc: 0.7494\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.8840 - acc: 0.7266 - val_loss: 3.7621 - val_acc: 0.7094\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7602 - acc: 0.7434 - val_loss: 3.4425 - val_acc: 0.7519\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7990 - acc: 0.7364 - val_loss: 3.6787 - val_acc: 0.7194\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7857 - acc: 0.7356 - val_loss: 3.6032 - val_acc: 0.7362\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.7551 - acc: 0.7405 - val_loss: 3.6323 - val_acc: 0.7531\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.7912 - acc: 0.7286 - val_loss: 3.7476 - val_acc: 0.7431\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6799 - acc: 0.7361 - val_loss: 3.7793 - val_acc: 0.7519\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7046 - acc: 0.7406 - val_loss: 4.0043 - val_acc: 0.7188\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.7152 - acc: 0.7252 - val_loss: 3.8603 - val_acc: 0.7369\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6096 - acc: 0.7413 - val_loss: 3.9669 - val_acc: 0.7206\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.5828 - acc: 0.7464 - val_loss: 4.0822 - val_acc: 0.7356\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 2.5599 - acc: 0.7467 - val_loss: 4.1664 - val_acc: 0.7200\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.6514 - acc: 0.7414 - val_loss: 4.0353 - val_acc: 0.7362\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.7171 - acc: 0.7280 - val_loss: 4.2625 - val_acc: 0.7163\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.5886 - acc: 0.7391 - val_loss: 4.2391 - val_acc: 0.7056\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4971 - acc: 0.7436 - val_loss: 4.0513 - val_acc: 0.7331\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5191 - acc: 0.7423 - val_loss: 4.0685 - val_acc: 0.7325\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4446 - acc: 0.7489 - val_loss: 3.9418 - val_acc: 0.7256\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4769 - acc: 0.7497 - val_loss: 4.0573 - val_acc: 0.7425\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5237 - acc: 0.7473 - val_loss: 3.9192 - val_acc: 0.7525\n",
      "Epoch 373/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 2.5471 - acc: 0.7366 - val_loss: 4.2556 - val_acc: 0.7075\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4750 - acc: 0.7508 - val_loss: 3.9012 - val_acc: 0.7362\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5074 - acc: 0.7414 - val_loss: 4.0300 - val_acc: 0.7231\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5169 - acc: 0.7409 - val_loss: 4.1305 - val_acc: 0.7231\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4673 - acc: 0.7431 - val_loss: 4.0543 - val_acc: 0.7250\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5330 - acc: 0.7384 - val_loss: 4.0414 - val_acc: 0.7312\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3865 - acc: 0.7488 - val_loss: 4.1447 - val_acc: 0.7150\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3825 - acc: 0.7481 - val_loss: 3.8940 - val_acc: 0.7425\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4332 - acc: 0.7428 - val_loss: 4.1659 - val_acc: 0.7231\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4305 - acc: 0.7353 - val_loss: 4.0253 - val_acc: 0.7194\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4366 - acc: 0.7400 - val_loss: 3.9622 - val_acc: 0.7325\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4546 - acc: 0.7344 - val_loss: 3.9279 - val_acc: 0.7369\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.2894 - acc: 0.7519 - val_loss: 3.8175 - val_acc: 0.7462\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3430 - acc: 0.7494 - val_loss: 3.8693 - val_acc: 0.7262\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3156 - acc: 0.7506 - val_loss: 3.9336 - val_acc: 0.7294\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3957 - acc: 0.7417 - val_loss: 3.8641 - val_acc: 0.7300\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3843 - acc: 0.7414 - val_loss: 3.3579 - val_acc: 0.7281\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3069 - acc: 0.7498 - val_loss: 3.4639 - val_acc: 0.7256\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3237 - acc: 0.7480 - val_loss: 3.4826 - val_acc: 0.7200\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2790 - acc: 0.7519 - val_loss: 3.4501 - val_acc: 0.7356\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3111 - acc: 0.7486 - val_loss: 3.5382 - val_acc: 0.7344\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3804 - acc: 0.7375 - val_loss: 3.5906 - val_acc: 0.7419\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.4119 - acc: 0.7355 - val_loss: 3.9856 - val_acc: 0.7094\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2397 - acc: 0.7595 - val_loss: 3.5724 - val_acc: 0.7425\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3228 - acc: 0.7425 - val_loss: 3.9270 - val_acc: 0.7113\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2846 - acc: 0.7463 - val_loss: 3.9681 - val_acc: 0.7119\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.4177 - acc: 0.7316 - val_loss: 3.4644 - val_acc: 0.7569\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2076 - acc: 0.7589 - val_loss: 3.7685 - val_acc: 0.7219\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1771 - acc: 0.7539 - val_loss: 3.8035 - val_acc: 0.7294\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3783 - acc: 0.7323 - val_loss: 3.5917 - val_acc: 0.7387\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2392 - acc: 0.7494 - val_loss: 3.5571 - val_acc: 0.7450\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3415 - acc: 0.7387 - val_loss: 3.5786 - val_acc: 0.7331\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3236 - acc: 0.7362 - val_loss: 3.7397 - val_acc: 0.7200\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.2742 - acc: 0.7484 - val_loss: 3.7521 - val_acc: 0.7175\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1995 - acc: 0.7539 - val_loss: 3.3447 - val_acc: 0.7281\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1704 - acc: 0.7537 - val_loss: 3.6372 - val_acc: 0.7063\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1560 - acc: 0.7602 - val_loss: 3.1660 - val_acc: 0.7538\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.2449 - acc: 0.7470 - val_loss: 3.2904 - val_acc: 0.7337\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.3628 - acc: 0.7344 - val_loss: 3.5163 - val_acc: 0.7419\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2613 - acc: 0.7450 - val_loss: 3.5842 - val_acc: 0.7412\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2276 - acc: 0.7458 - val_loss: 3.6868 - val_acc: 0.7262\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2367 - acc: 0.7481 - val_loss: 3.4899 - val_acc: 0.7487\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3013 - acc: 0.7378 - val_loss: 3.5638 - val_acc: 0.7600\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1937 - acc: 0.7481 - val_loss: 3.8267 - val_acc: 0.7250\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2175 - acc: 0.7403 - val_loss: 3.6059 - val_acc: 0.7475\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3602 - acc: 0.7272 - val_loss: 3.6254 - val_acc: 0.7337\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2246 - acc: 0.7470 - val_loss: 3.2788 - val_acc: 0.7337\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1895 - acc: 0.7469 - val_loss: 3.3274 - val_acc: 0.7306\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1524 - acc: 0.7545 - val_loss: 3.3480 - val_acc: 0.7356\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1761 - acc: 0.7462 - val_loss: 3.2754 - val_acc: 0.7369\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1884 - acc: 0.7442 - val_loss: 3.4313 - val_acc: 0.7344\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1615 - acc: 0.7469 - val_loss: 3.2599 - val_acc: 0.7538\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1512 - acc: 0.7411 - val_loss: 3.5048 - val_acc: 0.7269\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9388 - acc: 0.7786 - val_loss: 3.3271 - val_acc: 0.7394\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1984 - acc: 0.7438 - val_loss: 3.2419 - val_acc: 0.7550\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1795 - acc: 0.7438 - val_loss: 3.5920 - val_acc: 0.7362\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1898 - acc: 0.7464 - val_loss: 3.5908 - val_acc: 0.7406\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1376 - acc: 0.7462 - val_loss: 3.6453 - val_acc: 0.7337\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2536 - acc: 0.7327 - val_loss: 3.8164 - val_acc: 0.7044\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1832 - acc: 0.7411 - val_loss: 3.6129 - val_acc: 0.7244\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1629 - acc: 0.7461 - val_loss: 3.4984 - val_acc: 0.7394\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1299 - acc: 0.7400 - val_loss: 3.5742 - val_acc: 0.7312\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.5042 - acc: 0.7283 - val_loss: 3.5188 - val_acc: 0.7487\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.5509 - acc: 0.7380 - val_loss: 3.8635 - val_acc: 0.7519\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.2640 - acc: 0.7502 - val_loss: 3.7418 - val_acc: 0.7513\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1671 - acc: 0.7600 - val_loss: 3.7319 - val_acc: 0.7431\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1700 - acc: 0.7569 - val_loss: 3.5569 - val_acc: 0.7500\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1516 - acc: 0.7505 - val_loss: 3.6301 - val_acc: 0.7544\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1783 - acc: 0.7481 - val_loss: 3.7898 - val_acc: 0.7231\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1612 - acc: 0.7481 - val_loss: 3.6455 - val_acc: 0.7312\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2545 - acc: 0.7433 - val_loss: 3.6735 - val_acc: 0.7294\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1672 - acc: 0.7488 - val_loss: 3.5943 - val_acc: 0.7550\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0728 - acc: 0.7570 - val_loss: 3.6878 - val_acc: 0.7275\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0824 - acc: 0.7559 - val_loss: 3.5783 - val_acc: 0.7387\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2235 - acc: 0.7352 - val_loss: 3.5863 - val_acc: 0.7312\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2103 - acc: 0.7442 - val_loss: 3.3816 - val_acc: 0.7481\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0207 - acc: 0.7627 - val_loss: 3.3910 - val_acc: 0.7525\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1257 - acc: 0.7442 - val_loss: 3.5994 - val_acc: 0.7200\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0997 - acc: 0.7508 - val_loss: 3.4471 - val_acc: 0.7350\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0464 - acc: 0.7525 - val_loss: 3.2863 - val_acc: 0.7600\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1523 - acc: 0.7381 - val_loss: 3.3900 - val_acc: 0.7412\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0753 - acc: 0.7509 - val_loss: 3.3456 - val_acc: 0.7444\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.0183 - acc: 0.7539 - val_loss: 3.5626 - val_acc: 0.7194\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0918 - acc: 0.7505 - val_loss: 3.4138 - val_acc: 0.7531\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1140 - acc: 0.7444 - val_loss: 3.4476 - val_acc: 0.7394\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0968 - acc: 0.7475 - val_loss: 3.5960 - val_acc: 0.7138\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0105 - acc: 0.7508 - val_loss: 3.3892 - val_acc: 0.7475\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0825 - acc: 0.7462 - val_loss: 3.3919 - val_acc: 0.7437\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9835 - acc: 0.7592 - val_loss: 3.3447 - val_acc: 0.7387\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.0788 - acc: 0.7475 - val_loss: 3.2653 - val_acc: 0.7538\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1594 - acc: 0.7369 - val_loss: 3.4435 - val_acc: 0.7475\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9822 - acc: 0.7586 - val_loss: 3.4017 - val_acc: 0.7431\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0186 - acc: 0.7559 - val_loss: 3.3840 - val_acc: 0.7487\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9946 - acc: 0.7536 - val_loss: 3.3334 - val_acc: 0.7563\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9870 - acc: 0.7567 - val_loss: 3.5371 - val_acc: 0.7300\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0023 - acc: 0.7534 - val_loss: 3.3477 - val_acc: 0.7462\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9757 - acc: 0.7539 - val_loss: 3.4246 - val_acc: 0.7456\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9177 - acc: 0.7600 - val_loss: 3.2925 - val_acc: 0.7481\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0168 - acc: 0.7450 - val_loss: 3.3817 - val_acc: 0.7375\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0890 - acc: 0.7455 - val_loss: 3.3543 - val_acc: 0.7375\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0480 - acc: 0.7491 - val_loss: 3.3068 - val_acc: 0.7394\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0670 - acc: 0.7397 - val_loss: 3.1918 - val_acc: 0.7538\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9871 - acc: 0.7486 - val_loss: 3.1882 - val_acc: 0.7525\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0803 - acc: 0.7411 - val_loss: 3.3179 - val_acc: 0.7425\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9748 - acc: 0.7447 - val_loss: 2.5275 - val_acc: 0.7394\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0770 - acc: 0.7380 - val_loss: 2.7330 - val_acc: 0.7287\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0139 - acc: 0.7469 - val_loss: 2.3674 - val_acc: 0.7619\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0393 - acc: 0.7394 - val_loss: 2.6571 - val_acc: 0.7344\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9187 - acc: 0.7500 - val_loss: 2.4501 - val_acc: 0.7575\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0819 - acc: 0.7345 - val_loss: 2.4796 - val_acc: 0.7513\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0830 - acc: 0.7353 - val_loss: 2.3523 - val_acc: 0.7588\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8943 - acc: 0.7609 - val_loss: 2.5756 - val_acc: 0.7331\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9326 - acc: 0.7527 - val_loss: 2.5099 - val_acc: 0.7606\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1268 - acc: 0.7333 - val_loss: 2.1441 - val_acc: 0.7800\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8954 - acc: 0.7536 - val_loss: 2.5345 - val_acc: 0.7419\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0179 - acc: 0.7372 - val_loss: 2.4285 - val_acc: 0.7506\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9642 - acc: 0.7467 - val_loss: 2.4452 - val_acc: 0.7487\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0071 - acc: 0.7430 - val_loss: 2.5037 - val_acc: 0.7406\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9825 - acc: 0.7434 - val_loss: 2.2902 - val_acc: 0.7650\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8967 - acc: 0.7498 - val_loss: 2.4580 - val_acc: 0.7406\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0488 - acc: 0.7387 - val_loss: 2.6038 - val_acc: 0.7406\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8950 - acc: 0.7559 - val_loss: 2.3349 - val_acc: 0.7625\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0018 - acc: 0.7427 - val_loss: 2.3817 - val_acc: 0.7575\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0068 - acc: 0.7403 - val_loss: 2.5674 - val_acc: 0.7362\n",
      "Epoch 497/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 2.0422 - acc: 0.7402 - val_loss: 2.2245 - val_acc: 0.7738\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8579 - acc: 0.7623 - val_loss: 2.5736 - val_acc: 0.7344\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8656 - acc: 0.7534 - val_loss: 2.3135 - val_acc: 0.7581\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9170 - acc: 0.7520 - val_loss: 2.3004 - val_acc: 0.7650\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7843 - acc: 0.7653 - val_loss: 2.4774 - val_acc: 0.7375\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8532 - acc: 0.7566 - val_loss: 2.3278 - val_acc: 0.7556\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7909 - acc: 0.7600 - val_loss: 2.4781 - val_acc: 0.7494\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8178 - acc: 0.7608 - val_loss: 2.4626 - val_acc: 0.7456\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9037 - acc: 0.7492 - val_loss: 2.1359 - val_acc: 0.7844\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8982 - acc: 0.7527 - val_loss: 2.4705 - val_acc: 0.7519\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0212 - acc: 0.7422 - val_loss: 2.3015 - val_acc: 0.7663\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8741 - acc: 0.7522 - val_loss: 2.3239 - val_acc: 0.7581\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9129 - acc: 0.7475 - val_loss: 2.2688 - val_acc: 0.7669\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7581 - acc: 0.7680 - val_loss: 2.5698 - val_acc: 0.7387\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8823 - acc: 0.7534 - val_loss: 2.4245 - val_acc: 0.7494\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8391 - acc: 0.7572 - val_loss: 2.5497 - val_acc: 0.7531\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0037 - acc: 0.7397 - val_loss: 2.7424 - val_acc: 0.7319\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8885 - acc: 0.7584 - val_loss: 2.5885 - val_acc: 0.7369\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.9088 - acc: 0.7502 - val_loss: 2.2421 - val_acc: 0.7650\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8563 - acc: 0.7550 - val_loss: 2.3558 - val_acc: 0.7588\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8962 - acc: 0.7477 - val_loss: 2.4651 - val_acc: 0.7475\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8980 - acc: 0.7519 - val_loss: 2.6431 - val_acc: 0.7387\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9020 - acc: 0.7469 - val_loss: 2.4956 - val_acc: 0.7538\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8501 - acc: 0.7558 - val_loss: 2.6228 - val_acc: 0.7431\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9949 - acc: 0.7342 - val_loss: 2.5906 - val_acc: 0.7394\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8871 - acc: 0.7452 - val_loss: 2.4992 - val_acc: 0.7462\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8233 - acc: 0.7583 - val_loss: 2.7579 - val_acc: 0.7262\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8525 - acc: 0.7491 - val_loss: 2.5372 - val_acc: 0.7487\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9221 - acc: 0.7466 - val_loss: 2.4230 - val_acc: 0.7619\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7621 - acc: 0.7631 - val_loss: 2.8007 - val_acc: 0.7144\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7880 - acc: 0.7566 - val_loss: 2.5683 - val_acc: 0.7456\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8587 - acc: 0.7523 - val_loss: 2.5316 - val_acc: 0.7525\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7690 - acc: 0.7592 - val_loss: 2.3892 - val_acc: 0.7588\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8497 - acc: 0.7592 - val_loss: 2.7566 - val_acc: 0.7275\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9237 - acc: 0.7444 - val_loss: 2.4757 - val_acc: 0.7556\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8874 - acc: 0.7434 - val_loss: 2.6613 - val_acc: 0.7344\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9111 - acc: 0.7467 - val_loss: 2.5899 - val_acc: 0.7394\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8280 - acc: 0.7539 - val_loss: 2.5772 - val_acc: 0.7487\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9095 - acc: 0.7450 - val_loss: 2.2568 - val_acc: 0.7763\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8640 - acc: 0.7502 - val_loss: 2.4194 - val_acc: 0.7588\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8385 - acc: 0.7562 - val_loss: 2.8254 - val_acc: 0.7281\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8526 - acc: 0.7498 - val_loss: 2.5257 - val_acc: 0.7456\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0593 - acc: 0.7250 - val_loss: 2.1648 - val_acc: 0.7775\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8346 - acc: 0.7541 - val_loss: 2.9259 - val_acc: 0.7131\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0047 - acc: 0.7333 - val_loss: 2.5585 - val_acc: 0.7444\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8591 - acc: 0.7456 - val_loss: 2.7426 - val_acc: 0.7244\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8908 - acc: 0.7450 - val_loss: 2.7636 - val_acc: 0.7312\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9347 - acc: 0.7412 - val_loss: 2.7171 - val_acc: 0.7362\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8288 - acc: 0.7505 - val_loss: 2.6558 - val_acc: 0.7406\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8759 - acc: 0.7441 - val_loss: 2.2891 - val_acc: 0.7700\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8818 - acc: 0.7434 - val_loss: 2.7635 - val_acc: 0.7225\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8171 - acc: 0.7570 - val_loss: 2.3196 - val_acc: 0.7606\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0665 - acc: 0.7236 - val_loss: 2.7456 - val_acc: 0.7312\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7667 - acc: 0.7597 - val_loss: 2.5841 - val_acc: 0.7406\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8184 - acc: 0.7503 - val_loss: 2.7084 - val_acc: 0.7437\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8019 - acc: 0.7537 - val_loss: 2.7134 - val_acc: 0.7331\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8143 - acc: 0.7536 - val_loss: 2.5858 - val_acc: 0.7494\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8657 - acc: 0.7455 - val_loss: 2.7045 - val_acc: 0.7437\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8756 - acc: 0.7498 - val_loss: 2.6486 - val_acc: 0.7325\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7850 - acc: 0.7595 - val_loss: 2.2855 - val_acc: 0.7675\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8217 - acc: 0.7509 - val_loss: 2.2918 - val_acc: 0.7688\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8176 - acc: 0.7498 - val_loss: 2.6517 - val_acc: 0.7375\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8071 - acc: 0.7539 - val_loss: 2.9071 - val_acc: 0.7256\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7506 - acc: 0.7591 - val_loss: 2.3261 - val_acc: 0.7669\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8247 - acc: 0.7523 - val_loss: 2.3898 - val_acc: 0.7594\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8470 - acc: 0.7511 - val_loss: 2.6935 - val_acc: 0.7419\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7888 - acc: 0.7561 - val_loss: 2.8304 - val_acc: 0.7287\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7994 - acc: 0.7537 - val_loss: 2.4648 - val_acc: 0.7575\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8129 - acc: 0.7552 - val_loss: 2.5602 - val_acc: 0.7513\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8812 - acc: 0.7419 - val_loss: 2.3812 - val_acc: 0.7613\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.9362 - acc: 0.7448 - val_loss: 2.5325 - val_acc: 0.7469\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9048 - acc: 0.7447 - val_loss: 2.4755 - val_acc: 0.7563\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8034 - acc: 0.7516 - val_loss: 2.4011 - val_acc: 0.7781\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8275 - acc: 0.7484 - val_loss: 2.5306 - val_acc: 0.7444\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7889 - acc: 0.7547 - val_loss: 2.4890 - val_acc: 0.7419\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7524 - acc: 0.7606 - val_loss: 2.6111 - val_acc: 0.7412\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9434 - acc: 0.7366 - val_loss: 2.4580 - val_acc: 0.7494\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8284 - acc: 0.7508 - val_loss: 2.5573 - val_acc: 0.7387\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8427 - acc: 0.7480 - val_loss: 2.4963 - val_acc: 0.7525\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7742 - acc: 0.7573 - val_loss: 2.5067 - val_acc: 0.7487\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8225 - acc: 0.7519 - val_loss: 2.7508 - val_acc: 0.7312\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8311 - acc: 0.7511 - val_loss: 2.4279 - val_acc: 0.7531\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8364 - acc: 0.7500 - val_loss: 2.4502 - val_acc: 0.7475\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9943 - acc: 0.7338 - val_loss: 2.4586 - val_acc: 0.7419\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7379 - acc: 0.7619 - val_loss: 2.3459 - val_acc: 0.7475\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7481 - acc: 0.7633 - val_loss: 2.4238 - val_acc: 0.7394\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8633 - acc: 0.7419 - val_loss: 2.0147 - val_acc: 0.7819\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8269 - acc: 0.7492 - val_loss: 2.3351 - val_acc: 0.7525\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8114 - acc: 0.7513 - val_loss: 2.3648 - val_acc: 0.7475\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8025 - acc: 0.7564 - val_loss: 2.5923 - val_acc: 0.7344\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8117 - acc: 0.7547 - val_loss: 2.7007 - val_acc: 0.7269\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0114 - acc: 0.7344 - val_loss: 2.6370 - val_acc: 0.7281\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8665 - acc: 0.7425 - val_loss: 2.4343 - val_acc: 0.7412\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9845 - acc: 0.7300 - val_loss: 2.8262 - val_acc: 0.7194\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7839 - acc: 0.7538 - val_loss: 2.5925 - val_acc: 0.7419\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7507 - acc: 0.7562 - val_loss: 2.2918 - val_acc: 0.7625\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8045 - acc: 0.7520 - val_loss: 2.8178 - val_acc: 0.7287\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7134 - acc: 0.7666 - val_loss: 2.6720 - val_acc: 0.7344\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8657 - acc: 0.7470 - val_loss: 2.5598 - val_acc: 0.7431\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8107 - acc: 0.7523 - val_loss: 2.4111 - val_acc: 0.7525\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8201 - acc: 0.7500 - val_loss: 2.5239 - val_acc: 0.7469\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7326 - acc: 0.7638 - val_loss: 2.6243 - val_acc: 0.7381\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6800 - acc: 0.7650 - val_loss: 2.2989 - val_acc: 0.7638\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7993 - acc: 0.7527 - val_loss: 2.4726 - val_acc: 0.7550\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7697 - acc: 0.7548 - val_loss: 2.4969 - val_acc: 0.7462\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7412 - acc: 0.7598 - val_loss: 2.4777 - val_acc: 0.7619\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7521 - acc: 0.7552 - val_loss: 2.4063 - val_acc: 0.7606\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7661 - acc: 0.7527 - val_loss: 2.4044 - val_acc: 0.7569\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7092 - acc: 0.7595 - val_loss: 2.4444 - val_acc: 0.7531\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7106 - acc: 0.7647 - val_loss: 2.7125 - val_acc: 0.7237\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8455 - acc: 0.7447 - val_loss: 2.5974 - val_acc: 0.7406\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7918 - acc: 0.7509 - val_loss: 3.0080 - val_acc: 0.7125\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8315 - acc: 0.7447 - val_loss: 2.6399 - val_acc: 0.7362\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8042 - acc: 0.7467 - val_loss: 2.7040 - val_acc: 0.7375\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8276 - acc: 0.7536 - val_loss: 2.6169 - val_acc: 0.7387\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7467 - acc: 0.7564 - val_loss: 2.6104 - val_acc: 0.7437\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8343 - acc: 0.7427 - val_loss: 2.5200 - val_acc: 0.7387\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7414 - acc: 0.7620 - val_loss: 2.7882 - val_acc: 0.7194\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7456 - acc: 0.7573 - val_loss: 2.5639 - val_acc: 0.7444\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8779 - acc: 0.7448 - val_loss: 2.5518 - val_acc: 0.7400\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7831 - acc: 0.7544 - val_loss: 2.4227 - val_acc: 0.7606\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7396 - acc: 0.7566 - val_loss: 2.6588 - val_acc: 0.7369\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8668 - acc: 0.7411 - val_loss: 2.3818 - val_acc: 0.7525\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7810 - acc: 0.7513 - val_loss: 2.4864 - val_acc: 0.7475\n",
      "Epoch 621/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 1.8241 - acc: 0.7447 - val_loss: 2.7155 - val_acc: 0.7344\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7318 - acc: 0.7612 - val_loss: 2.2497 - val_acc: 0.7700\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8574 - acc: 0.7439 - val_loss: 2.6109 - val_acc: 0.7387\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8060 - acc: 0.7503 - val_loss: 2.7199 - val_acc: 0.7237\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7005 - acc: 0.7628 - val_loss: 2.2867 - val_acc: 0.7681\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8557 - acc: 0.7495 - val_loss: 2.6777 - val_acc: 0.7325\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8380 - acc: 0.7423 - val_loss: 2.0829 - val_acc: 0.7825\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7810 - acc: 0.7511 - val_loss: 2.5990 - val_acc: 0.7369\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7672 - acc: 0.7558 - val_loss: 2.3500 - val_acc: 0.7650\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7947 - acc: 0.7525 - val_loss: 2.8061 - val_acc: 0.7237\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8619 - acc: 0.7475 - val_loss: 2.6912 - val_acc: 0.7319\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8051 - acc: 0.7478 - val_loss: 2.4499 - val_acc: 0.7606\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8340 - acc: 0.7467 - val_loss: 2.6061 - val_acc: 0.7469\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7852 - acc: 0.7505 - val_loss: 2.7101 - val_acc: 0.7344\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7938 - acc: 0.7506 - val_loss: 2.1395 - val_acc: 0.7738\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8087 - acc: 0.7441 - val_loss: 2.6254 - val_acc: 0.7344\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9055 - acc: 0.7347 - val_loss: 2.5739 - val_acc: 0.7369\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8620 - acc: 0.7386 - val_loss: 2.3267 - val_acc: 0.7663\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7015 - acc: 0.7614 - val_loss: 2.6320 - val_acc: 0.7337\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8215 - acc: 0.7509 - val_loss: 2.4708 - val_acc: 0.7387\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8404 - acc: 0.7416 - val_loss: 2.4342 - val_acc: 0.7556\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7398 - acc: 0.7564 - val_loss: 2.5656 - val_acc: 0.7406\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6852 - acc: 0.7636 - val_loss: 2.4815 - val_acc: 0.7462\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7451 - acc: 0.7587 - val_loss: 2.6586 - val_acc: 0.7281\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7651 - acc: 0.7525 - val_loss: 2.4165 - val_acc: 0.7531\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7513 - acc: 0.7597 - val_loss: 2.5940 - val_acc: 0.7294\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8189 - acc: 0.7458 - val_loss: 2.1280 - val_acc: 0.7825\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8368 - acc: 0.7469 - val_loss: 2.4938 - val_acc: 0.7500\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7821 - acc: 0.7509 - val_loss: 2.3924 - val_acc: 0.7569\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8130 - acc: 0.7467 - val_loss: 2.2837 - val_acc: 0.7594\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7163 - acc: 0.7594 - val_loss: 2.5007 - val_acc: 0.7506\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7987 - acc: 0.7461 - val_loss: 2.7841 - val_acc: 0.7175\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6642 - acc: 0.7694 - val_loss: 2.4469 - val_acc: 0.7456\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7155 - acc: 0.7555 - val_loss: 2.3142 - val_acc: 0.7600\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8222 - acc: 0.7511 - val_loss: 2.5103 - val_acc: 0.7500\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8151 - acc: 0.7533 - val_loss: 2.4426 - val_acc: 0.7563\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8516 - acc: 0.7459 - val_loss: 2.4746 - val_acc: 0.7531\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9145 - acc: 0.7367 - val_loss: 2.4459 - val_acc: 0.7588\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8521 - acc: 0.7456 - val_loss: 2.5105 - val_acc: 0.7400\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7732 - acc: 0.7528 - val_loss: 2.4876 - val_acc: 0.7456\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8374 - acc: 0.7434 - val_loss: 2.4962 - val_acc: 0.7412\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7420 - acc: 0.7575 - val_loss: 2.6492 - val_acc: 0.7362\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7856 - acc: 0.7491 - val_loss: 2.3513 - val_acc: 0.7575\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7285 - acc: 0.7562 - val_loss: 2.4324 - val_acc: 0.7556\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6827 - acc: 0.7644 - val_loss: 2.4385 - val_acc: 0.7494\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6999 - acc: 0.7622 - val_loss: 2.4094 - val_acc: 0.7531\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8158 - acc: 0.7452 - val_loss: 2.4365 - val_acc: 0.7437\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7107 - acc: 0.7648 - val_loss: 2.4137 - val_acc: 0.7487\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6683 - acc: 0.7694 - val_loss: 2.4226 - val_acc: 0.7519\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8031 - acc: 0.7498 - val_loss: 2.1753 - val_acc: 0.7800\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7581 - acc: 0.7550 - val_loss: 2.5857 - val_acc: 0.7400\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7105 - acc: 0.7627 - val_loss: 2.6347 - val_acc: 0.7337\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6785 - acc: 0.7664 - val_loss: 2.6447 - val_acc: 0.7344\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6964 - acc: 0.7589 - val_loss: 2.3318 - val_acc: 0.7575\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8355 - acc: 0.7391 - val_loss: 2.3700 - val_acc: 0.7588\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7303 - acc: 0.7516 - val_loss: 2.4420 - val_acc: 0.7525\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8238 - acc: 0.7441 - val_loss: 2.4850 - val_acc: 0.7300\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8002 - acc: 0.7533 - val_loss: 2.2869 - val_acc: 0.7556\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7618 - acc: 0.7498 - val_loss: 2.7457 - val_acc: 0.7262\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7203 - acc: 0.7598 - val_loss: 2.2814 - val_acc: 0.7588\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1301 - acc: 0.7555 - val_loss: 2.4684 - val_acc: 0.7569\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0847 - acc: 0.7731 - val_loss: 2.4511 - val_acc: 0.7500\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8835 - acc: 0.7723 - val_loss: 2.7757 - val_acc: 0.7225\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9533 - acc: 0.7606 - val_loss: 2.7179 - val_acc: 0.7362\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9311 - acc: 0.7597 - val_loss: 2.5854 - val_acc: 0.7425\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1791 - acc: 0.7331 - val_loss: 2.1370 - val_acc: 0.7750\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9625 - acc: 0.7592 - val_loss: 2.4979 - val_acc: 0.7469\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1634 - acc: 0.7353 - val_loss: 2.4056 - val_acc: 0.7481\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0625 - acc: 0.7442 - val_loss: 2.5546 - val_acc: 0.7437\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0657 - acc: 0.7414 - val_loss: 2.6234 - val_acc: 0.7375\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.9605 - acc: 0.7559 - val_loss: 2.8077 - val_acc: 0.7212\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9582 - acc: 0.7547 - val_loss: 2.8031 - val_acc: 0.7150\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8917 - acc: 0.7611 - val_loss: 2.1033 - val_acc: 0.7819\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8708 - acc: 0.7628 - val_loss: 2.7017 - val_acc: 0.7287\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9662 - acc: 0.7509 - val_loss: 2.4996 - val_acc: 0.7475\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.9332 - acc: 0.7564 - val_loss: 2.3070 - val_acc: 0.7613\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9789 - acc: 0.7466 - val_loss: 2.3381 - val_acc: 0.7594\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9431 - acc: 0.7533 - val_loss: 2.5057 - val_acc: 0.7456\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9199 - acc: 0.7545 - val_loss: 2.3782 - val_acc: 0.7544\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8262 - acc: 0.7636 - val_loss: 2.5339 - val_acc: 0.7437\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0050 - acc: 0.7486 - val_loss: 2.2648 - val_acc: 0.7606\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0891 - acc: 0.7450 - val_loss: 2.4124 - val_acc: 0.7469\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9762 - acc: 0.7495 - val_loss: 2.2496 - val_acc: 0.7631\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9095 - acc: 0.7566 - val_loss: 2.4228 - val_acc: 0.7500\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8816 - acc: 0.7619 - val_loss: 2.2278 - val_acc: 0.7694\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9144 - acc: 0.7544 - val_loss: 2.1587 - val_acc: 0.7744\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0715 - acc: 0.7391 - val_loss: 2.2519 - val_acc: 0.7581\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9551 - acc: 0.7519 - val_loss: 2.5220 - val_acc: 0.7431\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0130 - acc: 0.7463 - val_loss: 2.5887 - val_acc: 0.7375\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8654 - acc: 0.7597 - val_loss: 2.3465 - val_acc: 0.7556\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1508 - acc: 0.7284 - val_loss: 2.4538 - val_acc: 0.7469\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0517 - acc: 0.7391 - val_loss: 2.1556 - val_acc: 0.7719\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9808 - acc: 0.7461 - val_loss: 2.4066 - val_acc: 0.7506\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8713 - acc: 0.7583 - val_loss: 2.3401 - val_acc: 0.7506\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8864 - acc: 0.7595 - val_loss: 2.4793 - val_acc: 0.7394\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8268 - acc: 0.7653 - val_loss: 2.0461 - val_acc: 0.7819\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9468 - acc: 0.7494 - val_loss: 2.7177 - val_acc: 0.7144\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8982 - acc: 0.7583 - val_loss: 2.1047 - val_acc: 0.7713\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8840 - acc: 0.7608 - val_loss: 2.4436 - val_acc: 0.7506\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9098 - acc: 0.7553 - val_loss: 2.3665 - val_acc: 0.7462\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9452 - acc: 0.7525 - val_loss: 2.3963 - val_acc: 0.7550\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9275 - acc: 0.7513 - val_loss: 2.7155 - val_acc: 0.7188\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9058 - acc: 0.7538 - val_loss: 2.0938 - val_acc: 0.7788\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8960 - acc: 0.7533 - val_loss: 2.6828 - val_acc: 0.7287\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7661 - acc: 0.7652 - val_loss: 2.4976 - val_acc: 0.7412\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8116 - acc: 0.7625 - val_loss: 2.5544 - val_acc: 0.7344\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8390 - acc: 0.7603 - val_loss: 2.3712 - val_acc: 0.7475\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8921 - acc: 0.7544 - val_loss: 2.2703 - val_acc: 0.7513\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8818 - acc: 0.7573 - val_loss: 2.1756 - val_acc: 0.7713\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9366 - acc: 0.7509 - val_loss: 2.2247 - val_acc: 0.7619\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9653 - acc: 0.7492 - val_loss: 2.3276 - val_acc: 0.7519\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9878 - acc: 0.7450 - val_loss: 2.5788 - val_acc: 0.7412\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8437 - acc: 0.7630 - val_loss: 2.5190 - val_acc: 0.7350\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8518 - acc: 0.7617 - val_loss: 2.7453 - val_acc: 0.7200\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0230 - acc: 0.7414 - val_loss: 2.6340 - val_acc: 0.7231\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9141 - acc: 0.7541 - val_loss: 2.4011 - val_acc: 0.7487\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9970 - acc: 0.7444 - val_loss: 2.3620 - val_acc: 0.7513\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9194 - acc: 0.7527 - val_loss: 2.3638 - val_acc: 0.7556\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9884 - acc: 0.7420 - val_loss: 2.6519 - val_acc: 0.7219\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8583 - acc: 0.7602 - val_loss: 2.4193 - val_acc: 0.7475\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0048 - acc: 0.7412 - val_loss: 2.5853 - val_acc: 0.7262\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9648 - acc: 0.7447 - val_loss: 2.4224 - val_acc: 0.7456\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8751 - acc: 0.7569 - val_loss: 2.4895 - val_acc: 0.7269\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8961 - acc: 0.7595 - val_loss: 2.5256 - val_acc: 0.7400\n",
      "Epoch 745/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 16s 4s/step - loss: 1.8658 - acc: 0.7602 - val_loss: 2.3353 - val_acc: 0.7569\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0059 - acc: 0.7409 - val_loss: 2.5137 - val_acc: 0.7412\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9466 - acc: 0.7480 - val_loss: 2.2935 - val_acc: 0.7606\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9586 - acc: 0.7466 - val_loss: 2.4394 - val_acc: 0.7475\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8816 - acc: 0.7547 - val_loss: 2.1940 - val_acc: 0.7606\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9751 - acc: 0.7439 - val_loss: 2.2040 - val_acc: 0.7631\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9247 - acc: 0.7625 - val_loss: 2.3420 - val_acc: 0.7506\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9654 - acc: 0.7502 - val_loss: 2.4391 - val_acc: 0.7431\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.0735 - acc: 0.7412 - val_loss: 2.6005 - val_acc: 0.7294\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8514 - acc: 0.7580 - val_loss: 2.5538 - val_acc: 0.7400\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8357 - acc: 0.7648 - val_loss: 2.5195 - val_acc: 0.7362\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7415 - acc: 0.7641 - val_loss: 2.2912 - val_acc: 0.7613\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7887 - acc: 0.7552 - val_loss: 2.2100 - val_acc: 0.7569\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0309 - acc: 0.7291 - val_loss: 2.0991 - val_acc: 0.7663\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8870 - acc: 0.7419 - val_loss: 2.3160 - val_acc: 0.7487\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7784 - acc: 0.7592 - val_loss: 2.6706 - val_acc: 0.7244\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7886 - acc: 0.7553 - val_loss: 2.4823 - val_acc: 0.7362\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7111 - acc: 0.7662 - val_loss: 2.5912 - val_acc: 0.7225\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7780 - acc: 0.7573 - val_loss: 2.1227 - val_acc: 0.7694\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8184 - acc: 0.7525 - val_loss: 2.4711 - val_acc: 0.7319\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7806 - acc: 0.7611 - val_loss: 2.3729 - val_acc: 0.7494\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7829 - acc: 0.7605 - val_loss: 2.3933 - val_acc: 0.7419\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7900 - acc: 0.7597 - val_loss: 2.4479 - val_acc: 0.7406\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7875 - acc: 0.7572 - val_loss: 2.3983 - val_acc: 0.7362\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8141 - acc: 0.7508 - val_loss: 2.2265 - val_acc: 0.7538\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7175 - acc: 0.7667 - val_loss: 2.3776 - val_acc: 0.7444\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7131 - acc: 0.7639 - val_loss: 2.2548 - val_acc: 0.7594\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6865 - acc: 0.7673 - val_loss: 2.4754 - val_acc: 0.7381\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7664 - acc: 0.7581 - val_loss: 2.3877 - val_acc: 0.7513\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7507 - acc: 0.7669 - val_loss: 2.4310 - val_acc: 0.7487\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8132 - acc: 0.7517 - val_loss: 2.3219 - val_acc: 0.7513\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7602 - acc: 0.7578 - val_loss: 2.4443 - val_acc: 0.7381\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8722 - acc: 0.7467 - val_loss: 2.1160 - val_acc: 0.7644\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8659 - acc: 0.7502 - val_loss: 2.2647 - val_acc: 0.7563\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7698 - acc: 0.7592 - val_loss: 2.3677 - val_acc: 0.7444\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7722 - acc: 0.7564 - val_loss: 2.4292 - val_acc: 0.7462\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7833 - acc: 0.7589 - val_loss: 2.3614 - val_acc: 0.7400\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8803 - acc: 0.7442 - val_loss: 2.2651 - val_acc: 0.7513\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7426 - acc: 0.7634 - val_loss: 2.2741 - val_acc: 0.7638\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7314 - acc: 0.7611 - val_loss: 2.3191 - val_acc: 0.7581\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8246 - acc: 0.7484 - val_loss: 2.5232 - val_acc: 0.7344\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7580 - acc: 0.7631 - val_loss: 2.5236 - val_acc: 0.7331\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8498 - acc: 0.7469 - val_loss: 2.5371 - val_acc: 0.7431\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8469 - acc: 0.7470 - val_loss: 2.0681 - val_acc: 0.7750\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7806 - acc: 0.7558 - val_loss: 2.1712 - val_acc: 0.7644\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7975 - acc: 0.7484 - val_loss: 2.2111 - val_acc: 0.7625\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8349 - acc: 0.7492 - val_loss: 2.3619 - val_acc: 0.7506\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8392 - acc: 0.7455 - val_loss: 2.4914 - val_acc: 0.7375\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8588 - acc: 0.7447 - val_loss: 2.2497 - val_acc: 0.7538\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8176 - acc: 0.7519 - val_loss: 2.9277 - val_acc: 0.6969\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8268 - acc: 0.7481 - val_loss: 2.2656 - val_acc: 0.7494\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8721 - acc: 0.7445 - val_loss: 1.8982 - val_acc: 0.7906\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8752 - acc: 0.7445 - val_loss: 2.4694 - val_acc: 0.7319\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7552 - acc: 0.7572 - val_loss: 2.4837 - val_acc: 0.7319\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7430 - acc: 0.7613 - val_loss: 2.3092 - val_acc: 0.7556\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8102 - acc: 0.7503 - val_loss: 2.1957 - val_acc: 0.7631\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8332 - acc: 0.7516 - val_loss: 2.5540 - val_acc: 0.7269\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7644 - acc: 0.7578 - val_loss: 2.2488 - val_acc: 0.7650\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8703 - acc: 0.7431 - val_loss: 2.3688 - val_acc: 0.7487\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7272 - acc: 0.7577 - val_loss: 2.7862 - val_acc: 0.7237\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7704 - acc: 0.7581 - val_loss: 2.4007 - val_acc: 0.7487\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8485 - acc: 0.7456 - val_loss: 2.2823 - val_acc: 0.7538\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7607 - acc: 0.7581 - val_loss: 2.4207 - val_acc: 0.7469\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8330 - acc: 0.7467 - val_loss: 2.3770 - val_acc: 0.7419\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6380 - acc: 0.7703 - val_loss: 2.4274 - val_acc: 0.7369\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6828 - acc: 0.7670 - val_loss: 2.8645 - val_acc: 0.7163\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6687 - acc: 0.7686 - val_loss: 2.3724 - val_acc: 0.7500\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7064 - acc: 0.7581 - val_loss: 2.3851 - val_acc: 0.7444\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7443 - acc: 0.7581 - val_loss: 2.0325 - val_acc: 0.7694\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6750 - acc: 0.7677 - val_loss: 2.1592 - val_acc: 0.7663\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8137 - acc: 0.7500 - val_loss: 2.1550 - val_acc: 0.7594\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8256 - acc: 0.7500 - val_loss: 2.3644 - val_acc: 0.7487\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8855 - acc: 0.7444 - val_loss: 2.0397 - val_acc: 0.7750\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7369 - acc: 0.7628 - val_loss: 2.3636 - val_acc: 0.7513\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8904 - acc: 0.7405 - val_loss: 2.3327 - val_acc: 0.7538\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8020 - acc: 0.7539 - val_loss: 2.3491 - val_acc: 0.7412\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8401 - acc: 0.7452 - val_loss: 2.2214 - val_acc: 0.7669\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7780 - acc: 0.7548 - val_loss: 2.0931 - val_acc: 0.7725\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7853 - acc: 0.7516 - val_loss: 2.4073 - val_acc: 0.7475\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8825 - acc: 0.7411 - val_loss: 2.2588 - val_acc: 0.7588\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7529 - acc: 0.7583 - val_loss: 2.4068 - val_acc: 0.7456\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8042 - acc: 0.7509 - val_loss: 2.3876 - val_acc: 0.7588\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7314 - acc: 0.7570 - val_loss: 2.3543 - val_acc: 0.7500\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8348 - acc: 0.7464 - val_loss: 2.2244 - val_acc: 0.7525\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7451 - acc: 0.7578 - val_loss: 2.1340 - val_acc: 0.7694\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8371 - acc: 0.7438 - val_loss: 2.4604 - val_acc: 0.7356\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7688 - acc: 0.7558 - val_loss: 2.7699 - val_acc: 0.7131\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7125 - acc: 0.7622 - val_loss: 2.3904 - val_acc: 0.7481\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6888 - acc: 0.7677 - val_loss: 2.5250 - val_acc: 0.7369\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7628 - acc: 0.7547 - val_loss: 2.3982 - val_acc: 0.7462\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6560 - acc: 0.7673 - val_loss: 2.4037 - val_acc: 0.7531\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7155 - acc: 0.7642 - val_loss: 2.2640 - val_acc: 0.7569\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8040 - acc: 0.7463 - val_loss: 2.5155 - val_acc: 0.7337\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7242 - acc: 0.7648 - val_loss: 2.6897 - val_acc: 0.7281\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7845 - acc: 0.7530 - val_loss: 2.4711 - val_acc: 0.7437\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7634 - acc: 0.7566 - val_loss: 2.5736 - val_acc: 0.7294\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7981 - acc: 0.7509 - val_loss: 2.0563 - val_acc: 0.7744\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8328 - acc: 0.7452 - val_loss: 2.5833 - val_acc: 0.7375\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9188 - acc: 0.7336 - val_loss: 2.4669 - val_acc: 0.7387\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6970 - acc: 0.7622 - val_loss: 2.3194 - val_acc: 0.7650\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6776 - acc: 0.7655 - val_loss: 2.4633 - val_acc: 0.7450\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8261 - acc: 0.7462 - val_loss: 2.3947 - val_acc: 0.7469\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7447 - acc: 0.7570 - val_loss: 2.6884 - val_acc: 0.7181\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7372 - acc: 0.7581 - val_loss: 2.5654 - val_acc: 0.7331\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7669 - acc: 0.7545 - val_loss: 1.9971 - val_acc: 0.7763\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7849 - acc: 0.7506 - val_loss: 2.5937 - val_acc: 0.7319\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7299 - acc: 0.7572 - val_loss: 2.2280 - val_acc: 0.7631\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7134 - acc: 0.7620 - val_loss: 2.1880 - val_acc: 0.7713\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7536 - acc: 0.7563 - val_loss: 2.4705 - val_acc: 0.7487\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7572 - acc: 0.7536 - val_loss: 2.4270 - val_acc: 0.7425\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7103 - acc: 0.7659 - val_loss: 2.2234 - val_acc: 0.7619\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8320 - acc: 0.7463 - val_loss: 2.4498 - val_acc: 0.7419\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7826 - acc: 0.7536 - val_loss: 2.2552 - val_acc: 0.7613\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7412 - acc: 0.7558 - val_loss: 2.4522 - val_acc: 0.7431\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7995 - acc: 0.7503 - val_loss: 2.2403 - val_acc: 0.7631\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7044 - acc: 0.7595 - val_loss: 2.1948 - val_acc: 0.7625\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6891 - acc: 0.7638 - val_loss: 2.4704 - val_acc: 0.7481\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7933 - acc: 0.7516 - val_loss: 2.9144 - val_acc: 0.7031\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6667 - acc: 0.7706 - val_loss: 2.6515 - val_acc: 0.7244\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8032 - acc: 0.7517 - val_loss: 2.2503 - val_acc: 0.7613\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7514 - acc: 0.7550 - val_loss: 2.3508 - val_acc: 0.7481\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6987 - acc: 0.7634 - val_loss: 2.4701 - val_acc: 0.7450\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7806 - acc: 0.7558 - val_loss: 2.4547 - val_acc: 0.7475\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7836 - acc: 0.7503 - val_loss: 2.3269 - val_acc: 0.7481\n",
      "Epoch 869/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 14s 3s/step - loss: 1.7522 - acc: 0.7603 - val_loss: 2.0721 - val_acc: 0.7781\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6267 - acc: 0.7742 - val_loss: 2.3236 - val_acc: 0.7525\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7290 - acc: 0.7603 - val_loss: 2.1364 - val_acc: 0.7756\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7957 - acc: 0.7527 - val_loss: 2.3150 - val_acc: 0.7600\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7294 - acc: 0.7656 - val_loss: 2.3605 - val_acc: 0.7600\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7704 - acc: 0.7555 - val_loss: 2.4935 - val_acc: 0.7450\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7427 - acc: 0.7612 - val_loss: 2.2926 - val_acc: 0.7556\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8422 - acc: 0.7517 - val_loss: 2.6413 - val_acc: 0.7331\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7612 - acc: 0.7605 - val_loss: 2.1898 - val_acc: 0.7663\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8071 - acc: 0.7525 - val_loss: 2.3148 - val_acc: 0.7500\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9206 - acc: 0.7558 - val_loss: 2.4509 - val_acc: 0.7469\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1788 - acc: 0.7552 - val_loss: 2.4387 - val_acc: 0.7531\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3871 - acc: 0.7572 - val_loss: 2.5191 - val_acc: 0.7425\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3262 - acc: 0.7564 - val_loss: 2.5649 - val_acc: 0.7425\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9586 - acc: 0.7519 - val_loss: 2.4803 - val_acc: 0.7456\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9318 - acc: 0.7475 - val_loss: 2.4572 - val_acc: 0.7369\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9391 - acc: 0.7484 - val_loss: 2.6755 - val_acc: 0.7375\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9326 - acc: 0.7419 - val_loss: 2.7240 - val_acc: 0.7231\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8595 - acc: 0.7397 - val_loss: 2.7296 - val_acc: 0.7250\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7501 - acc: 0.7577 - val_loss: 2.4259 - val_acc: 0.7462\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9445 - acc: 0.7356 - val_loss: 2.3658 - val_acc: 0.7563\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8621 - acc: 0.7466 - val_loss: 2.5559 - val_acc: 0.7431\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7494 - acc: 0.7547 - val_loss: 2.5074 - val_acc: 0.7506\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8426 - acc: 0.7513 - val_loss: 2.6856 - val_acc: 0.7362\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7952 - acc: 0.7495 - val_loss: 2.7900 - val_acc: 0.7294\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7871 - acc: 0.7539 - val_loss: 2.3101 - val_acc: 0.7588\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7914 - acc: 0.7523 - val_loss: 2.4264 - val_acc: 0.7525\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8257 - acc: 0.7464 - val_loss: 2.6362 - val_acc: 0.7387\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8068 - acc: 0.7475 - val_loss: 2.4338 - val_acc: 0.7494\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7383 - acc: 0.7644 - val_loss: 2.7098 - val_acc: 0.7250\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8099 - acc: 0.7512 - val_loss: 2.6367 - val_acc: 0.7456\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7193 - acc: 0.7588 - val_loss: 2.7046 - val_acc: 0.7306\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7283 - acc: 0.7600 - val_loss: 2.7572 - val_acc: 0.7181\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8310 - acc: 0.7456 - val_loss: 2.5882 - val_acc: 0.7419\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8547 - acc: 0.7434 - val_loss: 2.6388 - val_acc: 0.7319\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8657 - acc: 0.7466 - val_loss: 2.6040 - val_acc: 0.7337\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7833 - acc: 0.7550 - val_loss: 2.5924 - val_acc: 0.7419\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8645 - acc: 0.7481 - val_loss: 2.8790 - val_acc: 0.7125\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6840 - acc: 0.7655 - val_loss: 2.4032 - val_acc: 0.7469\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7247 - acc: 0.7606 - val_loss: 2.7361 - val_acc: 0.7325\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7502 - acc: 0.7586 - val_loss: 2.5498 - val_acc: 0.7462\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7692 - acc: 0.7533 - val_loss: 2.5580 - val_acc: 0.7462\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8181 - acc: 0.7464 - val_loss: 2.4903 - val_acc: 0.7469\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7651 - acc: 0.7594 - val_loss: 2.4597 - val_acc: 0.7563\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7790 - acc: 0.7537 - val_loss: 2.7496 - val_acc: 0.7244\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8488 - acc: 0.7473 - val_loss: 2.7558 - val_acc: 0.7106\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9479 - acc: 0.7300 - val_loss: 2.6844 - val_acc: 0.7300\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8360 - acc: 0.7483 - val_loss: 2.7134 - val_acc: 0.7256\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8316 - acc: 0.7483 - val_loss: 2.4043 - val_acc: 0.7494\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7553 - acc: 0.7575 - val_loss: 2.5176 - val_acc: 0.7444\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7247 - acc: 0.7595 - val_loss: 2.7085 - val_acc: 0.7400\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8127 - acc: 0.7488 - val_loss: 2.5160 - val_acc: 0.7456\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7792 - acc: 0.7541 - val_loss: 2.6207 - val_acc: 0.7350\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8368 - acc: 0.7466 - val_loss: 2.4634 - val_acc: 0.7475\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7328 - acc: 0.7591 - val_loss: 2.7212 - val_acc: 0.7312\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7247 - acc: 0.7638 - val_loss: 2.5364 - val_acc: 0.7431\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9197 - acc: 0.7373 - val_loss: 2.4842 - val_acc: 0.7469\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8540 - acc: 0.7438 - val_loss: 2.4814 - val_acc: 0.7444\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7775 - acc: 0.7498 - val_loss: 2.5792 - val_acc: 0.7350\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8675 - acc: 0.7478 - val_loss: 2.5247 - val_acc: 0.7525\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8918 - acc: 0.7389 - val_loss: 2.4843 - val_acc: 0.7469\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8058 - acc: 0.7458 - val_loss: 2.5113 - val_acc: 0.7369\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7931 - acc: 0.7509 - val_loss: 2.4231 - val_acc: 0.7569\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7838 - acc: 0.7542 - val_loss: 2.6938 - val_acc: 0.7312\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7342 - acc: 0.7602 - val_loss: 2.5889 - val_acc: 0.7362\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7795 - acc: 0.7542 - val_loss: 2.7254 - val_acc: 0.7281\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7899 - acc: 0.7505 - val_loss: 2.3728 - val_acc: 0.7631\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6542 - acc: 0.7703 - val_loss: 2.6330 - val_acc: 0.7325\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8339 - acc: 0.7505 - val_loss: 2.6803 - val_acc: 0.7331\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7653 - acc: 0.7569 - val_loss: 2.4967 - val_acc: 0.7450\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7833 - acc: 0.7545 - val_loss: 2.6185 - val_acc: 0.7375\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8041 - acc: 0.7506 - val_loss: 2.8631 - val_acc: 0.7212\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7981 - acc: 0.7473 - val_loss: 2.5011 - val_acc: 0.7419\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8295 - acc: 0.7491 - val_loss: 2.6518 - val_acc: 0.7294\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9462 - acc: 0.7397 - val_loss: 2.8878 - val_acc: 0.7100\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7940 - acc: 0.7498 - val_loss: 2.5554 - val_acc: 0.7300\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7794 - acc: 0.7516 - val_loss: 2.9282 - val_acc: 0.7131\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7557 - acc: 0.7536 - val_loss: 2.9141 - val_acc: 0.7056\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7382 - acc: 0.7595 - val_loss: 2.6035 - val_acc: 0.7394\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8197 - acc: 0.7430 - val_loss: 2.4200 - val_acc: 0.7569\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7914 - acc: 0.7494 - val_loss: 2.6257 - val_acc: 0.7356\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7465 - acc: 0.7542 - val_loss: 2.6229 - val_acc: 0.7481\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7396 - acc: 0.7555 - val_loss: 2.5464 - val_acc: 0.7394\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7973 - acc: 0.7502 - val_loss: 2.4631 - val_acc: 0.7450\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6880 - acc: 0.7614 - val_loss: 2.6934 - val_acc: 0.7269\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7724 - acc: 0.7544 - val_loss: 2.4620 - val_acc: 0.7519\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8738 - acc: 0.7427 - val_loss: 2.4915 - val_acc: 0.7444\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7597 - acc: 0.7522 - val_loss: 2.4318 - val_acc: 0.7525\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8511 - acc: 0.7442 - val_loss: 2.7332 - val_acc: 0.7144\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6620 - acc: 0.7689 - val_loss: 2.7801 - val_acc: 0.7212\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7490 - acc: 0.7566 - val_loss: 2.5709 - val_acc: 0.7387\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7394 - acc: 0.7564 - val_loss: 2.6464 - val_acc: 0.7369\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6990 - acc: 0.7592 - val_loss: 2.5260 - val_acc: 0.7544\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7433 - acc: 0.7527 - val_loss: 2.6925 - val_acc: 0.7319\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7290 - acc: 0.7602 - val_loss: 2.6937 - val_acc: 0.7325\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8635 - acc: 0.7411 - val_loss: 2.6030 - val_acc: 0.7319\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8469 - acc: 0.7519 - val_loss: 2.5513 - val_acc: 0.7462\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7619 - acc: 0.7564 - val_loss: 2.8133 - val_acc: 0.7250\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7767 - acc: 0.7564 - val_loss: 2.6703 - val_acc: 0.7344\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8514 - acc: 0.7389 - val_loss: 2.7658 - val_acc: 0.7219\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8254 - acc: 0.7469 - val_loss: 2.5156 - val_acc: 0.7450\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7960 - acc: 0.7502 - val_loss: 2.6134 - val_acc: 0.7312\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7395 - acc: 0.7552 - val_loss: 2.8268 - val_acc: 0.7200\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7799 - acc: 0.7467 - val_loss: 2.5471 - val_acc: 0.7419\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8078 - acc: 0.7486 - val_loss: 2.8321 - val_acc: 0.7256\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7606 - acc: 0.7527 - val_loss: 2.5232 - val_acc: 0.7444\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8173 - acc: 0.7487 - val_loss: 2.6540 - val_acc: 0.7350\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9289 - acc: 0.7314 - val_loss: 2.6533 - val_acc: 0.7319\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7154 - acc: 0.7578 - val_loss: 2.4971 - val_acc: 0.7431\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7937 - acc: 0.7512 - val_loss: 2.7763 - val_acc: 0.7256\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8765 - acc: 0.7378 - val_loss: 2.6118 - val_acc: 0.7369\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7855 - acc: 0.7552 - val_loss: 2.6489 - val_acc: 0.7350\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7729 - acc: 0.7484 - val_loss: 2.7302 - val_acc: 0.7294\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7248 - acc: 0.7594 - val_loss: 2.7671 - val_acc: 0.7256\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7763 - acc: 0.7531 - val_loss: 2.6559 - val_acc: 0.7381\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8129 - acc: 0.7458 - val_loss: 2.6243 - val_acc: 0.7412\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7627 - acc: 0.7545 - val_loss: 2.8819 - val_acc: 0.7131\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9572 - acc: 0.7305 - val_loss: 2.7967 - val_acc: 0.7212\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8509 - acc: 0.7436 - val_loss: 2.7952 - val_acc: 0.7275\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7627 - acc: 0.7506 - val_loss: 2.6713 - val_acc: 0.7294\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8172 - acc: 0.7425 - val_loss: 2.8090 - val_acc: 0.7231\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7674 - acc: 0.7489 - val_loss: 2.5287 - val_acc: 0.7456\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6916 - acc: 0.7631 - val_loss: 2.8267 - val_acc: 0.7188\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7776 - acc: 0.7483 - val_loss: 2.8889 - val_acc: 0.7131\n",
      "Epoch 993/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 4s/step - loss: 1.6497 - acc: 0.7655 - val_loss: 2.5822 - val_acc: 0.7419\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7201 - acc: 0.7589 - val_loss: 2.7029 - val_acc: 0.7237\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7426 - acc: 0.7544 - val_loss: 2.3053 - val_acc: 0.7619\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7565 - acc: 0.7508 - val_loss: 2.6856 - val_acc: 0.7212\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7192 - acc: 0.7553 - val_loss: 2.6534 - val_acc: 0.7375\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6856 - acc: 0.7625 - val_loss: 2.9920 - val_acc: 0.7106\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7192 - acc: 0.7584 - val_loss: 2.8969 - val_acc: 0.7169\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8024 - acc: 0.7491 - val_loss: 2.5385 - val_acc: 0.7412\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6979 - acc: 0.7584 - val_loss: 2.9012 - val_acc: 0.7244\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8248 - acc: 0.7455 - val_loss: 2.6018 - val_acc: 0.7400\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7851 - acc: 0.7466 - val_loss: 2.7056 - val_acc: 0.7344\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7579 - acc: 0.7547 - val_loss: 2.5208 - val_acc: 0.7444\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7507 - acc: 0.7616 - val_loss: 2.5001 - val_acc: 0.7425\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7243 - acc: 0.7578 - val_loss: 2.5588 - val_acc: 0.7475\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7063 - acc: 0.7573 - val_loss: 2.4602 - val_acc: 0.7494\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6741 - acc: 0.7633 - val_loss: 2.6925 - val_acc: 0.7294\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7319 - acc: 0.7597 - val_loss: 2.6386 - val_acc: 0.7281\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8652 - acc: 0.7378 - val_loss: 2.8537 - val_acc: 0.7225\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7437 - acc: 0.7519 - val_loss: 2.6522 - val_acc: 0.7369\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8306 - acc: 0.7411 - val_loss: 2.7116 - val_acc: 0.7319\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7990 - acc: 0.7470 - val_loss: 2.7033 - val_acc: 0.7294\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8237 - acc: 0.7494 - val_loss: 2.6588 - val_acc: 0.7350\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6765 - acc: 0.7637 - val_loss: 2.6204 - val_acc: 0.7337\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8300 - acc: 0.7430 - val_loss: 2.7286 - val_acc: 0.7181\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7853 - acc: 0.7477 - val_loss: 3.0453 - val_acc: 0.7094\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8172 - acc: 0.7441 - val_loss: 2.6409 - val_acc: 0.7356\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7879 - acc: 0.7495 - val_loss: 2.7675 - val_acc: 0.7287\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6576 - acc: 0.7645 - val_loss: 2.9439 - val_acc: 0.7188\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7044 - acc: 0.7655 - val_loss: 2.4682 - val_acc: 0.7513\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8027 - acc: 0.7525 - val_loss: 2.4455 - val_acc: 0.7462\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7826 - acc: 0.7489 - val_loss: 2.6955 - val_acc: 0.7369\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8102 - acc: 0.7453 - val_loss: 2.7580 - val_acc: 0.7281\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7774 - acc: 0.7522 - val_loss: 2.7620 - val_acc: 0.7275\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6979 - acc: 0.7645 - val_loss: 2.6427 - val_acc: 0.7375\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8724 - acc: 0.7395 - val_loss: 2.5664 - val_acc: 0.7431\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7809 - acc: 0.7473 - val_loss: 2.6905 - val_acc: 0.7437\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8445 - acc: 0.7395 - val_loss: 2.5971 - val_acc: 0.7394\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7637 - acc: 0.7513 - val_loss: 2.4303 - val_acc: 0.7550\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7926 - acc: 0.7459 - val_loss: 2.9493 - val_acc: 0.7131\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7869 - acc: 0.7431 - val_loss: 2.6455 - val_acc: 0.7400\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8283 - acc: 0.7447 - val_loss: 2.9381 - val_acc: 0.7219\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7218 - acc: 0.7556 - val_loss: 2.7107 - val_acc: 0.7312\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8848 - acc: 0.7425 - val_loss: 2.6651 - val_acc: 0.7431\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7144 - acc: 0.7555 - val_loss: 2.4859 - val_acc: 0.7506\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9669 - acc: 0.7286 - val_loss: 2.6867 - val_acc: 0.7437\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7926 - acc: 0.7514 - val_loss: 2.7182 - val_acc: 0.7437\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7212 - acc: 0.7584 - val_loss: 2.6444 - val_acc: 0.7281\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7035 - acc: 0.7602 - val_loss: 2.7057 - val_acc: 0.7362\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7891 - acc: 0.7505 - val_loss: 2.5534 - val_acc: 0.7469\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7576 - acc: 0.7498 - val_loss: 2.8368 - val_acc: 0.7138\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7516 - acc: 0.7527 - val_loss: 2.7910 - val_acc: 0.7381\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9098 - acc: 0.7356 - val_loss: 2.9103 - val_acc: 0.7206\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7771 - acc: 0.7492 - val_loss: 2.4901 - val_acc: 0.7581\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8928 - acc: 0.7369 - val_loss: 2.5912 - val_acc: 0.7456\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8022 - acc: 0.7459 - val_loss: 2.4511 - val_acc: 0.7575\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8422 - acc: 0.7402 - val_loss: 2.7990 - val_acc: 0.7281\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8015 - acc: 0.7484 - val_loss: 2.7664 - val_acc: 0.7275\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7896 - acc: 0.7502 - val_loss: 3.1817 - val_acc: 0.6988\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7553 - acc: 0.7534 - val_loss: 2.7413 - val_acc: 0.7225\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7621 - acc: 0.7527 - val_loss: 2.6892 - val_acc: 0.7375\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7542 - acc: 0.7516 - val_loss: 2.5353 - val_acc: 0.7525\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7859 - acc: 0.7497 - val_loss: 2.7052 - val_acc: 0.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7666 - acc: 0.7498 - val_loss: 2.4784 - val_acc: 0.7550\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8360 - acc: 0.7416 - val_loss: 2.7098 - val_acc: 0.7256\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7905 - acc: 0.7477 - val_loss: 2.5000 - val_acc: 0.7538\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8013 - acc: 0.7478 - val_loss: 2.7110 - val_acc: 0.7319\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7442 - acc: 0.7556 - val_loss: 2.6598 - val_acc: 0.7450\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7329 - acc: 0.7516 - val_loss: 2.7090 - val_acc: 0.7294\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8297 - acc: 0.7405 - val_loss: 2.5592 - val_acc: 0.7531\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7361 - acc: 0.7577 - val_loss: 2.6639 - val_acc: 0.7387\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7206 - acc: 0.7553 - val_loss: 2.6960 - val_acc: 0.7375\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6991 - acc: 0.7613 - val_loss: 2.3681 - val_acc: 0.7619\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8898 - acc: 0.7323 - val_loss: 2.7363 - val_acc: 0.7325\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6705 - acc: 0.7625 - val_loss: 2.6253 - val_acc: 0.7331\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7682 - acc: 0.7486 - val_loss: 2.5062 - val_acc: 0.7569\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7053 - acc: 0.7578 - val_loss: 2.6246 - val_acc: 0.7450\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7047 - acc: 0.7548 - val_loss: 3.0246 - val_acc: 0.7031\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7956 - acc: 0.7523 - val_loss: 2.9582 - val_acc: 0.7150\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7299 - acc: 0.7570 - val_loss: 2.7423 - val_acc: 0.7312\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7183 - acc: 0.7587 - val_loss: 2.7398 - val_acc: 0.7369\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7049 - acc: 0.7583 - val_loss: 2.8744 - val_acc: 0.7156\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7833 - acc: 0.7447 - val_loss: 2.8867 - val_acc: 0.7200\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7236 - acc: 0.7548 - val_loss: 2.5982 - val_acc: 0.7513\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7498 - acc: 0.7502 - val_loss: 2.4342 - val_acc: 0.7456\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6679 - acc: 0.7641 - val_loss: 2.4322 - val_acc: 0.7606\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7655 - acc: 0.7502 - val_loss: 2.6485 - val_acc: 0.7319\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7554 - acc: 0.7545 - val_loss: 2.4814 - val_acc: 0.7519\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8013 - acc: 0.7494 - val_loss: 2.9614 - val_acc: 0.7081\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7102 - acc: 0.7575 - val_loss: 2.6981 - val_acc: 0.7425\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7456 - acc: 0.7523 - val_loss: 2.7061 - val_acc: 0.7394\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.5688 - acc: 0.7773 - val_loss: 2.9255 - val_acc: 0.7256\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7450 - acc: 0.7534 - val_loss: 2.8925 - val_acc: 0.7125\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8116 - acc: 0.7491 - val_loss: 2.6120 - val_acc: 0.7450\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7907 - acc: 0.7609 - val_loss: 2.4420 - val_acc: 0.7544\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7225 - acc: 0.7630 - val_loss: 2.6298 - val_acc: 0.7262\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9239 - acc: 0.7292 - val_loss: 2.8242 - val_acc: 0.7237\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8018 - acc: 0.7495 - val_loss: 2.6270 - val_acc: 0.7337\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8428 - acc: 0.7442 - val_loss: 2.6016 - val_acc: 0.7431\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7988 - acc: 0.7505 - val_loss: 2.6386 - val_acc: 0.7487\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8906 - acc: 0.7356 - val_loss: 2.6682 - val_acc: 0.7306\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8210 - acc: 0.7420 - val_loss: 2.5156 - val_acc: 0.7475\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9211 - acc: 0.7352 - val_loss: 2.6004 - val_acc: 0.7362\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7422 - acc: 0.7528 - val_loss: 2.9629 - val_acc: 0.7088\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7833 - acc: 0.7544 - val_loss: 2.5775 - val_acc: 0.7419\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8219 - acc: 0.7425 - val_loss: 2.2524 - val_acc: 0.7788\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8443 - acc: 0.7452 - val_loss: 2.8240 - val_acc: 0.7169\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8649 - acc: 0.7373 - val_loss: 2.4742 - val_acc: 0.7550\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8313 - acc: 0.7531 - val_loss: 2.5067 - val_acc: 0.7462\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7357 - acc: 0.7512 - val_loss: 2.9157 - val_acc: 0.7181\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6842 - acc: 0.7655 - val_loss: 2.4392 - val_acc: 0.7550\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7807 - acc: 0.7458 - val_loss: 2.7739 - val_acc: 0.7287\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7820 - acc: 0.7498 - val_loss: 2.4380 - val_acc: 0.7606\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8147 - acc: 0.7464 - val_loss: 2.6101 - val_acc: 0.7544\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7956 - acc: 0.7484 - val_loss: 2.2357 - val_acc: 0.7781\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7376 - acc: 0.7575 - val_loss: 2.4915 - val_acc: 0.7600\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7884 - acc: 0.7495 - val_loss: 3.0021 - val_acc: 0.7219\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8391 - acc: 0.7455 - val_loss: 2.5872 - val_acc: 0.7425\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7094 - acc: 0.7614 - val_loss: 2.8071 - val_acc: 0.7225\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6709 - acc: 0.7616 - val_loss: 2.3696 - val_acc: 0.7563\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6569 - acc: 0.7678 - val_loss: 2.6769 - val_acc: 0.7406\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7829 - acc: 0.7480 - val_loss: 2.8360 - val_acc: 0.7244\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8530 - acc: 0.7442 - val_loss: 3.1012 - val_acc: 0.7113\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7557 - acc: 0.7523 - val_loss: 2.4849 - val_acc: 0.7581\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7220 - acc: 0.7531 - val_loss: 2.6771 - val_acc: 0.7356\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7973 - acc: 0.7508 - val_loss: 2.5677 - val_acc: 0.7431\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9017 - acc: 0.7366 - val_loss: 2.5049 - val_acc: 0.7575\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6819 - acc: 0.7652 - val_loss: 2.8553 - val_acc: 0.7250\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7015 - acc: 0.7547 - val_loss: 2.3912 - val_acc: 0.7631\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7421 - acc: 0.7548 - val_loss: 2.7704 - val_acc: 0.7350\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7363 - acc: 0.7570 - val_loss: 2.7482 - val_acc: 0.7306\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7688 - acc: 0.7567 - val_loss: 2.7238 - val_acc: 0.7375\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6819 - acc: 0.7584 - val_loss: 2.7361 - val_acc: 0.7337\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7229 - acc: 0.7556 - val_loss: 2.6105 - val_acc: 0.7469\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8391 - acc: 0.7430 - val_loss: 2.5280 - val_acc: 0.7487\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7244 - acc: 0.7547 - val_loss: 2.5395 - val_acc: 0.7481\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6451 - acc: 0.7647 - val_loss: 2.3365 - val_acc: 0.7663\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7752 - acc: 0.7534 - val_loss: 2.7685 - val_acc: 0.7287\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7561 - acc: 0.7528 - val_loss: 2.8965 - val_acc: 0.7219\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8541 - acc: 0.7366 - val_loss: 2.5223 - val_acc: 0.7581\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8739 - acc: 0.7408 - val_loss: 2.7538 - val_acc: 0.7381\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7917 - acc: 0.7452 - val_loss: 2.5697 - val_acc: 0.7494\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7454 - acc: 0.7572 - val_loss: 2.4449 - val_acc: 0.7594\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7870 - acc: 0.7491 - val_loss: 2.5069 - val_acc: 0.7469\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7411 - acc: 0.7569 - val_loss: 2.5350 - val_acc: 0.7387\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7902 - acc: 0.7486 - val_loss: 2.7585 - val_acc: 0.7444\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8645 - acc: 0.7391 - val_loss: 2.7243 - val_acc: 0.7362\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6693 - acc: 0.7641 - val_loss: 2.7241 - val_acc: 0.7331\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8842 - acc: 0.7366 - val_loss: 2.7887 - val_acc: 0.7281\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8423 - acc: 0.7419 - val_loss: 2.3606 - val_acc: 0.7625\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7515 - acc: 0.7550 - val_loss: 2.6885 - val_acc: 0.7450\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8277 - acc: 0.7439 - val_loss: 2.5036 - val_acc: 0.7550\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9301 - acc: 0.7388 - val_loss: 2.7355 - val_acc: 0.7375\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7863 - acc: 0.7445 - val_loss: 2.5690 - val_acc: 0.7456\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6647 - acc: 0.7664 - val_loss: 2.9190 - val_acc: 0.7250\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7769 - acc: 0.7514 - val_loss: 2.8037 - val_acc: 0.7287\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6868 - acc: 0.7559 - val_loss: 2.8834 - val_acc: 0.7250\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6859 - acc: 0.7636 - val_loss: 2.6540 - val_acc: 0.7437\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7491 - acc: 0.7539 - val_loss: 2.4648 - val_acc: 0.7519\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8153 - acc: 0.7488 - val_loss: 2.5124 - val_acc: 0.7538\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8297 - acc: 0.7459 - val_loss: 2.3503 - val_acc: 0.7650\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7919 - acc: 0.7508 - val_loss: 2.7207 - val_acc: 0.7269\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7832 - acc: 0.7472 - val_loss: 2.5856 - val_acc: 0.7412\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7628 - acc: 0.7500 - val_loss: 2.7688 - val_acc: 0.7362\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7569 - acc: 0.7517 - val_loss: 2.9395 - val_acc: 0.7181\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7666 - acc: 0.7525 - val_loss: 2.6378 - val_acc: 0.7344\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9080 - acc: 0.7400 - val_loss: 2.6014 - val_acc: 0.7437\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6314 - acc: 0.7702 - val_loss: 2.8695 - val_acc: 0.7262\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7984 - acc: 0.7437 - val_loss: 2.7401 - val_acc: 0.7306\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7206 - acc: 0.7575 - val_loss: 3.0888 - val_acc: 0.7088\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7295 - acc: 0.7572 - val_loss: 2.7093 - val_acc: 0.7337\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7448 - acc: 0.7505 - val_loss: 2.7600 - val_acc: 0.7387\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7231 - acc: 0.7595 - val_loss: 2.8508 - val_acc: 0.7269\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7414 - acc: 0.7527 - val_loss: 2.6837 - val_acc: 0.7412\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6983 - acc: 0.7578 - val_loss: 2.9187 - val_acc: 0.7181\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7045 - acc: 0.7595 - val_loss: 2.6115 - val_acc: 0.7356\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8488 - acc: 0.7441 - val_loss: 2.4697 - val_acc: 0.7525\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7702 - acc: 0.7467 - val_loss: 2.7438 - val_acc: 0.7356\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6994 - acc: 0.7605 - val_loss: 2.5603 - val_acc: 0.7475\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7003 - acc: 0.7614 - val_loss: 2.7381 - val_acc: 0.7356\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7093 - acc: 0.7592 - val_loss: 2.7740 - val_acc: 0.7362\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7772 - acc: 0.7503 - val_loss: 2.7012 - val_acc: 0.7487\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7386 - acc: 0.7547 - val_loss: 2.5312 - val_acc: 0.7531\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6930 - acc: 0.7606 - val_loss: 2.4978 - val_acc: 0.7606\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7381 - acc: 0.7545 - val_loss: 2.7558 - val_acc: 0.7362\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7671 - acc: 0.7487 - val_loss: 2.7063 - val_acc: 0.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7818 - acc: 0.7472 - val_loss: 2.7592 - val_acc: 0.7356\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6699 - acc: 0.7603 - val_loss: 2.6803 - val_acc: 0.7406\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6392 - acc: 0.7638 - val_loss: 3.0242 - val_acc: 0.7131\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7466 - acc: 0.7569 - val_loss: 3.1253 - val_acc: 0.7113\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7457 - acc: 0.7533 - val_loss: 2.7299 - val_acc: 0.7381\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6836 - acc: 0.7600 - val_loss: 2.7213 - val_acc: 0.7294\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8104 - acc: 0.7488 - val_loss: 2.7612 - val_acc: 0.7319\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6470 - acc: 0.7619 - val_loss: 2.4464 - val_acc: 0.7581\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6624 - acc: 0.7616 - val_loss: 2.6867 - val_acc: 0.7462\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7388 - acc: 0.7595 - val_loss: 2.5889 - val_acc: 0.7387\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7742 - acc: 0.7541 - val_loss: 2.5880 - val_acc: 0.7387\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7893 - acc: 0.7469 - val_loss: 2.6984 - val_acc: 0.7331\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8056 - acc: 0.7442 - val_loss: 2.4908 - val_acc: 0.7563\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7629 - acc: 0.7491 - val_loss: 2.6046 - val_acc: 0.7481\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6984 - acc: 0.7584 - val_loss: 2.4261 - val_acc: 0.7544\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6926 - acc: 0.7616 - val_loss: 2.4794 - val_acc: 0.7550\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7675 - acc: 0.7478 - val_loss: 2.3577 - val_acc: 0.7606\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7137 - acc: 0.7575 - val_loss: 2.7817 - val_acc: 0.7300\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8147 - acc: 0.7384 - val_loss: 2.5801 - val_acc: 0.7544\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8866 - acc: 0.7355 - val_loss: 2.9382 - val_acc: 0.7212\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7302 - acc: 0.7545 - val_loss: 2.6248 - val_acc: 0.7450\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7491 - acc: 0.7561 - val_loss: 2.5050 - val_acc: 0.7487\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6719 - acc: 0.7627 - val_loss: 2.9255 - val_acc: 0.7231\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6854 - acc: 0.7583 - val_loss: 2.7382 - val_acc: 0.7369\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7353 - acc: 0.7598 - val_loss: 2.4140 - val_acc: 0.7706\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7495 - acc: 0.7539 - val_loss: 2.9725 - val_acc: 0.7269\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6550 - acc: 0.7648 - val_loss: 3.2128 - val_acc: 0.7050\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7235 - acc: 0.7527 - val_loss: 2.5757 - val_acc: 0.7494\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8598 - acc: 0.7386 - val_loss: 2.6938 - val_acc: 0.7437\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7220 - acc: 0.7530 - val_loss: 2.7077 - val_acc: 0.7312\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7023 - acc: 0.7583 - val_loss: 3.1334 - val_acc: 0.7019\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7489 - acc: 0.7511 - val_loss: 2.8078 - val_acc: 0.7362\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7697 - acc: 0.7464 - val_loss: 2.2947 - val_acc: 0.7844\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7843 - acc: 0.7459 - val_loss: 2.6673 - val_acc: 0.7312\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6968 - acc: 0.7573 - val_loss: 2.5154 - val_acc: 0.7544\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6366 - acc: 0.7642 - val_loss: 2.4539 - val_acc: 0.7544\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8197 - acc: 0.7489 - val_loss: 2.7033 - val_acc: 0.7412\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6623 - acc: 0.7666 - val_loss: 2.7446 - val_acc: 0.7300\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6573 - acc: 0.7637 - val_loss: 2.5739 - val_acc: 0.7475\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7318 - acc: 0.7475 - val_loss: 2.8718 - val_acc: 0.7231\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6995 - acc: 0.7602 - val_loss: 2.7481 - val_acc: 0.7375\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6653 - acc: 0.7633 - val_loss: 2.4905 - val_acc: 0.7606\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7439 - acc: 0.7527 - val_loss: 2.7268 - val_acc: 0.7412\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7795 - acc: 0.7456 - val_loss: 2.6650 - val_acc: 0.7387\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7856 - acc: 0.7463 - val_loss: 2.4511 - val_acc: 0.7681\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6598 - acc: 0.7636 - val_loss: 2.2898 - val_acc: 0.7738\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6359 - acc: 0.7658 - val_loss: 2.5961 - val_acc: 0.7400\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7347 - acc: 0.7547 - val_loss: 2.5428 - val_acc: 0.7575\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7640 - acc: 0.7469 - val_loss: 2.7311 - val_acc: 0.7375\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7786 - acc: 0.7459 - val_loss: 2.4805 - val_acc: 0.7613\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7592 - acc: 0.7531 - val_loss: 2.5694 - val_acc: 0.7481\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7077 - acc: 0.7531 - val_loss: 2.6192 - val_acc: 0.7462\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7533 - acc: 0.7548 - val_loss: 2.7744 - val_acc: 0.7350\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7553 - acc: 0.7494 - val_loss: 2.6898 - val_acc: 0.7425\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6554 - acc: 0.7608 - val_loss: 2.8046 - val_acc: 0.7356\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7496 - acc: 0.7555 - val_loss: 2.7749 - val_acc: 0.7344\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7167 - acc: 0.7548 - val_loss: 2.7276 - val_acc: 0.7337\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7756 - acc: 0.7450 - val_loss: 2.7504 - val_acc: 0.7356\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7030 - acc: 0.7570 - val_loss: 2.6593 - val_acc: 0.7350\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6636 - acc: 0.7631 - val_loss: 2.7024 - val_acc: 0.7375\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7100 - acc: 0.7486 - val_loss: 2.5083 - val_acc: 0.7500\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6914 - acc: 0.7564 - val_loss: 2.7204 - val_acc: 0.7431\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7502 - acc: 0.7512 - val_loss: 2.7337 - val_acc: 0.7387\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7271 - acc: 0.7547 - val_loss: 2.4838 - val_acc: 0.7544\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6895 - acc: 0.7598 - val_loss: 2.7254 - val_acc: 0.7344\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7382 - acc: 0.7548 - val_loss: 2.7455 - val_acc: 0.7412\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6998 - acc: 0.7567 - val_loss: 2.6797 - val_acc: 0.7437\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7006 - acc: 0.7555 - val_loss: 3.0115 - val_acc: 0.7050\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7904 - acc: 0.7441 - val_loss: 2.6059 - val_acc: 0.7481\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6719 - acc: 0.7594 - val_loss: 2.7086 - val_acc: 0.7312\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7899 - acc: 0.7502 - val_loss: 2.5886 - val_acc: 0.7550\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7156 - acc: 0.7595 - val_loss: 2.6839 - val_acc: 0.7375\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7682 - acc: 0.7467 - val_loss: 2.7016 - val_acc: 0.7506\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7210 - acc: 0.7492 - val_loss: 2.6683 - val_acc: 0.7406\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6710 - acc: 0.7613 - val_loss: 2.5137 - val_acc: 0.7500\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7651 - acc: 0.7536 - val_loss: 2.2568 - val_acc: 0.7688\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6425 - acc: 0.7627 - val_loss: 2.6318 - val_acc: 0.7462\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7120 - acc: 0.7498 - val_loss: 2.8034 - val_acc: 0.7362\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7598 - acc: 0.7533 - val_loss: 2.5135 - val_acc: 0.7563\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6068 - acc: 0.7683 - val_loss: 2.7710 - val_acc: 0.7400\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7372 - acc: 0.7519 - val_loss: 2.4854 - val_acc: 0.7550\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8301 - acc: 0.7464 - val_loss: 2.9457 - val_acc: 0.7275\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7917 - acc: 0.7425 - val_loss: 2.7317 - val_acc: 0.7419\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6580 - acc: 0.7577 - val_loss: 2.5051 - val_acc: 0.7487\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6883 - acc: 0.7609 - val_loss: 2.6120 - val_acc: 0.7350\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7256 - acc: 0.7541 - val_loss: 2.6751 - val_acc: 0.7300\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7530 - acc: 0.7520 - val_loss: 2.5658 - val_acc: 0.7481\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7316 - acc: 0.7537 - val_loss: 2.7692 - val_acc: 0.7312\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7975 - acc: 0.7453 - val_loss: 2.5581 - val_acc: 0.7406\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7660 - acc: 0.7498 - val_loss: 2.3886 - val_acc: 0.7638\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8865 - acc: 0.7348 - val_loss: 2.5588 - val_acc: 0.7538\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8806 - acc: 0.7331 - val_loss: 2.6795 - val_acc: 0.7381\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6216 - acc: 0.7716 - val_loss: 2.4436 - val_acc: 0.7538\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7563 - acc: 0.7508 - val_loss: 2.3635 - val_acc: 0.7669\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6131 - acc: 0.7642 - val_loss: 2.5539 - val_acc: 0.7575\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6471 - acc: 0.7687 - val_loss: 2.4198 - val_acc: 0.7688\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7695 - acc: 0.7450 - val_loss: 2.6147 - val_acc: 0.7412\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7641 - acc: 0.7456 - val_loss: 2.5962 - val_acc: 0.7425\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7584 - acc: 0.7469 - val_loss: 2.7372 - val_acc: 0.7319\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7087 - acc: 0.7538 - val_loss: 2.9179 - val_acc: 0.7200\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7458 - acc: 0.7544 - val_loss: 2.5652 - val_acc: 0.7538\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8750 - acc: 0.7352 - val_loss: 2.5736 - val_acc: 0.7431\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7162 - acc: 0.7566 - val_loss: 2.4445 - val_acc: 0.7538\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7334 - acc: 0.7534 - val_loss: 2.5794 - val_acc: 0.7425\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7008 - acc: 0.7592 - val_loss: 2.5855 - val_acc: 0.7462\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.6781 - acc: 0.7594 - val_loss: 2.2956 - val_acc: 0.7669\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7211 - acc: 0.7530 - val_loss: 2.4894 - val_acc: 0.7525\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7360 - acc: 0.7481 - val_loss: 2.5127 - val_acc: 0.7462\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7808 - acc: 0.7467 - val_loss: 2.6405 - val_acc: 0.7381\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7386 - acc: 0.7534 - val_loss: 2.3173 - val_acc: 0.7669\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6633 - acc: 0.7578 - val_loss: 2.3304 - val_acc: 0.7688\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6951 - acc: 0.7558 - val_loss: 2.7340 - val_acc: 0.7356\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6777 - acc: 0.7598 - val_loss: 2.8470 - val_acc: 0.7262\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6993 - acc: 0.7564 - val_loss: 2.4552 - val_acc: 0.7619\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8164 - acc: 0.7389 - val_loss: 2.9109 - val_acc: 0.7175\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7174 - acc: 0.7541 - val_loss: 2.4672 - val_acc: 0.7500\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6424 - acc: 0.7655 - val_loss: 2.6220 - val_acc: 0.7450\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8165 - acc: 0.7420 - val_loss: 2.2577 - val_acc: 0.7706\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7508 - acc: 0.7472 - val_loss: 2.5173 - val_acc: 0.7506\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7384 - acc: 0.7494 - val_loss: 2.7824 - val_acc: 0.7306\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.5969 - acc: 0.7706 - val_loss: 2.4422 - val_acc: 0.7588\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7491 - acc: 0.7498 - val_loss: 2.6995 - val_acc: 0.7394\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8553 - acc: 0.7383 - val_loss: 2.7484 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6956 - acc: 0.7608 - val_loss: 2.3250 - val_acc: 0.7619\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7125 - acc: 0.7534 - val_loss: 2.7320 - val_acc: 0.7337\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6099 - acc: 0.7684 - val_loss: 2.8757 - val_acc: 0.7237\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6366 - acc: 0.7653 - val_loss: 2.5782 - val_acc: 0.7400\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6313 - acc: 0.7638 - val_loss: 2.5396 - val_acc: 0.7594\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6781 - acc: 0.7564 - val_loss: 2.5953 - val_acc: 0.7450\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.6431 - acc: 0.7625 - val_loss: 2.6277 - val_acc: 0.7444\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6780 - acc: 0.7572 - val_loss: 2.5982 - val_acc: 0.7412\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7718 - acc: 0.7472 - val_loss: 2.5564 - val_acc: 0.7481\n",
      "Epoch 1310/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7006 - acc: 0.7569 - val_loss: 2.5962 - val_acc: 0.7425\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8884 - acc: 0.7359 - val_loss: 2.2141 - val_acc: 0.7725\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6215 - acc: 0.7652 - val_loss: 3.1816 - val_acc: 0.7031\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7197 - acc: 0.7542 - val_loss: 2.7167 - val_acc: 0.7356\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7113 - acc: 0.7555 - val_loss: 2.4075 - val_acc: 0.7606\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8227 - acc: 0.7383 - val_loss: 2.7932 - val_acc: 0.7231\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8279 - acc: 0.7409 - val_loss: 2.5147 - val_acc: 0.7487\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8127 - acc: 0.7397 - val_loss: 2.6411 - val_acc: 0.7387\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6628 - acc: 0.7595 - val_loss: 2.6456 - val_acc: 0.7337\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6461 - acc: 0.7614 - val_loss: 2.8511 - val_acc: 0.7256\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6479 - acc: 0.7623 - val_loss: 2.4286 - val_acc: 0.7675\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6533 - acc: 0.7625 - val_loss: 2.4844 - val_acc: 0.7500\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6833 - acc: 0.7611 - val_loss: 3.1300 - val_acc: 0.7006\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6767 - acc: 0.7586 - val_loss: 2.6651 - val_acc: 0.7437\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6927 - acc: 0.7600 - val_loss: 2.5950 - val_acc: 0.7319\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7210 - acc: 0.7520 - val_loss: 2.2498 - val_acc: 0.7681\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7155 - acc: 0.7563 - val_loss: 2.6351 - val_acc: 0.7412\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7703 - acc: 0.7572 - val_loss: 2.2185 - val_acc: 0.7700\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6520 - acc: 0.7633 - val_loss: 2.4737 - val_acc: 0.7563\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7127 - acc: 0.7533 - val_loss: 2.6767 - val_acc: 0.7356\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7164 - acc: 0.7539 - val_loss: 2.8040 - val_acc: 0.7344\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8263 - acc: 0.7406 - val_loss: 2.4090 - val_acc: 0.7550\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7785 - acc: 0.7444 - val_loss: 2.3634 - val_acc: 0.7663\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7010 - acc: 0.7527 - val_loss: 2.6986 - val_acc: 0.7444\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8172 - acc: 0.7427 - val_loss: 2.5605 - val_acc: 0.7538\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6529 - acc: 0.7608 - val_loss: 2.6464 - val_acc: 0.7375\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6269 - acc: 0.7622 - val_loss: 2.4633 - val_acc: 0.7588\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7000 - acc: 0.7572 - val_loss: 2.3494 - val_acc: 0.7669\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8651 - acc: 0.7381 - val_loss: 2.5339 - val_acc: 0.7431\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7807 - acc: 0.7453 - val_loss: 2.2073 - val_acc: 0.7675\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7206 - acc: 0.7494 - val_loss: 2.4810 - val_acc: 0.7550\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6371 - acc: 0.7642 - val_loss: 2.7488 - val_acc: 0.7262\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7886 - acc: 0.7447 - val_loss: 2.3137 - val_acc: 0.7619\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6713 - acc: 0.7603 - val_loss: 2.7382 - val_acc: 0.7356\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7478 - acc: 0.7502 - val_loss: 2.6305 - val_acc: 0.7375\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6545 - acc: 0.7633 - val_loss: 2.4435 - val_acc: 0.7600\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6588 - acc: 0.7616 - val_loss: 2.4575 - val_acc: 0.7356\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7431 - acc: 0.7500 - val_loss: 2.9100 - val_acc: 0.7256\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6925 - acc: 0.7580 - val_loss: 2.2665 - val_acc: 0.7631\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9181 - acc: 0.7325 - val_loss: 2.5701 - val_acc: 0.7456\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7534 - acc: 0.7523 - val_loss: 2.5674 - val_acc: 0.7350\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8989 - acc: 0.7289 - val_loss: 2.5078 - val_acc: 0.7544\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7667 - acc: 0.7441 - val_loss: 2.6806 - val_acc: 0.7394\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7242 - acc: 0.7522 - val_loss: 2.4366 - val_acc: 0.7606\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6977 - acc: 0.7547 - val_loss: 2.1920 - val_acc: 0.7738\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7526 - acc: 0.7503 - val_loss: 2.4565 - val_acc: 0.7581\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7526 - acc: 0.7495 - val_loss: 2.1171 - val_acc: 0.7763\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7684 - acc: 0.7447 - val_loss: 2.5873 - val_acc: 0.7494\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8099 - acc: 0.7414 - val_loss: 2.5764 - val_acc: 0.7344\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7340 - acc: 0.7489 - val_loss: 2.6813 - val_acc: 0.7300\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7657 - acc: 0.7488 - val_loss: 2.5333 - val_acc: 0.7481\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7039 - acc: 0.7542 - val_loss: 2.5287 - val_acc: 0.7481\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.5789 - acc: 0.7737 - val_loss: 2.3153 - val_acc: 0.7606\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7704 - acc: 0.7433 - val_loss: 2.5413 - val_acc: 0.7344\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8335 - acc: 0.7378 - val_loss: 2.4745 - val_acc: 0.7519\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7352 - acc: 0.7528 - val_loss: 2.3198 - val_acc: 0.7638\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.5734 - acc: 0.7731 - val_loss: 2.4529 - val_acc: 0.7513\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7033 - acc: 0.7520 - val_loss: 2.5677 - val_acc: 0.7362\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6643 - acc: 0.7617 - val_loss: 2.7439 - val_acc: 0.7188\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7011 - acc: 0.7520 - val_loss: 2.6328 - val_acc: 0.7369\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6847 - acc: 0.7570 - val_loss: 2.4802 - val_acc: 0.7469\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7193 - acc: 0.7514 - val_loss: 2.5172 - val_acc: 0.7412\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7538 - acc: 0.7486 - val_loss: 2.2831 - val_acc: 0.7619\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7099 - acc: 0.7578 - val_loss: 2.6320 - val_acc: 0.7387\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6263 - acc: 0.7631 - val_loss: 2.3444 - val_acc: 0.7663\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8279 - acc: 0.7403 - val_loss: 2.6544 - val_acc: 0.7375\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6459 - acc: 0.7655 - val_loss: 2.8433 - val_acc: 0.7256\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6873 - acc: 0.7558 - val_loss: 2.4854 - val_acc: 0.7563\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6890 - acc: 0.7550 - val_loss: 2.4407 - val_acc: 0.7519\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6734 - acc: 0.7552 - val_loss: 2.1242 - val_acc: 0.7825\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7258 - acc: 0.7537 - val_loss: 2.4844 - val_acc: 0.7563\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7139 - acc: 0.7500 - val_loss: 2.7567 - val_acc: 0.7300\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6938 - acc: 0.7548 - val_loss: 2.7987 - val_acc: 0.7175\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6863 - acc: 0.7578 - val_loss: 2.6160 - val_acc: 0.7469\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7786 - acc: 0.7433 - val_loss: 2.6697 - val_acc: 0.7406\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7168 - acc: 0.7547 - val_loss: 2.0492 - val_acc: 0.7812\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7123 - acc: 0.7566 - val_loss: 2.3899 - val_acc: 0.7519\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6331 - acc: 0.7617 - val_loss: 2.3906 - val_acc: 0.7550\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6141 - acc: 0.7675 - val_loss: 2.5405 - val_acc: 0.7375\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6414 - acc: 0.7622 - val_loss: 2.3712 - val_acc: 0.7563\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7686 - acc: 0.7456 - val_loss: 2.6259 - val_acc: 0.7475\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7141 - acc: 0.7512 - val_loss: 2.5953 - val_acc: 0.7344\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7085 - acc: 0.7506 - val_loss: 2.6497 - val_acc: 0.7337\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6706 - acc: 0.7592 - val_loss: 2.4341 - val_acc: 0.7563\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6170 - acc: 0.7678 - val_loss: 2.6987 - val_acc: 0.7244\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7401 - acc: 0.7502 - val_loss: 2.7759 - val_acc: 0.7325\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7684 - acc: 0.7488 - val_loss: 2.1621 - val_acc: 0.7781\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8075 - acc: 0.7391 - val_loss: 2.2022 - val_acc: 0.7738\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6620 - acc: 0.7580 - val_loss: 2.7265 - val_acc: 0.7350\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8565 - acc: 0.7442 - val_loss: 2.5698 - val_acc: 0.7462\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7151 - acc: 0.7522 - val_loss: 2.4951 - val_acc: 0.7406\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7778 - acc: 0.7480 - val_loss: 2.7247 - val_acc: 0.7150\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6114 - acc: 0.7655 - val_loss: 2.4468 - val_acc: 0.7425\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6906 - acc: 0.7575 - val_loss: 2.6944 - val_acc: 0.7250\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7158 - acc: 0.7505 - val_loss: 2.6513 - val_acc: 0.7369\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6503 - acc: 0.7606 - val_loss: 2.3947 - val_acc: 0.7506\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6196 - acc: 0.7669 - val_loss: 2.7635 - val_acc: 0.7312\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6821 - acc: 0.7578 - val_loss: 2.7383 - val_acc: 0.7144\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6977 - acc: 0.7542 - val_loss: 2.6995 - val_acc: 0.7394\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7857 - acc: 0.7414 - val_loss: 2.5971 - val_acc: 0.7312\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7728 - acc: 0.7409 - val_loss: 2.5451 - val_acc: 0.7494\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7381 - acc: 0.7463 - val_loss: 2.5694 - val_acc: 0.7431\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7229 - acc: 0.7491 - val_loss: 2.2767 - val_acc: 0.7644\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7165 - acc: 0.7509 - val_loss: 2.4333 - val_acc: 0.7506\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7096 - acc: 0.7522 - val_loss: 2.4711 - val_acc: 0.7487\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6293 - acc: 0.7638 - val_loss: 2.6926 - val_acc: 0.7294\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8583 - acc: 0.7339 - val_loss: 2.3571 - val_acc: 0.7513\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6585 - acc: 0.7584 - val_loss: 2.3391 - val_acc: 0.7694\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7045 - acc: 0.7553 - val_loss: 2.6216 - val_acc: 0.7406\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6074 - acc: 0.7636 - val_loss: 2.6946 - val_acc: 0.7387\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7082 - acc: 0.7545 - val_loss: 2.3962 - val_acc: 0.7475\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7452 - acc: 0.7508 - val_loss: 2.7128 - val_acc: 0.7306\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6626 - acc: 0.7567 - val_loss: 2.4169 - val_acc: 0.7437\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6591 - acc: 0.7578 - val_loss: 2.2277 - val_acc: 0.7706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7526 - acc: 0.7464 - val_loss: 2.1726 - val_acc: 0.7669\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6574 - acc: 0.7630 - val_loss: 2.6807 - val_acc: 0.7362\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6867 - acc: 0.7558 - val_loss: 2.4970 - val_acc: 0.7375\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6722 - acc: 0.7584 - val_loss: 2.5420 - val_acc: 0.7513\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8108 - acc: 0.7366 - val_loss: 2.4180 - val_acc: 0.7406\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6885 - acc: 0.7537 - val_loss: 2.7468 - val_acc: 0.7237\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7596 - acc: 0.7462 - val_loss: 2.6101 - val_acc: 0.7356\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7259 - acc: 0.7452 - val_loss: 2.7975 - val_acc: 0.7169\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7724 - acc: 0.7444 - val_loss: 2.4055 - val_acc: 0.7387\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.5821 - acc: 0.7695 - val_loss: 2.4854 - val_acc: 0.7563\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7898 - acc: 0.7430 - val_loss: 2.4287 - val_acc: 0.7656\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7618 - acc: 0.7489 - val_loss: 2.4148 - val_acc: 0.7600\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7563 - acc: 0.7469 - val_loss: 2.4097 - val_acc: 0.7462\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7249 - acc: 0.7458 - val_loss: 2.3264 - val_acc: 0.7606\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6072 - acc: 0.7664 - val_loss: 2.4363 - val_acc: 0.7556\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6548 - acc: 0.7583 - val_loss: 2.5992 - val_acc: 0.7312\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6967 - acc: 0.7569 - val_loss: 2.7135 - val_acc: 0.7219\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7010 - acc: 0.7508 - val_loss: 2.5957 - val_acc: 0.7369\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6586 - acc: 0.7559 - val_loss: 2.5770 - val_acc: 0.7381\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6406 - acc: 0.7586 - val_loss: 2.3147 - val_acc: 0.7581\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6584 - acc: 0.7595 - val_loss: 2.4486 - val_acc: 0.7544\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7162 - acc: 0.7473 - val_loss: 2.3285 - val_acc: 0.7594\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8057 - acc: 0.7411 - val_loss: 2.4880 - val_acc: 0.7356\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6021 - acc: 0.7645 - val_loss: 2.5044 - val_acc: 0.7569\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8927 - acc: 0.7325 - val_loss: 2.3463 - val_acc: 0.7688\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6575 - acc: 0.7525 - val_loss: 2.5514 - val_acc: 0.7381\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7177 - acc: 0.7530 - val_loss: 2.5542 - val_acc: 0.7481\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6339 - acc: 0.7592 - val_loss: 2.8068 - val_acc: 0.7175\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7141 - acc: 0.7463 - val_loss: 2.3319 - val_acc: 0.7650\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6880 - acc: 0.7519 - val_loss: 2.4162 - val_acc: 0.7506\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6505 - acc: 0.7594 - val_loss: 2.7870 - val_acc: 0.7125\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7360 - acc: 0.7452 - val_loss: 2.4024 - val_acc: 0.7425\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7451 - acc: 0.7461 - val_loss: 2.1364 - val_acc: 0.7600\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6246 - acc: 0.7625 - val_loss: 2.5425 - val_acc: 0.7225\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7110 - acc: 0.7511 - val_loss: 2.4556 - val_acc: 0.7481\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6192 - acc: 0.7609 - val_loss: 2.5879 - val_acc: 0.7294\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.6886 - acc: 0.7519 - val_loss: 2.3352 - val_acc: 0.7588\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7363 - acc: 0.7484 - val_loss: 2.4398 - val_acc: 0.7506\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6292 - acc: 0.7581 - val_loss: 2.6885 - val_acc: 0.7188\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6449 - acc: 0.7572 - val_loss: 2.5213 - val_acc: 0.7487\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7567 - acc: 0.7450 - val_loss: 2.7017 - val_acc: 0.7400\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7584 - acc: 0.7441 - val_loss: 2.4985 - val_acc: 0.7369\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.6931 - acc: 0.7541 - val_loss: 2.4972 - val_acc: 0.7319\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6535 - acc: 0.7580 - val_loss: 2.4676 - val_acc: 0.7519\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6436 - acc: 0.7573 - val_loss: 2.4343 - val_acc: 0.7544\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6297 - acc: 0.7630 - val_loss: 2.7415 - val_acc: 0.7269\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7270 - acc: 0.7511 - val_loss: 2.4808 - val_acc: 0.7387\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6928 - acc: 0.7536 - val_loss: 2.7169 - val_acc: 0.7275\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8391 - acc: 0.7414 - val_loss: 2.8078 - val_acc: 0.7175\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6886 - acc: 0.7533 - val_loss: 2.5649 - val_acc: 0.7425\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6577 - acc: 0.7581 - val_loss: 2.4352 - val_acc: 0.7506\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6247 - acc: 0.7609 - val_loss: 2.7804 - val_acc: 0.7206\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6608 - acc: 0.7622 - val_loss: 2.3614 - val_acc: 0.7594\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.5798 - acc: 0.7678 - val_loss: 2.4529 - val_acc: 0.7487\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6395 - acc: 0.7600 - val_loss: 2.4819 - val_acc: 0.7419\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8465 - acc: 0.7327 - val_loss: 2.5087 - val_acc: 0.7481\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6303 - acc: 0.7577 - val_loss: 2.5244 - val_acc: 0.7481\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6954 - acc: 0.7573 - val_loss: 2.3920 - val_acc: 0.7487\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6138 - acc: 0.7625 - val_loss: 2.5907 - val_acc: 0.7287\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7541 - acc: 0.7466 - val_loss: 2.5767 - val_acc: 0.7319\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7123 - acc: 0.7470 - val_loss: 2.5098 - val_acc: 0.7375\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6921 - acc: 0.7534 - val_loss: 2.3003 - val_acc: 0.7631\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.5828 - acc: 0.7673 - val_loss: 2.4116 - val_acc: 0.7456\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6717 - acc: 0.7552 - val_loss: 2.6177 - val_acc: 0.7356\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9410 - acc: 0.7273 - val_loss: 2.4347 - val_acc: 0.7575\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6288 - acc: 0.7597 - val_loss: 2.8082 - val_acc: 0.7156\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6607 - acc: 0.7588 - val_loss: 2.5543 - val_acc: 0.7306\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7093 - acc: 0.7500 - val_loss: 2.5660 - val_acc: 0.7400\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7463 - acc: 0.7481 - val_loss: 3.0156 - val_acc: 0.7125\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6301 - acc: 0.7578 - val_loss: 2.3357 - val_acc: 0.7588\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7931 - acc: 0.7397 - val_loss: 2.0827 - val_acc: 0.7738\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6967 - acc: 0.7525 - val_loss: 2.4346 - val_acc: 0.7481\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7633 - acc: 0.7441 - val_loss: 2.3639 - val_acc: 0.7481\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7056 - acc: 0.7533 - val_loss: 2.4696 - val_acc: 0.7431\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7406 - acc: 0.7456 - val_loss: 2.5166 - val_acc: 0.7375\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7944 - acc: 0.7392 - val_loss: 2.2397 - val_acc: 0.7444\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7117 - acc: 0.7487 - val_loss: 2.7256 - val_acc: 0.7237\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8498 - acc: 0.7334 - val_loss: 2.6069 - val_acc: 0.7231\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6904 - acc: 0.7573 - val_loss: 2.5543 - val_acc: 0.7387\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6819 - acc: 0.7523 - val_loss: 2.3759 - val_acc: 0.7494\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.6329 - acc: 0.7656 - val_loss: 2.2589 - val_acc: 0.7538\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6741 - acc: 0.7575 - val_loss: 2.5669 - val_acc: 0.7312\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7801 - acc: 0.7464 - val_loss: 2.6209 - val_acc: 0.7275\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8432 - acc: 0.7472 - val_loss: 2.5748 - val_acc: 0.7475\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9522 - acc: 0.7577 - val_loss: 2.3065 - val_acc: 0.7581\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1118 - acc: 0.7330 - val_loss: 2.4620 - val_acc: 0.7419\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9827 - acc: 0.7547 - val_loss: 2.4309 - val_acc: 0.7487\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0067 - acc: 0.7548 - val_loss: 2.5129 - val_acc: 0.7469\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1793 - acc: 0.7586 - val_loss: 2.3836 - val_acc: 0.7581\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1722 - acc: 0.7613 - val_loss: 2.2807 - val_acc: 0.7644\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2649 - acc: 0.7439 - val_loss: 2.3962 - val_acc: 0.7456\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2657 - acc: 0.7431 - val_loss: 2.2960 - val_acc: 0.7588\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2382 - acc: 0.7484 - val_loss: 2.5236 - val_acc: 0.7412\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.2503 - acc: 0.7478 - val_loss: 2.3543 - val_acc: 0.7525\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1318 - acc: 0.7595 - val_loss: 2.4491 - val_acc: 0.7494\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0727 - acc: 0.7609 - val_loss: 2.5700 - val_acc: 0.7337\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1216 - acc: 0.7569 - val_loss: 2.6049 - val_acc: 0.7344\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0485 - acc: 0.7630 - val_loss: 2.4906 - val_acc: 0.7425\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.1255 - acc: 0.7616 - val_loss: 2.1196 - val_acc: 0.7731\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1974 - acc: 0.7461 - val_loss: 2.1720 - val_acc: 0.7681\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0796 - acc: 0.7617 - val_loss: 2.3382 - val_acc: 0.7594\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1875 - acc: 0.7528 - val_loss: 2.3363 - val_acc: 0.7625\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1048 - acc: 0.7609 - val_loss: 2.6249 - val_acc: 0.7356\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3993 - acc: 0.7336 - val_loss: 2.2705 - val_acc: 0.7556\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2577 - acc: 0.7433 - val_loss: 2.0451 - val_acc: 0.7906\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1676 - acc: 0.7505 - val_loss: 2.4491 - val_acc: 0.7538\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0759 - acc: 0.7658 - val_loss: 2.4786 - val_acc: 0.7444\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0808 - acc: 0.7616 - val_loss: 2.5609 - val_acc: 0.7337\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.2076 - acc: 0.7511 - val_loss: 2.1977 - val_acc: 0.7694\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1644 - acc: 0.7542 - val_loss: 2.3949 - val_acc: 0.7644\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9845 - acc: 0.7745 - val_loss: 2.6611 - val_acc: 0.7269\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1042 - acc: 0.7605 - val_loss: 2.0637 - val_acc: 0.7856\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0821 - acc: 0.7584 - val_loss: 2.2270 - val_acc: 0.7650\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1448 - acc: 0.7533 - val_loss: 2.3494 - val_acc: 0.7613\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1185 - acc: 0.7559 - val_loss: 2.4222 - val_acc: 0.7594\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0578 - acc: 0.7636 - val_loss: 2.6228 - val_acc: 0.7362\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1714 - acc: 0.7528 - val_loss: 2.6435 - val_acc: 0.7337\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1239 - acc: 0.7539 - val_loss: 2.4295 - val_acc: 0.7487\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0919 - acc: 0.7577 - val_loss: 2.4717 - val_acc: 0.7500\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1236 - acc: 0.7538 - val_loss: 2.2452 - val_acc: 0.7738\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1368 - acc: 0.7573 - val_loss: 2.5573 - val_acc: 0.7369\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0808 - acc: 0.7627 - val_loss: 2.3241 - val_acc: 0.7600\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1632 - acc: 0.7519 - val_loss: 2.1421 - val_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1155 - acc: 0.7563 - val_loss: 2.5067 - val_acc: 0.7425\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0392 - acc: 0.7645 - val_loss: 2.2198 - val_acc: 0.7675\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1050 - acc: 0.7580 - val_loss: 2.3443 - val_acc: 0.7563\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0634 - acc: 0.7638 - val_loss: 2.2568 - val_acc: 0.7688\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.1947 - acc: 0.7487 - val_loss: 2.4309 - val_acc: 0.7519\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1010 - acc: 0.7605 - val_loss: 2.6308 - val_acc: 0.7425\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3192 - acc: 0.7373 - val_loss: 2.3298 - val_acc: 0.7638\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9988 - acc: 0.7716 - val_loss: 2.6918 - val_acc: 0.7269\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1661 - acc: 0.7503 - val_loss: 2.4713 - val_acc: 0.7487\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.3039 - acc: 0.7381 - val_loss: 2.5578 - val_acc: 0.7400\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1777 - acc: 0.7511 - val_loss: 2.2779 - val_acc: 0.7694\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.1866 - acc: 0.7498 - val_loss: 2.5965 - val_acc: 0.7513\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0990 - acc: 0.7575 - val_loss: 2.2094 - val_acc: 0.7681\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0766 - acc: 0.7580 - val_loss: 2.5669 - val_acc: 0.7444\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2248 - acc: 0.7433 - val_loss: 2.6626 - val_acc: 0.7325\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1656 - acc: 0.7486 - val_loss: 2.5857 - val_acc: 0.7350\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9977 - acc: 0.7694 - val_loss: 2.3056 - val_acc: 0.7619\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0602 - acc: 0.7606 - val_loss: 2.3753 - val_acc: 0.7581\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9878 - acc: 0.7681 - val_loss: 2.5429 - val_acc: 0.7412\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.2317 - acc: 0.7428 - val_loss: 2.3667 - val_acc: 0.7631\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1772 - acc: 0.7438 - val_loss: 2.3759 - val_acc: 0.7556\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1571 - acc: 0.7506 - val_loss: 2.3042 - val_acc: 0.7606\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1768 - acc: 0.7484 - val_loss: 2.5655 - val_acc: 0.7387\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0835 - acc: 0.7533 - val_loss: 2.4600 - val_acc: 0.7481\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1901 - acc: 0.7441 - val_loss: 2.5512 - val_acc: 0.7381\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1391 - acc: 0.7467 - val_loss: 2.4480 - val_acc: 0.7506\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0673 - acc: 0.7563 - val_loss: 2.4247 - val_acc: 0.7487\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0948 - acc: 0.7522 - val_loss: 2.5672 - val_acc: 0.7362\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0878 - acc: 0.7541 - val_loss: 2.5779 - val_acc: 0.7319\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0309 - acc: 0.7620 - val_loss: 2.3130 - val_acc: 0.7619\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.1287 - acc: 0.7512 - val_loss: 2.6483 - val_acc: 0.7244\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8803 - acc: 0.7731 - val_loss: 2.3777 - val_acc: 0.7487\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0728 - acc: 0.7553 - val_loss: 2.6043 - val_acc: 0.7437\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0298 - acc: 0.7581 - val_loss: 2.4032 - val_acc: 0.7487\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9815 - acc: 0.7619 - val_loss: 2.3381 - val_acc: 0.7606\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0297 - acc: 0.7523 - val_loss: 2.3911 - val_acc: 0.7569\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9311 - acc: 0.7692 - val_loss: 2.4574 - val_acc: 0.7525\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0498 - acc: 0.7553 - val_loss: 2.4203 - val_acc: 0.7513\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 2.0035 - acc: 0.7566 - val_loss: 2.4840 - val_acc: 0.7444\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1052 - acc: 0.7480 - val_loss: 2.5012 - val_acc: 0.7513\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0254 - acc: 0.7569 - val_loss: 2.4688 - val_acc: 0.7450\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8845 - acc: 0.7719 - val_loss: 1.9341 - val_acc: 0.7969\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0308 - acc: 0.7550 - val_loss: 2.5607 - val_acc: 0.7362\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0473 - acc: 0.7488 - val_loss: 2.2817 - val_acc: 0.7613\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0398 - acc: 0.7600 - val_loss: 2.3129 - val_acc: 0.7569\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9721 - acc: 0.7608 - val_loss: 2.3787 - val_acc: 0.7475\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0270 - acc: 0.7523 - val_loss: 2.5128 - val_acc: 0.7544\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0081 - acc: 0.7564 - val_loss: 2.5317 - val_acc: 0.7325\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0384 - acc: 0.7542 - val_loss: 2.4116 - val_acc: 0.7531\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9481 - acc: 0.7603 - val_loss: 2.2122 - val_acc: 0.7638\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0269 - acc: 0.7522 - val_loss: 2.2312 - val_acc: 0.7663\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9912 - acc: 0.7572 - val_loss: 2.4932 - val_acc: 0.7469\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9260 - acc: 0.7598 - val_loss: 2.5263 - val_acc: 0.7450\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0707 - acc: 0.7528 - val_loss: 2.4338 - val_acc: 0.7387\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9616 - acc: 0.7609 - val_loss: 2.3545 - val_acc: 0.7644\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9747 - acc: 0.7534 - val_loss: 2.1737 - val_acc: 0.7744\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0471 - acc: 0.7509 - val_loss: 1.9798 - val_acc: 0.7969\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9213 - acc: 0.7642 - val_loss: 2.3597 - val_acc: 0.7556\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9558 - acc: 0.7645 - val_loss: 2.4511 - val_acc: 0.7500\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9836 - acc: 0.7567 - val_loss: 2.5079 - val_acc: 0.7444\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9632 - acc: 0.7553 - val_loss: 2.4480 - val_acc: 0.7437\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8953 - acc: 0.7647 - val_loss: 2.3820 - val_acc: 0.7569\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0580 - acc: 0.7484 - val_loss: 2.4570 - val_acc: 0.7450\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9030 - acc: 0.7647 - val_loss: 2.3389 - val_acc: 0.7663\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8773 - acc: 0.7677 - val_loss: 2.7085 - val_acc: 0.7231\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0300 - acc: 0.7514 - val_loss: 2.3734 - val_acc: 0.7631\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9538 - acc: 0.7620 - val_loss: 2.5721 - val_acc: 0.7450\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0125 - acc: 0.7516 - val_loss: 2.4820 - val_acc: 0.7381\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0486 - acc: 0.7486 - val_loss: 2.8917 - val_acc: 0.7169\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0393 - acc: 0.7519 - val_loss: 2.5303 - val_acc: 0.7444\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8634 - acc: 0.7666 - val_loss: 2.2432 - val_acc: 0.7500\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0552 - acc: 0.7452 - val_loss: 2.5720 - val_acc: 0.7362\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0319 - acc: 0.7477 - val_loss: 2.4770 - val_acc: 0.7475\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0918 - acc: 0.7372 - val_loss: 2.6449 - val_acc: 0.7362\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0268 - acc: 0.7495 - val_loss: 2.1316 - val_acc: 0.7756\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8546 - acc: 0.7644 - val_loss: 2.4869 - val_acc: 0.7419\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0185 - acc: 0.7473 - val_loss: 2.4390 - val_acc: 0.7437\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9239 - acc: 0.7583 - val_loss: 2.2689 - val_acc: 0.7675\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0292 - acc: 0.7439 - val_loss: 2.5328 - val_acc: 0.7387\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9945 - acc: 0.7558 - val_loss: 2.1189 - val_acc: 0.7806\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0633 - acc: 0.7547 - val_loss: 2.3264 - val_acc: 0.7575\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0747 - acc: 0.7494 - val_loss: 2.2821 - val_acc: 0.7606\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9740 - acc: 0.7627 - val_loss: 2.2611 - val_acc: 0.7650\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0230 - acc: 0.7556 - val_loss: 2.1107 - val_acc: 0.7856\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1751 - acc: 0.7392 - val_loss: 2.4064 - val_acc: 0.7331\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9594 - acc: 0.7638 - val_loss: 2.7269 - val_acc: 0.7144\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0087 - acc: 0.7567 - val_loss: 2.4093 - val_acc: 0.7494\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0054 - acc: 0.7583 - val_loss: 2.5943 - val_acc: 0.7400\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0389 - acc: 0.7553 - val_loss: 2.5260 - val_acc: 0.7387\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0152 - acc: 0.7536 - val_loss: 2.4476 - val_acc: 0.7475\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.1548 - acc: 0.7413 - val_loss: 2.1937 - val_acc: 0.7688\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9694 - acc: 0.7583 - val_loss: 2.4384 - val_acc: 0.7456\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9937 - acc: 0.7573 - val_loss: 2.1406 - val_acc: 0.7744\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9747 - acc: 0.7559 - val_loss: 2.1268 - val_acc: 0.7781\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9719 - acc: 0.7620 - val_loss: 2.4714 - val_acc: 0.7450\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0693 - acc: 0.7475 - val_loss: 2.3908 - val_acc: 0.7594\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0851 - acc: 0.7417 - val_loss: 2.6976 - val_acc: 0.7275\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9294 - acc: 0.7617 - val_loss: 2.3443 - val_acc: 0.7513\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1345 - acc: 0.7398 - val_loss: 2.4875 - val_acc: 0.7387\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.9316 - acc: 0.7577 - val_loss: 2.1765 - val_acc: 0.7688\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9084 - acc: 0.7666 - val_loss: 2.4452 - val_acc: 0.7487\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1112 - acc: 0.7403 - val_loss: 2.1782 - val_acc: 0.7700\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.1125 - acc: 0.7406 - val_loss: 2.6772 - val_acc: 0.7225\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9956 - acc: 0.7534 - val_loss: 2.2656 - val_acc: 0.7669\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.9973 - acc: 0.7489 - val_loss: 2.5218 - val_acc: 0.7400\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9647 - acc: 0.7531 - val_loss: 2.3293 - val_acc: 0.7519\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0040 - acc: 0.7545 - val_loss: 2.3457 - val_acc: 0.7469\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9076 - acc: 0.7642 - val_loss: 2.5197 - val_acc: 0.7494\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 2.0523 - acc: 0.7475 - val_loss: 2.8904 - val_acc: 0.7094\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9265 - acc: 0.7620 - val_loss: 2.3189 - val_acc: 0.7706\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8282 - acc: 0.7658 - val_loss: 2.3899 - val_acc: 0.7456\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9218 - acc: 0.7586 - val_loss: 2.5399 - val_acc: 0.7344\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0632 - acc: 0.7422 - val_loss: 2.4780 - val_acc: 0.7444\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9270 - acc: 0.7559 - val_loss: 2.3972 - val_acc: 0.7619\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0406 - acc: 0.7466 - val_loss: 2.2740 - val_acc: 0.7563\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9926 - acc: 0.7480 - val_loss: 2.6278 - val_acc: 0.7300\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8789 - acc: 0.7614 - val_loss: 2.3287 - val_acc: 0.7556\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9997 - acc: 0.7498 - val_loss: 2.1616 - val_acc: 0.7744\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8400 - acc: 0.7717 - val_loss: 2.2868 - val_acc: 0.7650\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9084 - acc: 0.7589 - val_loss: 2.6276 - val_acc: 0.7275\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8941 - acc: 0.7555 - val_loss: 2.6744 - val_acc: 0.7269\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9988 - acc: 0.7473 - val_loss: 2.3212 - val_acc: 0.7569\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8845 - acc: 0.7598 - val_loss: 2.3575 - val_acc: 0.7594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0741 - acc: 0.7372 - val_loss: 2.3596 - val_acc: 0.7569\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8542 - acc: 0.7669 - val_loss: 2.5617 - val_acc: 0.7231\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0082 - acc: 0.7480 - val_loss: 2.6115 - val_acc: 0.7412\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9218 - acc: 0.7566 - val_loss: 2.4776 - val_acc: 0.7487\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9731 - acc: 0.7498 - val_loss: 2.3919 - val_acc: 0.7513\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9073 - acc: 0.7511 - val_loss: 2.3982 - val_acc: 0.7519\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9276 - acc: 0.7544 - val_loss: 2.5489 - val_acc: 0.7356\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0128 - acc: 0.7423 - val_loss: 2.3263 - val_acc: 0.7594\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7621 - acc: 0.7739 - val_loss: 2.3603 - val_acc: 0.7613\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9012 - acc: 0.7572 - val_loss: 2.4166 - val_acc: 0.7481\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0216 - acc: 0.7394 - val_loss: 2.6155 - val_acc: 0.7350\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9455 - acc: 0.7498 - val_loss: 2.4987 - val_acc: 0.7456\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9216 - acc: 0.7545 - val_loss: 2.4830 - val_acc: 0.7519\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 2.0199 - acc: 0.7433 - val_loss: 2.1419 - val_acc: 0.7619\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9587 - acc: 0.7514 - val_loss: 2.4199 - val_acc: 0.7462\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9709 - acc: 0.7503 - val_loss: 2.1843 - val_acc: 0.7725\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9963 - acc: 0.7467 - val_loss: 2.4129 - val_acc: 0.7569\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8708 - acc: 0.7587 - val_loss: 2.5773 - val_acc: 0.7369\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8014 - acc: 0.7664 - val_loss: 2.3103 - val_acc: 0.7525\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9081 - acc: 0.7569 - val_loss: 2.3181 - val_acc: 0.7494\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8576 - acc: 0.7578 - val_loss: 2.4399 - val_acc: 0.7481\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8507 - acc: 0.7578 - val_loss: 2.4851 - val_acc: 0.7381\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9095 - acc: 0.7480 - val_loss: 2.3327 - val_acc: 0.7606\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8851 - acc: 0.7556 - val_loss: 2.4248 - val_acc: 0.7550\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9078 - acc: 0.7569 - val_loss: 2.1815 - val_acc: 0.7650\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8714 - acc: 0.7547 - val_loss: 2.3344 - val_acc: 0.7631\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8164 - acc: 0.7653 - val_loss: 2.1842 - val_acc: 0.7731\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9600 - acc: 0.7480 - val_loss: 2.5730 - val_acc: 0.7356\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8912 - acc: 0.7525 - val_loss: 2.3008 - val_acc: 0.7575\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9478 - acc: 0.7505 - val_loss: 2.6388 - val_acc: 0.7163\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9608 - acc: 0.7508 - val_loss: 2.2634 - val_acc: 0.7656\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8489 - acc: 0.7630 - val_loss: 2.3253 - val_acc: 0.7531\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8021 - acc: 0.7666 - val_loss: 2.2999 - val_acc: 0.7631\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9913 - acc: 0.7483 - val_loss: 2.5724 - val_acc: 0.7350\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8860 - acc: 0.7587 - val_loss: 2.4957 - val_acc: 0.7375\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8703 - acc: 0.7600 - val_loss: 2.1810 - val_acc: 0.7694\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9884 - acc: 0.7491 - val_loss: 2.3089 - val_acc: 0.7581\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9542 - acc: 0.7517 - val_loss: 2.4202 - val_acc: 0.7531\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0482 - acc: 0.7380 - val_loss: 2.3121 - val_acc: 0.7594\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9983 - acc: 0.7466 - val_loss: 2.3281 - val_acc: 0.7550\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9332 - acc: 0.7530 - val_loss: 2.6417 - val_acc: 0.7319\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9660 - acc: 0.7534 - val_loss: 2.1270 - val_acc: 0.7669\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8826 - acc: 0.7541 - val_loss: 2.0738 - val_acc: 0.7781\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 2.0023 - acc: 0.7444 - val_loss: 2.7125 - val_acc: 0.7194\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0085 - acc: 0.7447 - val_loss: 2.1675 - val_acc: 0.7694\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9504 - acc: 0.7512 - val_loss: 2.3234 - val_acc: 0.7513\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8746 - acc: 0.7564 - val_loss: 2.1397 - val_acc: 0.7719\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8835 - acc: 0.7570 - val_loss: 2.3122 - val_acc: 0.7563\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9636 - acc: 0.7508 - val_loss: 2.4544 - val_acc: 0.7494\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7927 - acc: 0.7673 - val_loss: 2.2250 - val_acc: 0.7581\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9058 - acc: 0.7517 - val_loss: 2.2466 - val_acc: 0.7663\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8438 - acc: 0.7588 - val_loss: 2.1121 - val_acc: 0.7706\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7185 - acc: 0.7792 - val_loss: 2.1732 - val_acc: 0.7706\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8782 - acc: 0.7541 - val_loss: 2.4821 - val_acc: 0.7362\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8710 - acc: 0.7592 - val_loss: 2.3297 - val_acc: 0.7581\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7934 - acc: 0.7642 - val_loss: 2.2674 - val_acc: 0.7638\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8409 - acc: 0.7617 - val_loss: 2.1648 - val_acc: 0.7669\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8443 - acc: 0.7584 - val_loss: 2.4348 - val_acc: 0.7462\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7736 - acc: 0.7681 - val_loss: 2.3875 - val_acc: 0.7544\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8881 - acc: 0.7548 - val_loss: 2.1187 - val_acc: 0.7700\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 2.0121 - acc: 0.7417 - val_loss: 2.1220 - val_acc: 0.7719\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.8823 - acc: 0.7505 - val_loss: 2.4905 - val_acc: 0.7431\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8596 - acc: 0.7569 - val_loss: 2.2064 - val_acc: 0.7681\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8166 - acc: 0.7614 - val_loss: 2.1217 - val_acc: 0.7825\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9146 - acc: 0.7533 - val_loss: 2.2988 - val_acc: 0.7606\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8270 - acc: 0.7575 - val_loss: 2.3930 - val_acc: 0.7487\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8874 - acc: 0.7517 - val_loss: 2.3822 - val_acc: 0.7494\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.9462 - acc: 0.7461 - val_loss: 2.4664 - val_acc: 0.7487\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9250 - acc: 0.7467 - val_loss: 2.3470 - val_acc: 0.7538\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8739 - acc: 0.7548 - val_loss: 2.4571 - val_acc: 0.7450\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8847 - acc: 0.7545 - val_loss: 2.2389 - val_acc: 0.7713\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7436 - acc: 0.7691 - val_loss: 2.4533 - val_acc: 0.7431\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7974 - acc: 0.7634 - val_loss: 2.2569 - val_acc: 0.7594\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7871 - acc: 0.7608 - val_loss: 2.1831 - val_acc: 0.7606\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9249 - acc: 0.7489 - val_loss: 2.2750 - val_acc: 0.7506\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8448 - acc: 0.7544 - val_loss: 2.3754 - val_acc: 0.7487\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8716 - acc: 0.7537 - val_loss: 2.2357 - val_acc: 0.7638\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8189 - acc: 0.7616 - val_loss: 2.2368 - val_acc: 0.7644\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8658 - acc: 0.7536 - val_loss: 2.1762 - val_acc: 0.7744\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8744 - acc: 0.7548 - val_loss: 2.1579 - val_acc: 0.7706\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9015 - acc: 0.7477 - val_loss: 2.4622 - val_acc: 0.7375\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.8413 - acc: 0.7545 - val_loss: 2.5149 - val_acc: 0.7356\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8178 - acc: 0.7586 - val_loss: 2.2262 - val_acc: 0.7594\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9049 - acc: 0.7497 - val_loss: 2.5787 - val_acc: 0.7394\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8922 - acc: 0.7514 - val_loss: 2.3925 - val_acc: 0.7419\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8285 - acc: 0.7525 - val_loss: 2.2472 - val_acc: 0.7744\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7263 - acc: 0.7686 - val_loss: 2.5892 - val_acc: 0.7231\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9139 - acc: 0.7461 - val_loss: 2.3060 - val_acc: 0.7469\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8028 - acc: 0.7564 - val_loss: 2.3865 - val_acc: 0.7494\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9376 - acc: 0.7386 - val_loss: 2.1338 - val_acc: 0.7750\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9473 - acc: 0.7406 - val_loss: 2.4319 - val_acc: 0.7375\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7823 - acc: 0.7663 - val_loss: 2.3506 - val_acc: 0.7538\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7939 - acc: 0.7547 - val_loss: 2.4038 - val_acc: 0.7481\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8815 - acc: 0.7436 - val_loss: 2.4145 - val_acc: 0.7500\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8908 - acc: 0.7427 - val_loss: 2.2566 - val_acc: 0.7631\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8109 - acc: 0.7538 - val_loss: 2.1449 - val_acc: 0.7750\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9721 - acc: 0.7383 - val_loss: 2.1944 - val_acc: 0.7619\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8780 - acc: 0.7425 - val_loss: 2.1619 - val_acc: 0.7756\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8797 - acc: 0.7477 - val_loss: 2.4770 - val_acc: 0.7369\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8832 - acc: 0.7412 - val_loss: 2.4965 - val_acc: 0.7375\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7857 - acc: 0.7589 - val_loss: 2.0924 - val_acc: 0.7831\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8869 - acc: 0.7478 - val_loss: 2.3777 - val_acc: 0.7475\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8158 - acc: 0.7541 - val_loss: 2.1901 - val_acc: 0.7619\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6924 - acc: 0.7687 - val_loss: 2.4443 - val_acc: 0.7494\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7646 - acc: 0.7580 - val_loss: 2.4188 - val_acc: 0.7563\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7722 - acc: 0.7597 - val_loss: 2.4216 - val_acc: 0.7506\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6947 - acc: 0.7680 - val_loss: 2.4002 - val_acc: 0.7500\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7667 - acc: 0.7575 - val_loss: 2.2797 - val_acc: 0.7544\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8881 - acc: 0.7400 - val_loss: 2.4595 - val_acc: 0.7319\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8886 - acc: 0.7430 - val_loss: 2.1745 - val_acc: 0.7663\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7949 - acc: 0.7537 - val_loss: 2.7528 - val_acc: 0.7188\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6925 - acc: 0.7675 - val_loss: 2.2182 - val_acc: 0.7469\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8387 - acc: 0.7453 - val_loss: 2.2752 - val_acc: 0.7563\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8753 - acc: 0.7456 - val_loss: 2.3163 - val_acc: 0.7544\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.9348 - acc: 0.7359 - val_loss: 2.3320 - val_acc: 0.7513\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8455 - acc: 0.7509 - val_loss: 2.4801 - val_acc: 0.7300\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 2.0043 - acc: 0.7234 - val_loss: 2.1506 - val_acc: 0.7694\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8365 - acc: 0.7467 - val_loss: 2.1671 - val_acc: 0.7656\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.8238 - acc: 0.7498 - val_loss: 2.6079 - val_acc: 0.7269\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7578 - acc: 0.7598 - val_loss: 2.5141 - val_acc: 0.7256\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7917 - acc: 0.7514 - val_loss: 2.0796 - val_acc: 0.7738\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8704 - acc: 0.7425 - val_loss: 2.2440 - val_acc: 0.7644\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7366 - acc: 0.7616 - val_loss: 2.0177 - val_acc: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8085 - acc: 0.7544 - val_loss: 2.4026 - val_acc: 0.7425\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8057 - acc: 0.7555 - val_loss: 2.6478 - val_acc: 0.7375\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7335 - acc: 0.7652 - val_loss: 2.4922 - val_acc: 0.7350\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7854 - acc: 0.7527 - val_loss: 2.3186 - val_acc: 0.7556\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8943 - acc: 0.7406 - val_loss: 2.5648 - val_acc: 0.7294\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7054 - acc: 0.7681 - val_loss: 2.1355 - val_acc: 0.7756\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7559 - acc: 0.7575 - val_loss: 2.4234 - val_acc: 0.7462\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7784 - acc: 0.7558 - val_loss: 2.2481 - val_acc: 0.7706\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8030 - acc: 0.7541 - val_loss: 2.5576 - val_acc: 0.7319\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8032 - acc: 0.7533 - val_loss: 2.3273 - val_acc: 0.7600\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7672 - acc: 0.7603 - val_loss: 2.5731 - val_acc: 0.7356\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7925 - acc: 0.7581 - val_loss: 2.4832 - val_acc: 0.7462\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7682 - acc: 0.7586 - val_loss: 2.1289 - val_acc: 0.7688\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7510 - acc: 0.7584 - val_loss: 2.3872 - val_acc: 0.7531\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6939 - acc: 0.7664 - val_loss: 2.6069 - val_acc: 0.7356\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7472 - acc: 0.7641 - val_loss: 2.2945 - val_acc: 0.7563\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7456 - acc: 0.7602 - val_loss: 2.4312 - val_acc: 0.7431\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8334 - acc: 0.7500 - val_loss: 2.2850 - val_acc: 0.7588\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7385 - acc: 0.7617 - val_loss: 2.4047 - val_acc: 0.7419\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8041 - acc: 0.7570 - val_loss: 2.4358 - val_acc: 0.7400\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9947 - acc: 0.7319 - val_loss: 2.1844 - val_acc: 0.7775\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7456 - acc: 0.7602 - val_loss: 2.0751 - val_acc: 0.7887\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6722 - acc: 0.7716 - val_loss: 2.4820 - val_acc: 0.7444\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8525 - acc: 0.7514 - val_loss: 2.3107 - val_acc: 0.7569\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8664 - acc: 0.7461 - val_loss: 2.4545 - val_acc: 0.7437\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8033 - acc: 0.7536 - val_loss: 2.1879 - val_acc: 0.7719\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8356 - acc: 0.7475 - val_loss: 2.4997 - val_acc: 0.7481\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7522 - acc: 0.7609 - val_loss: 2.5903 - val_acc: 0.7431\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8838 - acc: 0.7461 - val_loss: 2.4604 - val_acc: 0.7531\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9240 - acc: 0.7478 - val_loss: 2.4151 - val_acc: 0.7506\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8381 - acc: 0.7500 - val_loss: 2.2755 - val_acc: 0.7606\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7902 - acc: 0.7588 - val_loss: 2.2170 - val_acc: 0.7669\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9223 - acc: 0.7417 - val_loss: 2.4258 - val_acc: 0.7575\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7359 - acc: 0.7633 - val_loss: 2.1671 - val_acc: 0.7731\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8534 - acc: 0.7513 - val_loss: 2.1151 - val_acc: 0.7800\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8309 - acc: 0.7547 - val_loss: 2.7182 - val_acc: 0.7300\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6996 - acc: 0.7662 - val_loss: 2.4314 - val_acc: 0.7475\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7862 - acc: 0.7558 - val_loss: 2.0644 - val_acc: 0.7819\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7756 - acc: 0.7584 - val_loss: 2.4038 - val_acc: 0.7500\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8236 - acc: 0.7511 - val_loss: 2.1950 - val_acc: 0.7706\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8004 - acc: 0.7519 - val_loss: 2.4950 - val_acc: 0.7425\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.6888 - acc: 0.7678 - val_loss: 2.3255 - val_acc: 0.7531\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8533 - acc: 0.7475 - val_loss: 2.2668 - val_acc: 0.7725\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8195 - acc: 0.7542 - val_loss: 2.4385 - val_acc: 0.7387\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8157 - acc: 0.7527 - val_loss: 2.3065 - val_acc: 0.7650\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7111 - acc: 0.7673 - val_loss: 2.5057 - val_acc: 0.7381\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7652 - acc: 0.7602 - val_loss: 2.2676 - val_acc: 0.7613\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7798 - acc: 0.7572 - val_loss: 2.4047 - val_acc: 0.7425\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7860 - acc: 0.7514 - val_loss: 2.5375 - val_acc: 0.7506\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8341 - acc: 0.7480 - val_loss: 2.5295 - val_acc: 0.7469\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7247 - acc: 0.7558 - val_loss: 2.4368 - val_acc: 0.7475\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.6827 - acc: 0.7647 - val_loss: 2.3608 - val_acc: 0.7500\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7525 - acc: 0.7581 - val_loss: 2.3004 - val_acc: 0.7663\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7923 - acc: 0.7517 - val_loss: 2.3207 - val_acc: 0.7638\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7671 - acc: 0.7570 - val_loss: 2.2603 - val_acc: 0.7613\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7818 - acc: 0.7566 - val_loss: 2.3884 - val_acc: 0.7613\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.6555 - acc: 0.7678 - val_loss: 2.2819 - val_acc: 0.7575\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.9772 - acc: 0.7317 - val_loss: 2.5694 - val_acc: 0.7444\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7357 - acc: 0.7580 - val_loss: 2.5668 - val_acc: 0.7337\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 17s 4s/step - loss: 1.7033 - acc: 0.7581 - val_loss: 2.4201 - val_acc: 0.7487\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.7351 - acc: 0.7572 - val_loss: 2.2778 - val_acc: 0.7594\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7733 - acc: 0.7548 - val_loss: 2.3322 - val_acc: 0.7663\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8730 - acc: 0.7419 - val_loss: 2.4397 - val_acc: 0.7450\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7304 - acc: 0.7605 - val_loss: 2.2968 - val_acc: 0.7494\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8952 - acc: 0.7427 - val_loss: 2.3882 - val_acc: 0.7525\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 16s 4s/step - loss: 1.8461 - acc: 0.7480 - val_loss: 2.4480 - val_acc: 0.7481\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8119 - acc: 0.7486 - val_loss: 2.3967 - val_acc: 0.7569\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7463 - acc: 0.7575 - val_loss: 2.3004 - val_acc: 0.7531\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 14s 4s/step - loss: 1.7875 - acc: 0.7505 - val_loss: 2.3689 - val_acc: 0.7494\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7992 - acc: 0.7480 - val_loss: 2.5930 - val_acc: 0.7406\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7695 - acc: 0.7539 - val_loss: 2.6717 - val_acc: 0.7219\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8311 - acc: 0.7477 - val_loss: 2.5345 - val_acc: 0.7387\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8004 - acc: 0.7527 - val_loss: 2.3253 - val_acc: 0.7581\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.9416 - acc: 0.7338 - val_loss: 2.3681 - val_acc: 0.7506\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7850 - acc: 0.7480 - val_loss: 2.3607 - val_acc: 0.7556\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.8160 - acc: 0.7486 - val_loss: 2.8440 - val_acc: 0.7019\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7077 - acc: 0.7612 - val_loss: 2.4220 - val_acc: 0.7431\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.9025 - acc: 0.7394 - val_loss: 2.6721 - val_acc: 0.7325\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 14s 3s/step - loss: 1.7743 - acc: 0.7509 - val_loss: 2.4708 - val_acc: 0.7494\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7246 - acc: 0.7586 - val_loss: 2.1970 - val_acc: 0.7713\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7473 - acc: 0.7577 - val_loss: 2.2593 - val_acc: 0.7644\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7923 - acc: 0.7514 - val_loss: 2.5562 - val_acc: 0.7369\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 15s 4s/step - loss: 1.7384 - acc: 0.7580 - val_loss: 1.9492 - val_acc: 0.7919\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6316 - acc: 0.7680 - val_loss: 2.2983 - val_acc: 0.7594\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6894 - acc: 0.7612 - val_loss: 2.5068 - val_acc: 0.7437\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8639 - acc: 0.7417 - val_loss: 2.4074 - val_acc: 0.7569\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7751 - acc: 0.7534 - val_loss: 2.8098 - val_acc: 0.7237\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7461 - acc: 0.7545 - val_loss: 2.3125 - val_acc: 0.7563\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8279 - acc: 0.7480 - val_loss: 2.3111 - val_acc: 0.7531\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6386 - acc: 0.7614 - val_loss: 2.5870 - val_acc: 0.7319\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6983 - acc: 0.7612 - val_loss: 2.3632 - val_acc: 0.7494\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7885 - acc: 0.7484 - val_loss: 2.7600 - val_acc: 0.7256\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6019 - acc: 0.7681 - val_loss: 2.8362 - val_acc: 0.7144\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8504 - acc: 0.7470 - val_loss: 2.8070 - val_acc: 0.7237\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7828 - acc: 0.7447 - val_loss: 2.4474 - val_acc: 0.7469\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7585 - acc: 0.7555 - val_loss: 2.1518 - val_acc: 0.7806\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7325 - acc: 0.7605 - val_loss: 2.4336 - val_acc: 0.7531\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7771 - acc: 0.7503 - val_loss: 2.5485 - val_acc: 0.7369\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7929 - acc: 0.7469 - val_loss: 2.4710 - val_acc: 0.7513\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7033 - acc: 0.7611 - val_loss: 2.0458 - val_acc: 0.7788\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6998 - acc: 0.7566 - val_loss: 2.7578 - val_acc: 0.7231\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8015 - acc: 0.7466 - val_loss: 2.7700 - val_acc: 0.7250\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8418 - acc: 0.7430 - val_loss: 2.4263 - val_acc: 0.7538\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7554 - acc: 0.7547 - val_loss: 2.6213 - val_acc: 0.7312\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7109 - acc: 0.7547 - val_loss: 2.6117 - val_acc: 0.7244\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6550 - acc: 0.7667 - val_loss: 2.4660 - val_acc: 0.7494\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6674 - acc: 0.7605 - val_loss: 2.3612 - val_acc: 0.7619\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7117 - acc: 0.7539 - val_loss: 2.4989 - val_acc: 0.7462\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6733 - acc: 0.7606 - val_loss: 2.5368 - val_acc: 0.7381\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6901 - acc: 0.7580 - val_loss: 2.2348 - val_acc: 0.7669\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7375 - acc: 0.7505 - val_loss: 2.3944 - val_acc: 0.7525\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7266 - acc: 0.7564 - val_loss: 2.3243 - val_acc: 0.7594\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8421 - acc: 0.7427 - val_loss: 2.3874 - val_acc: 0.7481\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7754 - acc: 0.7509 - val_loss: 2.7870 - val_acc: 0.7206\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.8066 - acc: 0.7481 - val_loss: 2.3849 - val_acc: 0.7588\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6626 - acc: 0.7617 - val_loss: 2.5190 - val_acc: 0.7344\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7832 - acc: 0.7445 - val_loss: 2.4335 - val_acc: 0.7538\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6416 - acc: 0.7647 - val_loss: 2.2281 - val_acc: 0.7638\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7333 - acc: 0.7534 - val_loss: 2.6041 - val_acc: 0.7394\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6238 - acc: 0.7658 - val_loss: 2.2900 - val_acc: 0.7606\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7767 - acc: 0.7467 - val_loss: 2.2937 - val_acc: 0.7581\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7492 - acc: 0.7527 - val_loss: 2.5420 - val_acc: 0.7469\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7187 - acc: 0.7517 - val_loss: 2.0366 - val_acc: 0.7900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8573 - acc: 0.7397 - val_loss: 2.2912 - val_acc: 0.7494\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7089 - acc: 0.7569 - val_loss: 2.4056 - val_acc: 0.7525\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8314 - acc: 0.7394 - val_loss: 2.5318 - val_acc: 0.7306\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7698 - acc: 0.7459 - val_loss: 2.0916 - val_acc: 0.7788\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7762 - acc: 0.7458 - val_loss: 2.4065 - val_acc: 0.7481\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7354 - acc: 0.7503 - val_loss: 2.7123 - val_acc: 0.7163\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7508 - acc: 0.7492 - val_loss: 2.6318 - val_acc: 0.7375\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7160 - acc: 0.7514 - val_loss: 2.3047 - val_acc: 0.7575\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6917 - acc: 0.7617 - val_loss: 2.7073 - val_acc: 0.7312\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6948 - acc: 0.7544 - val_loss: 2.8560 - val_acc: 0.7113\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6166 - acc: 0.7683 - val_loss: 2.3425 - val_acc: 0.7744\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6258 - acc: 0.7630 - val_loss: 2.4945 - val_acc: 0.7419\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7443 - acc: 0.7559 - val_loss: 2.7377 - val_acc: 0.7244\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7445 - acc: 0.7488 - val_loss: 2.4306 - val_acc: 0.7450\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6785 - acc: 0.7584 - val_loss: 2.5031 - val_acc: 0.7356\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.8506 - acc: 0.7425 - val_loss: 2.4325 - val_acc: 0.7406\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7239 - acc: 0.7539 - val_loss: 2.5946 - val_acc: 0.7275\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7844 - acc: 0.7467 - val_loss: 2.4816 - val_acc: 0.7487\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6592 - acc: 0.7609 - val_loss: 2.2962 - val_acc: 0.7519\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7270 - acc: 0.7550 - val_loss: 2.5723 - val_acc: 0.7312\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7209 - acc: 0.7547 - val_loss: 2.0440 - val_acc: 0.7856\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.8723 - acc: 0.7391 - val_loss: 2.2595 - val_acc: 0.7575\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7170 - acc: 0.7522 - val_loss: 2.6160 - val_acc: 0.7344\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7158 - acc: 0.7511 - val_loss: 2.1732 - val_acc: 0.7756\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.7205 - acc: 0.7580 - val_loss: 2.2821 - val_acc: 0.7625\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7716 - acc: 0.7441 - val_loss: 2.2796 - val_acc: 0.7688\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.8076 - acc: 0.7473 - val_loss: 2.1261 - val_acc: 0.7713\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.7128 - acc: 0.7581 - val_loss: 2.2615 - val_acc: 0.7594\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6551 - acc: 0.7623 - val_loss: 2.5120 - val_acc: 0.7419\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.8261 - acc: 0.7433 - val_loss: 2.3990 - val_acc: 0.7538\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7465 - acc: 0.7466 - val_loss: 2.5591 - val_acc: 0.7419\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6735 - acc: 0.7580 - val_loss: 2.5761 - val_acc: 0.7487\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7425 - acc: 0.7500 - val_loss: 2.5839 - val_acc: 0.7319\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.6556 - acc: 0.7631 - val_loss: 2.4006 - val_acc: 0.7506\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6890 - acc: 0.7603 - val_loss: 2.6083 - val_acc: 0.7400\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.7475 - acc: 0.7578 - val_loss: 2.2408 - val_acc: 0.7619\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.8077 - acc: 0.7430 - val_loss: 2.5938 - val_acc: 0.7387\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.6625 - acc: 0.7581 - val_loss: 2.5790 - val_acc: 0.7356\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.7168 - acc: 0.7484 - val_loss: 2.5684 - val_acc: 0.7506\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 12s 3s/step - loss: 1.6825 - acc: 0.7548 - val_loss: 2.4739 - val_acc: 0.7419\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6335 - acc: 0.7648 - val_loss: 2.6127 - val_acc: 0.7306\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7023 - acc: 0.7548 - val_loss: 2.5851 - val_acc: 0.7344\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7388 - acc: 0.7522 - val_loss: 2.5324 - val_acc: 0.7369\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.6742 - acc: 0.7586 - val_loss: 2.5160 - val_acc: 0.7425\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.7108 - acc: 0.7555 - val_loss: 2.5741 - val_acc: 0.7356\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6948 - acc: 0.7545 - val_loss: 2.6111 - val_acc: 0.7375\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7213 - acc: 0.7534 - val_loss: 2.5678 - val_acc: 0.7356\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7242 - acc: 0.7545 - val_loss: 2.5098 - val_acc: 0.7437\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6901 - acc: 0.7594 - val_loss: 2.4134 - val_acc: 0.7456\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6978 - acc: 0.7542 - val_loss: 2.7145 - val_acc: 0.7206\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6737 - acc: 0.7588 - val_loss: 2.0756 - val_acc: 0.7812\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6532 - acc: 0.7644 - val_loss: 2.4575 - val_acc: 0.7506\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6688 - acc: 0.7591 - val_loss: 2.6986 - val_acc: 0.7269\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.6130 - acc: 0.7687 - val_loss: 2.3605 - val_acc: 0.7563\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.5613 - acc: 0.7752 - val_loss: 2.5762 - val_acc: 0.7362\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6565 - acc: 0.7612 - val_loss: 2.6611 - val_acc: 0.7325\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7227 - acc: 0.7536 - val_loss: 2.2741 - val_acc: 0.7519\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7452 - acc: 0.7467 - val_loss: 2.7491 - val_acc: 0.7294\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6945 - acc: 0.7559 - val_loss: 2.5094 - val_acc: 0.7475\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6929 - acc: 0.7556 - val_loss: 2.6853 - val_acc: 0.7300\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7546 - acc: 0.7495 - val_loss: 2.6177 - val_acc: 0.7481\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.7193 - acc: 0.7562 - val_loss: 2.2114 - val_acc: 0.7638\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6694 - acc: 0.7600 - val_loss: 2.5797 - val_acc: 0.7362\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.6396 - acc: 0.7603 - val_loss: 2.4999 - val_acc: 0.7519\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7368 - acc: 0.7498 - val_loss: 2.3855 - val_acc: 0.7638\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.6974 - acc: 0.7528 - val_loss: 2.4911 - val_acc: 0.7569\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6726 - acc: 0.7562 - val_loss: 2.3555 - val_acc: 0.7656\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.5857 - acc: 0.7706 - val_loss: 2.7389 - val_acc: 0.7219\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7063 - acc: 0.7520 - val_loss: 2.3363 - val_acc: 0.7513\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7261 - acc: 0.7511 - val_loss: 2.2770 - val_acc: 0.7563\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.6898 - acc: 0.7509 - val_loss: 2.4976 - val_acc: 0.7475\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.6933 - acc: 0.7581 - val_loss: 2.3523 - val_acc: 0.7550\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7928 - acc: 0.7433 - val_loss: 2.3825 - val_acc: 0.7581\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 10s 2s/step - loss: 1.6246 - acc: 0.7670 - val_loss: 2.2116 - val_acc: 0.7800\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.7621 - acc: 0.7509 - val_loss: 2.5753 - val_acc: 0.7431\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7028 - acc: 0.7564 - val_loss: 2.4151 - val_acc: 0.7525\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7172 - acc: 0.7506 - val_loss: 2.1677 - val_acc: 0.7719\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6292 - acc: 0.7611 - val_loss: 2.5320 - val_acc: 0.7544\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7079 - acc: 0.7577 - val_loss: 2.1758 - val_acc: 0.7763\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.8121 - acc: 0.7361 - val_loss: 2.3663 - val_acc: 0.7538\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 10s 3s/step - loss: 1.7039 - acc: 0.7595 - val_loss: 2.5405 - val_acc: 0.7394\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7721 - acc: 0.7464 - val_loss: 2.4894 - val_acc: 0.7437\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7032 - acc: 0.7509 - val_loss: 2.6424 - val_acc: 0.7387\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.7543 - acc: 0.7425 - val_loss: 2.2606 - val_acc: 0.7588\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.6630 - acc: 0.7605 - val_loss: 2.4847 - val_acc: 0.7431\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(tdg, callbacks=[mc,tb], initial_epoch=0\n",
    "                           ,steps_per_epoch=train_steps_per_epoch\n",
    "                           ,validation_data=vdg\n",
    "                           ,validation_steps=val_steps_per_epoch\n",
    "                           ,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# acc = hist.history['acc']\n",
    "# loss = hist.history['loss']\n",
    "\n",
    "# # Create count of the number of epochs\n",
    "# epoch_count = range(1, len(acc) + 1)\n",
    "\n",
    "# # Visualize loss history\n",
    "# # plt.plot(epoch_count, acc, 'b-')\n",
    "# fig, ax = plt.subplots(ncols=2,sharex=True)\n",
    "# ax[0].plot(epoch_count, loss, 'r--')\n",
    "# ax[0].legend(['Loss'])\n",
    "# ax[0].set_xlabel('Epoch')\n",
    "# ax[0].set_ylabel('Loss')\n",
    "# ax[1].plot(epoch_count, acc, 'b-')\n",
    "# ax[1].legend(['Accuracy'])\n",
    "# ax[1].set_xlabel('Epoch')\n",
    "# ax[1].set_xlabel('Accuracy')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_res[['acc','val_acc']].plot()\n",
    "# hd_nlp = nlp(\"What is you name my name is Anthony Gonsalves What is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony GonsalvesWhat is you name my name is Anthony Gonsalves!\".lower())\n",
    "# len(hd_nlp[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaikh2/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<generator..., use_multiprocessing=True, steps=5)`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.463012361526489, 0.7322400093078614]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/dnf700_sa_sent_hd_word_gl.hdf5')\n",
    "model.evaluate_generator(test_dg,steps=5,pickle_safe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(test_dg)\n",
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['department of homeland security chairman officially indicts hillary clinton of treason',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'former nato chief: we need us as ‘world’s policeman’',\n",
       "       'clinton received debate questions week before debate',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'breaking: fraudulent clinton votes discovered by the tens of thousands',\n",
       "       \"doj's loretta lynch tried to squash comey's letter to congress\",\n",
       "       \"fantastic! trump's 7 point plan to reform healthcare begins with a bombshell! » 100percentfedup.com\",\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'hillary’s (islamic) america is already here where ‘muslim no-go zones’ are popping up all over michiganistan',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'isis leader calls for american muslim voters to support hillary clinton',\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'nsa whistleblower says dnc email hack was not by russia, but by us intelligence | alternative',\n",
       "       'he’s never sold an original painting until now…and this one’s going in the white house',\n",
       "       'president obama confirms he will refuse to leave office if trump is elected',\n",
       "       'wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news',\n",
       "       'isis leader calls for american muslim voters to support hillary clinton',\n",
       "       'department of homeland security chairman officially indicts hillary clinton of treason',\n",
       "       'wikileaks confirms hillary sold weapons to isis... then drops another bombshell! breaking news',\n",
       "       'us officials try to scare voters with terror threat',\n",
       "       'jill stein endorsed donald trump',\n",
       "       'pentagon officials furious after clinton announces us response time for nuclear launch during debate',\n",
       "       'wikileaks: hillary clinton knew saudi, qatar were funding isis – but still took their money for foundation',\n",
       "       'assange confirms: wikileaks didn’t get emails from russian govt',\n",
       "       'leaked 2013 trump tax return shows he paid over 40 million in taxes',\n",
       "       'hillary friend bribed fbi agent and his wife',\n",
       "       'trump accuses obama, hillary clinton of founding daesh',\n",
       "       '(video) female college students protesting because ‘trump is a rapist’',\n",
       "       'fbi director comey’s ‘leaked’ memo explains why he’s reopening the clinton email case',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       'hillary clinton used hand signals to rig debate?',\n",
       "       'breaking: fraudulent clinton votes discovered by the tens of thousands',\n",
       "       'former nato chief: we need us as ‘world’s policeman’',\n",
       "       'julian assange makes very suspect post election announcement, seeks pardon from trump',\n",
       "       \"clinton camp demands 'compliant citizenry' for master plan\",\n",
       "       'us officials see no link between trump and russia',\n",
       "       \"george soros: trump will win popular vote by a landslide but clinton victory a 'done deal'\",\n",
       "       'isis leader calls for american muslim voters to support hillary clinton',\n",
       "       'trump accuses obama, hillary clinton of founding daesh',\n",
       "       'fbi director received millions from clinton foundation, his brother’s law firm does clinton’s taxes',\n",
       "       'us officials try to scare voters with terror threat',\n",
       "       'clinton received debate questions week before debate',\n",
       "       \"physician confirms hillary clinton has parkinson's disease\",\n",
       "       'hillary sold weapons to isis, wikileaks confirms'], dtype='<U108')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'former nato chief: we need us as ‘world’s policeman’'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_idx = np.random.randint(0,50)\n",
    "display(x['headline'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.',\n",
       " 'Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.',\n",
       " 'Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.',\n",
       " 'This of course closely mirrors recent Pentagon talk of wars in the decades to come.',\n",
       " 'The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.',\n",
       " 'Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['sentences'][test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca1 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca2 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca3 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_vectors (InputLayer)   (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 16)      14416       sentence_vectors[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 16)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 500, 32)      1568        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500, 32)      128         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sa1 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa2 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa3 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa4 (SelfAttention)             [(None, 500, 32), (5 2377        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_headline_vector (InputLay (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500, 128)     0           sa1[0][0]                        \n",
      "                                                                 sa2[0][0]                        \n",
      "                                                                 sa3[0][0]                        \n",
      "                                                                 sa4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       input_headline_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 256)     98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 256)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 500, 256)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 256)       1024        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500, 256)     1024        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ca4 (CrossAttention)            [(None, 500, 256), ( 148033      batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 351,317\n",
      "Trainable params: 350,229\n",
      "Non-trainable params: 1,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(model.inputs,model.get_layer(name='ca1').output)\n",
    "model_2 = Model(model.inputs,model.get_layer(name='ca2').output)\n",
    "model_3 = Model(model.inputs,model.get_layer(name='ca3').output)\n",
    "model_4 = Model(model.inputs,model.get_layer(name='ca4').output)\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, b1, g1 = model_1.predict(x)\n",
    "_, b2, g2 = model_2.predict(x)\n",
    "_, b3, g3 = model_3.predict(x)\n",
    "_, b4, g4 = model_4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b1+b2+b3+b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 5, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_N = 5\n",
    "t = b[test_idx][0][:len(x['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101.703926"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "b[test_idx][0][:len(x['sentences'][test_idx])].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'former nato chief: we need us as ‘world’s policeman’'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the world’s policeman, but with a lot more upbeat of an attitude about it than one would generally see.',\n",
       " 'Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring  we need America as the world’s policeman,  adding that the US needs to restore international law and order through wars.',\n",
       " 'The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.',\n",
       " 'Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as very dangerous for the world.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x['headline'][test_idx])\n",
    "display(x['claims'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : This of course closely mirrors recent Pentagon talk of wars in the decades to come.\n",
      "4 : The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.\n",
      "2 : Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.\n",
      "5 : Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”\n",
      "1 : Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.\n"
     ]
    }
   ],
   "source": [
    "for s in t:\n",
    "    if s>=len(x['sentences'][test_idx]):continue\n",
    "    print(s,':',x['sentences'][test_idx][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101.703926"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x['sentences'][test_idx]))\n",
    "h_s_attended_vector = b[test_idx][0][:len(x['sentences'][test_idx])]\n",
    "h_s_attended_vector.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h_s_attended_vector = pd.DataFrame(h_s_attended_vector)\n",
    "\n",
    "\n",
    "xw = df_h_s_attended_vector.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "xw_scaled = min_max_scaler.fit_transform(xw)\n",
    "df_h_s_attended_vector = pd.DataFrame(xw_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4f7db306a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(2.0,7.0)})\n",
    "sns.heatmap(df_h_s_attended_vector, annot=True, cmap='YlGnBu', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa1 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa2 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa3 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_vectors (InputLayer (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 16)           14416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "sa4 (SelfAttention)          [(None, 500, 32), (500, 5 2377      \n",
      "=================================================================\n",
      "Total params: 18,489\n",
      "Trainable params: 18,425\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_s1 = Model(model.inputs,model.get_layer(name='sa1').output)\n",
    "model_s2 = Model(model.inputs,model.get_layer(name='sa2').output)\n",
    "model_s3 = Model(model.inputs,model.get_layer(name='sa3').output)\n",
    "model_s4 = Model(model.inputs,model.get_layer(name='sa4').output)\n",
    "model_s1.summary()\n",
    "model_s2.summary()\n",
    "model_s3.summary()\n",
    "model_s4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sb1, sg1 = model_s1.predict([x['sentence_vectors'],x['input_headline_vector']])\n",
    "_, sb2, sg2 = model_s2.predict(x)\n",
    "_, sb3, sg3 = model_s3.predict(x)\n",
    "_, sb4, sg4 = model_s4.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = sb1[test_idx]#+sb2[test_idx]+sb3[test_idx]+sb4[test_idx]\n",
    "sb = sb[:len(x['sentences'][test_idx]),:len(x['sentences'][test_idx])]\n",
    "\n",
    "sb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = pd.DataFrame(sb)\n",
    "\n",
    "\n",
    "# zx = df_sb.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# zx_scaled = min_max_scaler.fit_transform(zx)\n",
    "# df_sb = pd.DataFrame(zx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4f942016d8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(14.0,12.0)})\n",
    "sns.heatmap(pd.DataFrame(df_sb),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This of course closely mirrors recent Pentagon talk of wars in the decades to come.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                            0\n",
       "0  In a new interview with Britain’s Sky News, former NATO Secretary-General Anders Fogh Rasmussen brought out the old narrative of America as the “world’s policeman,” but with a lot more upbeat of an attitude about it than one would generally see.                     \n",
       "1  Rasmussen criticized President Obama for not being hawkish enough, saying his successor needs to be much more interventionist, and declaring “ we need America as the world’s policeman, ” adding that the US needs to “restore international law and order” through wars.\n",
       "2  Rasmussen, who was always a relative hawk in the post but seems to have taken it to an entirely new level, set out a series of things the US needs to fix militarily, including Iraq, Syria, Libya, Russia, China, and North Korea.                                       \n",
       "3  This of course closely mirrors recent Pentagon talk of wars in the decades to come.                                                                                                                                                                                       \n",
       "4  The timing of his calls for extreme US bellicosity are centered on trying to influence the upcoming US election in favor of Democratic nominee Hillary Clinton, who has campaigned heavily on picking fights in Syria and against Russia.                                 \n",
       "5  Rasmussen underscored this fact by declaring Donald Trump, who openly said the US cannot be the world’s police, as “very dangerous for the world.”                                                                                                                        "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x['sentences'][test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datagen_dnf_eval():\n",
    "\n",
    "    ar_ids,ar_sents,ar_head_vectors,ar_head_classes,hds,ar_claims, ar_sentences=[],[],[],[],[],[],[]\n",
    "\n",
    "    for idx in dnf_eval.id: \n",
    "        hd = dnf_eval[dnf_eval.id==idx]['headline'].values[0].lower()\n",
    "        ar_id = dnf_eval[dnf_eval.id==idx]['id'].values[0]\n",
    "        cl = dnf_eval[dnf_eval.id==idx]['claim_ids'].values[0]\n",
    "        ar_claims.append(cl)\n",
    "        sentences = articles300[ar_id]\n",
    "        vectors = article_vectors300[ar_id]\n",
    "\n",
    "\n",
    "        hds.append(hd)\n",
    "        ar_sentences.append(sentences)\n",
    "    #         print(len(sentences))\n",
    "        sents = np.zeros((max_sentences,300))\n",
    "\n",
    "        sents[:len(vectors)] = vectors\n",
    "        ar_ids.append(ar_id)\n",
    "        ar_sents.append(sents)\n",
    "        hd_nlp = nlp(hd.lower())\n",
    "        hd_nlp = hd_nlp[:50]\n",
    "        head_classes = np.zeros(50, dtype='int')\n",
    "        for i in range(len(hd_nlp)):\n",
    "            head_classes[i] = hd_nlp[i].rank\n",
    "        ar_head_vectors.append(hd_nlp.vector)\n",
    "        ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "\n",
    "        inputs = {\n",
    "            'article_id': np.array(ar_ids)\n",
    "            ,'headline': np.array(hds)\n",
    "            ,'sentence_vectors' : np.array(ar_sents)\n",
    "            ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "            ,'claims':np.array(ar_claims)\n",
    "            ,'sentences':np.array(ar_sentences)\n",
    "        }\n",
    "        outputs = {\n",
    "            'headline_token_classes': np.array(ar_head_classes)\n",
    "            ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "        }\n",
    "    return inputs,outputs\n",
    "testX,testY = datagen_dnf_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display(testX['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.401, 0.5546666666666666, 0.4654788978025811)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(testX)\n",
    "_, b2, g2 = model_2.predict(testX)\n",
    "_, b3, g3 = model_3.predict(testX)\n",
    "_, b4, g4 = model_4.predict(testX)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in range(len(testX['headline'])):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(testX['claims'][test_idx])\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    pred = b[0][:len(testX['sentences'][test_idx])].argsort()[-best_N:][::-1]\n",
    "    \n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for p in pred:\n",
    "        if p in claims:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    for c in claims:\n",
    "        if c not in pred:\n",
    "            fn+=1\n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "#     print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "#     counter+=1\n",
    "#     if counter==5:\n",
    "#         break\n",
    "#     print(\"----------------------------\")\n",
    "#     for s in t:\n",
    "#         if s>=len(x['sentences'][test_idx]):continue\n",
    "#         x['sentences'][test_idx][s]\n",
    "np.average(ps), np.average(rs), 2*np.average(ps)*np.average(rs)/(np.average(ps)+ np.average(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1be18e0d34dfb8f8f253d8ad71b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5b7ba22dfb432b81a3442fa4f2cdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.19148936170212763, 0.07894410553985023, 0.1117979703780154)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hd_tp_cdc = pd.read_csv('evaluation_set/cdc_ibm/headline_topic_mapping.csv')\n",
    "df_ar_cl_cdc = pd.read_csv('evaluation_set/cdc_ibm/article_claim_mapping.csv')\n",
    "df_hd_tp_dnf = pd.read_json('evaluation_set/deepnofakes/Evaluation_Final_50_V4.json')\n",
    "df_hd_tp_dnf.columns = ['authors','claim_ids', 'evidence', 'headline', 'id', 'reason', 'claims', 'type', 'urls']\n",
    "with open('evaluation_set/cdc_ibm/articles.p', 'rb') as fp:\n",
    "    articles = pickle.load(fp)\n",
    "with open('evaluation_set/cdc_ibm/article_vectors.p', 'rb') as fp:\n",
    "    article_vectors = pickle.load(fp)\n",
    "with open('evaluation_set/word_mapping/id_word_mapping.p', 'rb') as fp:\n",
    "    id_word_mapping = pickle.load(fp)\n",
    "df_hd_tp_cdc.keys(),df_ar_cl_cdc.keys(), len(articles.keys()), len(article_vectors.keys()), df_hd_tp_dnf.keys()\n",
    "test_titles = []\n",
    "for ar in df_ar_cl_cdc.Article.unique():\n",
    "    if len(df_ar_cl_cdc[df_ar_cl_cdc.Article==ar]['Claim'].values)>8:\n",
    "        test_titles.append(ar)\n",
    "ar_ids,ar_sents,ar_sentences,ar_head_vectors,ar_head_classes,hds,claims=[],[],[],[],[],[],[]\n",
    "for idx in tqdm_notebook(test_titles):\n",
    "#     print(idx)\n",
    "    hd = df_hd_tp_cdc[df_hd_tp_cdc.Title==idx]['Headline'].values[0].lower()\n",
    "    hds.append(hd)\n",
    "    ar_id = df_hd_tp_cdc[df_hd_tp_cdc.Title==idx]['article Id'].values[0]\n",
    "    cl = df_ar_cl_cdc[df_ar_cl_cdc.Article==idx]['Claim'].values\n",
    "    claims.append(cl)\n",
    "#     sentences=articles[ar_id]\n",
    "#     ar_sentences.append(ar_sentences)\n",
    "    #         print(len(sentences))\n",
    "    sents = np.zeros((max_sentences,300))\n",
    "    vectors = article_vectors[ar_id]\n",
    "    sents[:len(vectors)] = vectors[:max_sentences]\n",
    "    ar_ids.append(ar_id)\n",
    "    ar_sents.append(sents)\n",
    "    hd_nlp = nlp(hd.lower())\n",
    "    head_classes = np.zeros(50, dtype='int')\n",
    "    for i in range(len(hd_nlp)):\n",
    "        head_classes[i] = hd_nlp[i].rank\n",
    "    ar_head_vectors.append(hd_nlp.vector)\n",
    "    ar_head_classes.append(to_categorical(num_classes=20000,y=head_classes))\n",
    "inputs = {\n",
    "    'article_id': np.array(ar_ids)\n",
    "    ,'headline': np.array(hds)\n",
    "    ,'sentence_vectors' : np.array(ar_sents)\n",
    "#     ,'sentences' : np.array(ar_sentences)\n",
    "    ,'input_headline_vector': np.array(ar_head_vectors)\n",
    "    ,'claims':np.array(claims)\n",
    "}\n",
    "outputs = {\n",
    "    'headline_token_classes': np.array(ar_head_classes)\n",
    "    ,'output_headline_vector': np.array(ar_head_vectors)\n",
    "}\n",
    "threshold = 0.95\n",
    "best_N = 5\n",
    "_, b1, g1 = model_1.predict(inputs)\n",
    "_, b2, g2 = model_2.predict(inputs)\n",
    "_, b3, g3 = model_3.predict(inputs)\n",
    "_, b4, g4 = model_4.predict(inputs)\n",
    "ps, rs = [],[]\n",
    "counter=0\n",
    "for test_idx in tqdm_notebook(range(len(inputs['headline']))):    \n",
    "    tp,fp,fn = 0,0,0\n",
    "    claims = np.array(inputs['claims'][test_idx])\n",
    "#     sentences = list(range(len(articles[test_idx])))\n",
    "    b = b1[test_idx]+b2[test_idx]+b3[test_idx]+b4[test_idx]\n",
    "    ids = b[0][:len(articles[inputs['article_id'][test_idx]])].argsort()[-best_N:][::-1]\n",
    "#     print(ids)\n",
    "    pred = np.array(articles[inputs['article_id'][test_idx]])[ids]\n",
    "#     print('claims:',claims)\n",
    "#     print('pred:',pred)\n",
    "    for i in range(len(pred)):\n",
    "    #     print('===========***********',i,'***********============')\n",
    "        t5 = nlp(str(pred[i]))\n",
    "        flag = False\n",
    "        #pred_claim_sent.append(pred[i])\n",
    "    #     print(t5.vector)\n",
    "        for j in range(len(cl)):\n",
    "            _c = nlp(cl[j])\n",
    "    #         print(_c.vector)\n",
    "    #         print('top_5:',t5.text)\n",
    "    #         print('-------------------')\n",
    "    #         print('ground_truth:',_c.text)\n",
    "    #         print('t5:{0}, cl:{1}, sim: {2}'.format(i,j,np.around(t5.similarity(_c),4)))\n",
    "    #         print('===================================================================')\n",
    "            if np.around(t5.similarity(_c),4) > threshold:\n",
    "                tp+=1\n",
    "                flag = True\n",
    "                break\n",
    "        if flag is False:\n",
    "            fp+=1\n",
    "        \n",
    "            \n",
    "   \n",
    "    #     print(t5.vector)\n",
    "    for j in range(len(cl)):\n",
    "        _c = nlp(cl[j])\n",
    "        flag = False\n",
    "        for i in range(len(pred)):\n",
    "    #     print('===========***********',i,'***********============')\n",
    "            t5 = nlp(str(pred[i]))\n",
    "        \n",
    "    #         print(_c.vector)\n",
    "    #         print('top_5:',t5.text)\n",
    "    #         print('-------------------')\n",
    "    #         print('ground_truth:',_c.text)\n",
    "    #         print('t5:{0}, cl:{1}, sim: {2}'.format(i,j,np.around(t5.similarity(_c),4)))\n",
    "    #         print('===================================================================')\n",
    "            if np.around(t5.similarity(_c),4) > threshold:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag is False:\n",
    "            fn+=1\n",
    "         \n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "#     print()\n",
    "#     tn = sentences - list(set(list(pred)+list(claims)))\n",
    "#     print(test_idx,', article id:',x['article_id'][test_idx], ',# sentences:',len(articles[x['article_id'][test_idx]]),\":\",p,r)\n",
    "    ps.append(p)\n",
    "    rs.append(r)\n",
    "\n",
    "np.average(ps), np.average(rs), 2*np.average(ps)*np.average(rs)/(np.average(ps)+ np.average(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
